WARNING: Logging before InitGoogleLogging() is written to STDERR
I0626 02:28:31.748890  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 02:28:31.749013  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/train"
    batch_size: 256
    backend: LEVELDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "data_drop"
  name: "data_dropdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "data_drop"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner1"
  top: "d_data"
  name: "d_data"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 3600
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_data"
  bottom: "data"
  top: "pt_loss1"
  name: "pt_loss1"
  type: EUCLIDEAN_LOSS
}
I0626 02:28:31.749066  8898 net.cpp:67] Creating Layer data
I0626 02:28:31.749078  8898 net.cpp:358] data -> data
I0626 02:28:31.749094  8898 net.cpp:96] Setting up data
I0626 02:28:31.749122  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/train
I0626 02:28:31.824822  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 02:28:31.825551  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 02:28:31.825600  8898 net.cpp:67] Creating Layer data_data_0_split
I0626 02:28:31.825619  8898 net.cpp:396] data_data_0_split <- data
I0626 02:28:31.825641  8898 net.cpp:358] data_data_0_split -> data_data_0_split_0
I0626 02:28:31.825668  8898 net.cpp:358] data_data_0_split -> data_data_0_split_1
I0626 02:28:31.825690  8898 net.cpp:96] Setting up data_data_0_split
I0626 02:28:31.825716  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 02:28:31.825733  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 02:28:31.825763  8898 net.cpp:67] Creating Layer data_dropdrop
I0626 02:28:31.825779  8898 net.cpp:396] data_dropdrop <- data_data_0_split_0
I0626 02:28:31.825800  8898 net.cpp:358] data_dropdrop -> data_drop
I0626 02:28:31.825821  8898 net.cpp:96] Setting up data_dropdrop
I0626 02:28:31.825851  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 02:28:31.825875  8898 net.cpp:67] Creating Layer inner1
I0626 02:28:31.825891  8898 net.cpp:396] inner1 <- data_drop
I0626 02:28:31.825911  8898 net.cpp:358] inner1 -> inner1
I0626 02:28:31.825934  8898 net.cpp:96] Setting up inner1
I0626 02:28:31.852154  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:28:31.852205  8898 net.cpp:67] Creating Layer inner1relu
I0626 02:28:31.852214  8898 net.cpp:396] inner1relu <- inner1
I0626 02:28:31.852226  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 02:28:31.852237  8898 net.cpp:96] Setting up inner1relu
I0626 02:28:31.852246  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:28:31.852255  8898 net.cpp:67] Creating Layer inner1drop
I0626 02:28:31.852263  8898 net.cpp:396] inner1drop <- inner1
I0626 02:28:31.852272  8898 net.cpp:347] inner1drop -> inner1 (in-place)
I0626 02:28:31.852282  8898 net.cpp:96] Setting up inner1drop
I0626 02:28:31.852289  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:28:31.852301  8898 net.cpp:67] Creating Layer d_data
I0626 02:28:31.852309  8898 net.cpp:396] d_data <- inner1
I0626 02:28:31.852319  8898 net.cpp:358] d_data -> d_data
I0626 02:28:31.852330  8898 net.cpp:96] Setting up d_data
I0626 02:28:31.873319  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 02:28:31.873368  8898 net.cpp:67] Creating Layer pt_loss1
I0626 02:28:31.873378  8898 net.cpp:396] pt_loss1 <- d_data
I0626 02:28:31.873387  8898 net.cpp:396] pt_loss1 <- data_data_0_split_1
I0626 02:28:31.873396  8898 net.cpp:358] pt_loss1 -> pt_loss1
I0626 02:28:31.873407  8898 net.cpp:96] Setting up pt_loss1
I0626 02:28:31.873425  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 02:28:31.873432  8898 net.cpp:109]     with loss weight 1
I0626 02:28:31.873448  8898 net.cpp:170] pt_loss1 needs backward computation.
I0626 02:28:31.873456  8898 net.cpp:170] d_data needs backward computation.
I0626 02:28:31.873464  8898 net.cpp:170] inner1drop needs backward computation.
I0626 02:28:31.873472  8898 net.cpp:170] inner1relu needs backward computation.
I0626 02:28:31.873479  8898 net.cpp:170] inner1 needs backward computation.
I0626 02:28:31.873487  8898 net.cpp:172] data_dropdrop does not need backward computation.
I0626 02:28:31.873494  8898 net.cpp:172] data_data_0_split does not need backward computation.
I0626 02:28:31.873502  8898 net.cpp:172] data does not need backward computation.
I0626 02:28:31.873509  8898 net.cpp:208] This network produces output pt_loss1
I0626 02:28:31.873520  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 02:28:31.873529  8898 net.cpp:219] Network initialization done.
I0626 02:28:31.873536  8898 net.cpp:220] Memory required for data: 19968004
I0626 02:28:32.334642  8910 caffe.cpp:99] Use GPU with device ID 0
I0626 02:28:32.578631  8910 caffe.cpp:107] Starting Optimization
I0626 02:28:32.578727  8910 solver.cpp:32] Initializing solver from parameters: 
test_iter: 100
test_interval: 10000
base_lr: 0.1
display: 1000
max_iter: 50000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 20000
snapshot: 10000
snapshot_prefix: "modules/image30x30_dim50/exp/save"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/stack_net.prototxt"
snapshot_after_train: true
momentum_burnin: 1000
I0626 02:28:32.578752  8910 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/stack_net.prototxt
I0626 02:28:32.578891  8910 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 02:28:32.578960  8910 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/train"
    batch_size: 256
    backend: LEVELDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "data_drop"
  name: "data_dropdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "data_drop"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner1"
  top: "d_data"
  name: "d_data"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 3600
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_data"
  bottom: "data"
  top: "pt_loss1"
  name: "pt_loss1"
  type: EUCLIDEAN_LOSS
}
state {
  phase: TRAIN
}
I0626 02:28:32.579020  8910 net.cpp:67] Creating Layer data
I0626 02:28:32.579032  8910 net.cpp:358] data -> data
I0626 02:28:32.579051  8910 net.cpp:96] Setting up data
I0626 02:28:32.579082  8910 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/train
I0626 02:28:32.659104  8910 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 02:28:32.659304  8910 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 02:28:32.659340  8910 net.cpp:67] Creating Layer data_data_0_split
I0626 02:28:32.659359  8910 net.cpp:396] data_data_0_split <- data
I0626 02:28:32.659380  8910 net.cpp:358] data_data_0_split -> data_data_0_split_0
I0626 02:28:32.659411  8910 net.cpp:358] data_data_0_split -> data_data_0_split_1
I0626 02:28:32.659436  8910 net.cpp:96] Setting up data_data_0_split
I0626 02:28:32.659462  8910 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 02:28:32.659507  8910 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 02:28:32.659529  8910 net.cpp:67] Creating Layer data_dropdrop
I0626 02:28:32.659548  8910 net.cpp:396] data_dropdrop <- data_data_0_split_0
I0626 02:28:32.659574  8910 net.cpp:358] data_dropdrop -> data_drop
I0626 02:28:32.659597  8910 net.cpp:96] Setting up data_dropdrop
I0626 02:28:32.659628  8910 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 02:28:32.659651  8910 net.cpp:67] Creating Layer inner1
I0626 02:28:32.659668  8910 net.cpp:396] inner1 <- data_drop
I0626 02:28:32.659689  8910 net.cpp:358] inner1 -> inner1
I0626 02:28:32.659713  8910 net.cpp:96] Setting up inner1
I0626 02:28:32.687341  8910 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:28:32.687397  8910 net.cpp:67] Creating Layer inner1relu
I0626 02:28:32.687407  8910 net.cpp:396] inner1relu <- inner1
I0626 02:28:32.687418  8910 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 02:28:32.687430  8910 net.cpp:96] Setting up inner1relu
I0626 02:28:32.687439  8910 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:28:32.687482  8910 net.cpp:67] Creating Layer inner1drop
I0626 02:28:32.687492  8910 net.cpp:396] inner1drop <- inner1
I0626 02:28:32.687501  8910 net.cpp:347] inner1drop -> inner1 (in-place)
I0626 02:28:32.687518  8910 net.cpp:96] Setting up inner1drop
I0626 02:28:32.687527  8910 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:28:32.687538  8910 net.cpp:67] Creating Layer d_data
I0626 02:28:32.687546  8910 net.cpp:396] d_data <- inner1
I0626 02:28:32.687556  8910 net.cpp:358] d_data -> d_data
I0626 02:28:32.687566  8910 net.cpp:96] Setting up d_data
I0626 02:28:32.708667  8910 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 02:28:32.708714  8910 net.cpp:67] Creating Layer pt_loss1
I0626 02:28:32.708724  8910 net.cpp:396] pt_loss1 <- d_data
I0626 02:28:32.708734  8910 net.cpp:396] pt_loss1 <- data_data_0_split_1
I0626 02:28:32.708744  8910 net.cpp:358] pt_loss1 -> pt_loss1
I0626 02:28:32.708756  8910 net.cpp:96] Setting up pt_loss1
I0626 02:28:32.708773  8910 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 02:28:32.708781  8910 net.cpp:109]     with loss weight 1
I0626 02:28:32.708817  8910 net.cpp:170] pt_loss1 needs backward computation.
I0626 02:28:32.708825  8910 net.cpp:170] d_data needs backward computation.
I0626 02:28:32.708833  8910 net.cpp:170] inner1drop needs backward computation.
I0626 02:28:32.708842  8910 net.cpp:170] inner1relu needs backward computation.
I0626 02:28:32.708848  8910 net.cpp:170] inner1 needs backward computation.
I0626 02:28:32.708858  8910 net.cpp:172] data_dropdrop does not need backward computation.
I0626 02:28:32.708865  8910 net.cpp:172] data_data_0_split does not need backward computation.
I0626 02:28:32.708873  8910 net.cpp:172] data does not need backward computation.
I0626 02:28:32.708881  8910 net.cpp:208] This network produces output pt_loss1
I0626 02:28:32.708894  8910 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 02:28:32.708904  8910 net.cpp:219] Network initialization done.
I0626 02:28:32.708915  8910 net.cpp:220] Memory required for data: 19968004
I0626 02:28:32.709060  8910 solver.cpp:151] Creating test net (#0) specified by net file: modules/image30x30_dim50/stack_net.prototxt
I0626 02:28:32.709081  8910 net.cpp:277] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0626 02:28:32.709153  8910 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/test"
    batch_size: 100
    backend: LEVELDB
  }
  include {
    phase: TEST
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "data_drop"
  name: "data_dropdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "data_drop"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner1"
  top: "d_data"
  name: "d_data"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 3600
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_data"
  bottom: "data"
  top: "pt_loss1"
  name: "pt_loss1"
  type: EUCLIDEAN_LOSS
}
state {
  phase: TEST
}
I0626 02:28:32.709208  8910 net.cpp:67] Creating Layer data
I0626 02:28:32.709221  8910 net.cpp:358] data -> data
I0626 02:28:32.709233  8910 net.cpp:96] Setting up data
I0626 02:28:32.709242  8910 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/test
I0626 02:28:32.792889  8910 data_layer.cpp:145] output data size: 100,3600,1,1
I0626 02:28:32.793128  8910 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 02:28:32.793162  8910 net.cpp:67] Creating Layer data_data_0_split
I0626 02:28:32.793181  8910 net.cpp:396] data_data_0_split <- data
I0626 02:28:32.793203  8910 net.cpp:358] data_data_0_split -> data_data_0_split_0
I0626 02:28:32.793229  8910 net.cpp:358] data_data_0_split -> data_data_0_split_1
I0626 02:28:32.793253  8910 net.cpp:96] Setting up data_data_0_split
I0626 02:28:32.793272  8910 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 02:28:32.793290  8910 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 02:28:32.793313  8910 net.cpp:67] Creating Layer data_dropdrop
I0626 02:28:32.793329  8910 net.cpp:396] data_dropdrop <- data_data_0_split_0
I0626 02:28:32.793351  8910 net.cpp:358] data_dropdrop -> data_drop
I0626 02:28:32.793372  8910 net.cpp:96] Setting up data_dropdrop
I0626 02:28:32.793392  8910 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 02:28:32.793414  8910 net.cpp:67] Creating Layer inner1
I0626 02:28:32.793431  8910 net.cpp:396] inner1 <- data_drop
I0626 02:28:32.793452  8910 net.cpp:358] inner1 -> inner1
I0626 02:28:32.793475  8910 net.cpp:96] Setting up inner1
I0626 02:28:32.821962  8910 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:28:32.822012  8910 net.cpp:67] Creating Layer inner1relu
I0626 02:28:32.822021  8910 net.cpp:396] inner1relu <- inner1
I0626 02:28:32.822033  8910 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 02:28:32.822046  8910 net.cpp:96] Setting up inner1relu
I0626 02:28:32.822054  8910 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:28:32.822064  8910 net.cpp:67] Creating Layer inner1drop
I0626 02:28:32.822072  8910 net.cpp:396] inner1drop <- inner1
I0626 02:28:32.822084  8910 net.cpp:347] inner1drop -> inner1 (in-place)
I0626 02:28:32.822094  8910 net.cpp:96] Setting up inner1drop
I0626 02:28:32.822103  8910 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:28:32.822114  8910 net.cpp:67] Creating Layer d_data
I0626 02:28:32.822122  8910 net.cpp:396] d_data <- inner1
I0626 02:28:32.822134  8910 net.cpp:358] d_data -> d_data
I0626 02:28:32.822144  8910 net.cpp:96] Setting up d_data
I0626 02:28:32.843416  8910 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 02:28:32.843461  8910 net.cpp:67] Creating Layer pt_loss1
I0626 02:28:32.843477  8910 net.cpp:396] pt_loss1 <- d_data
I0626 02:28:32.843488  8910 net.cpp:396] pt_loss1 <- data_data_0_split_1
I0626 02:28:32.843508  8910 net.cpp:358] pt_loss1 -> pt_loss1
I0626 02:28:32.843523  8910 net.cpp:96] Setting up pt_loss1
I0626 02:28:32.843533  8910 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 02:28:32.843541  8910 net.cpp:109]     with loss weight 1
I0626 02:28:32.843557  8910 net.cpp:170] pt_loss1 needs backward computation.
I0626 02:28:32.843565  8910 net.cpp:170] d_data needs backward computation.
I0626 02:28:32.843574  8910 net.cpp:170] inner1drop needs backward computation.
I0626 02:28:32.843581  8910 net.cpp:170] inner1relu needs backward computation.
I0626 02:28:32.843590  8910 net.cpp:170] inner1 needs backward computation.
I0626 02:28:32.843597  8910 net.cpp:172] data_dropdrop does not need backward computation.
I0626 02:28:32.843605  8910 net.cpp:172] data_data_0_split does not need backward computation.
I0626 02:28:32.843614  8910 net.cpp:172] data does not need backward computation.
I0626 02:28:32.843621  8910 net.cpp:208] This network produces output pt_loss1
I0626 02:28:32.843633  8910 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 02:28:32.843643  8910 net.cpp:219] Network initialization done.
I0626 02:28:32.843650  8910 net.cpp:220] Memory required for data: 7800004
I0626 02:28:32.843681  8910 solver.cpp:41] Solver scaffolding done.
I0626 02:28:32.843690  8910 caffe.cpp:115] Finetuning from modules/image30x30_dim50/stack_init.caffemodel
I0626 02:28:32.856189  8910 solver.cpp:160] Solving net
I0626 02:28:32.856237  8910 solver.cpp:248] Iteration 0, Testing net (#0)
I0626 02:28:33.043421  8910 solver.cpp:299]     Test net output #0: pt_loss1 = 0.122105 (* 1 = 0.122105 loss)
I0626 02:28:33.053330  8910 solver.cpp:192] Iteration 0, loss = 0.104941
I0626 02:28:33.053398  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.104941 (* 1 = 0.104941 loss)
I0626 02:28:33.053436  8910 solver.cpp:408] Iteration 0, lr = 0.1, mom = 0
I0626 02:28:44.490237  8910 solver.cpp:192] Iteration 1000, loss = 0.0225168
I0626 02:28:44.490316  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0225168 (* 1 = 0.0225168 loss)
I0626 02:28:44.490336  8910 solver.cpp:408] Iteration 1000, lr = 0.1, mom = 0.9
I0626 02:28:55.869201  8910 solver.cpp:192] Iteration 2000, loss = 0.0175397
I0626 02:28:55.869271  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0175397 (* 1 = 0.0175397 loss)
I0626 02:28:55.869288  8910 solver.cpp:408] Iteration 2000, lr = 0.1, mom = 0.9
I0626 02:29:09.605576  8910 solver.cpp:192] Iteration 3000, loss = 0.0147409
I0626 02:29:09.605697  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0147409 (* 1 = 0.0147409 loss)
I0626 02:29:09.605717  8910 solver.cpp:408] Iteration 3000, lr = 0.1, mom = 0.9
I0626 02:29:20.985414  8910 solver.cpp:192] Iteration 4000, loss = 0.0135118
I0626 02:29:20.985507  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0135118 (* 1 = 0.0135118 loss)
I0626 02:29:20.985532  8910 solver.cpp:408] Iteration 4000, lr = 0.1, mom = 0.9
I0626 02:29:32.376562  8910 solver.cpp:192] Iteration 5000, loss = 0.0130844
I0626 02:29:32.376623  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0130844 (* 1 = 0.0130844 loss)
I0626 02:29:32.376637  8910 solver.cpp:408] Iteration 5000, lr = 0.1, mom = 0.9
I0626 02:29:43.791260  8910 solver.cpp:192] Iteration 6000, loss = 0.0126436
I0626 02:29:43.791411  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0126436 (* 1 = 0.0126436 loss)
I0626 02:29:43.791430  8910 solver.cpp:408] Iteration 6000, lr = 0.1, mom = 0.9
I0626 02:29:55.172039  8910 solver.cpp:192] Iteration 7000, loss = 0.0129493
I0626 02:29:55.172106  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0129493 (* 1 = 0.0129493 loss)
I0626 02:29:55.172121  8910 solver.cpp:408] Iteration 7000, lr = 0.1, mom = 0.9
I0626 02:30:06.566967  8910 solver.cpp:192] Iteration 8000, loss = 0.0129597
I0626 02:30:06.567057  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0129597 (* 1 = 0.0129597 loss)
I0626 02:30:06.567081  8910 solver.cpp:408] Iteration 8000, lr = 0.1, mom = 0.9
I0626 02:30:17.984864  8910 solver.cpp:192] Iteration 9000, loss = 0.0132876
I0626 02:30:17.985026  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0132876 (* 1 = 0.0132876 loss)
I0626 02:30:17.985051  8910 solver.cpp:408] Iteration 9000, lr = 0.1, mom = 0.9
I0626 02:30:29.346148  8910 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_10000.caffemodel
I0626 02:30:29.394240  8910 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_10000.solverstate
I0626 02:30:29.424530  8910 solver.cpp:248] Iteration 10000, Testing net (#0)
I0626 02:30:29.605396  8910 solver.cpp:299]     Test net output #0: pt_loss1 = 0.00741148 (* 1 = 0.00741148 loss)
I0626 02:30:29.607128  8910 solver.cpp:192] Iteration 10000, loss = 0.0136569
I0626 02:30:29.607193  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0136569 (* 1 = 0.0136569 loss)
I0626 02:30:29.607218  8910 solver.cpp:408] Iteration 10000, lr = 0.1, mom = 0.9
I0626 02:30:40.969851  8910 solver.cpp:192] Iteration 11000, loss = 0.0140015
I0626 02:30:40.969949  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0140015 (* 1 = 0.0140015 loss)
I0626 02:30:40.969974  8910 solver.cpp:408] Iteration 11000, lr = 0.1, mom = 0.9
I0626 02:30:52.366741  8910 solver.cpp:192] Iteration 12000, loss = 0.0138599
I0626 02:30:52.366853  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0138599 (* 1 = 0.0138599 loss)
I0626 02:30:52.366876  8910 solver.cpp:408] Iteration 12000, lr = 0.1, mom = 0.9
I0626 02:31:03.933825  8910 solver.cpp:192] Iteration 13000, loss = 0.0138264
I0626 02:31:03.933917  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0138264 (* 1 = 0.0138264 loss)
I0626 02:31:03.933944  8910 solver.cpp:408] Iteration 13000, lr = 0.1, mom = 0.9
I0626 02:31:15.394188  8910 solver.cpp:192] Iteration 14000, loss = 0.0131163
I0626 02:31:15.394279  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0131163 (* 1 = 0.0131163 loss)
I0626 02:31:15.394302  8910 solver.cpp:408] Iteration 14000, lr = 0.1, mom = 0.9
I0626 02:31:26.799381  8910 solver.cpp:192] Iteration 15000, loss = 0.0124706
I0626 02:31:26.799628  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0124706 (* 1 = 0.0124706 loss)
I0626 02:31:26.799656  8910 solver.cpp:408] Iteration 15000, lr = 0.1, mom = 0.9
I0626 02:31:38.143940  8910 solver.cpp:192] Iteration 16000, loss = 0.0118353
I0626 02:31:38.144021  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0118353 (* 1 = 0.0118353 loss)
I0626 02:31:38.144042  8910 solver.cpp:408] Iteration 16000, lr = 0.1, mom = 0.9
I0626 02:31:49.413142  8910 solver.cpp:192] Iteration 17000, loss = 0.0108664
I0626 02:31:49.413205  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0108664 (* 1 = 0.0108664 loss)
I0626 02:31:49.413218  8910 solver.cpp:408] Iteration 17000, lr = 0.1, mom = 0.9
I0626 02:32:00.761890  8910 solver.cpp:192] Iteration 18000, loss = 0.0100539
I0626 02:32:00.762114  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0100539 (* 1 = 0.0100539 loss)
I0626 02:32:00.762145  8910 solver.cpp:408] Iteration 18000, lr = 0.1, mom = 0.9
I0626 02:32:12.219121  8910 solver.cpp:192] Iteration 19000, loss = 0.00986625
I0626 02:32:12.219188  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00986625 (* 1 = 0.00986625 loss)
I0626 02:32:12.219202  8910 solver.cpp:408] Iteration 19000, lr = 0.1, mom = 0.9
I0626 02:32:23.694006  8910 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_20000.caffemodel
I0626 02:32:23.731163  8910 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_20000.solverstate
I0626 02:32:23.758617  8910 solver.cpp:248] Iteration 20000, Testing net (#0)
I0626 02:32:23.929719  8910 solver.cpp:299]     Test net output #0: pt_loss1 = 0.00636166 (* 1 = 0.00636166 loss)
I0626 02:32:23.931413  8910 solver.cpp:192] Iteration 20000, loss = 0.00961877
I0626 02:32:23.931496  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00961877 (* 1 = 0.00961877 loss)
I0626 02:32:23.931524  8910 solver.cpp:408] Iteration 20000, lr = 0.01, mom = 0.9
I0626 02:32:35.341361  8910 solver.cpp:192] Iteration 21000, loss = 0.00932785
I0626 02:32:35.341462  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00932785 (* 1 = 0.00932785 loss)
I0626 02:32:35.341481  8910 solver.cpp:408] Iteration 21000, lr = 0.01, mom = 0.9
I0626 02:32:46.572679  8910 solver.cpp:192] Iteration 22000, loss = 0.00918426
I0626 02:32:46.572746  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00918426 (* 1 = 0.00918426 loss)
I0626 02:32:46.572760  8910 solver.cpp:408] Iteration 22000, lr = 0.01, mom = 0.9
I0626 02:32:57.798254  8910 solver.cpp:192] Iteration 23000, loss = 0.0091146
I0626 02:32:57.798338  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0091146 (* 1 = 0.0091146 loss)
I0626 02:32:57.798362  8910 solver.cpp:408] Iteration 23000, lr = 0.01, mom = 0.9
I0626 02:33:09.225783  8910 solver.cpp:192] Iteration 24000, loss = 0.00923872
I0626 02:33:09.225890  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00923872 (* 1 = 0.00923872 loss)
I0626 02:33:09.226032  8910 solver.cpp:408] Iteration 24000, lr = 0.01, mom = 0.9
I0626 02:33:20.532768  8910 solver.cpp:192] Iteration 25000, loss = 0.00930499
I0626 02:33:20.532832  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00930499 (* 1 = 0.00930499 loss)
I0626 02:33:20.532846  8910 solver.cpp:408] Iteration 25000, lr = 0.01, mom = 0.9
I0626 02:33:31.907948  8910 solver.cpp:192] Iteration 26000, loss = 0.00916232
I0626 02:33:31.908033  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00916232 (* 1 = 0.00916232 loss)
I0626 02:33:31.908056  8910 solver.cpp:408] Iteration 26000, lr = 0.01, mom = 0.9
I0626 02:33:43.240736  8910 solver.cpp:192] Iteration 27000, loss = 0.00884839
I0626 02:33:43.240980  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00884839 (* 1 = 0.00884839 loss)
I0626 02:33:43.241001  8910 solver.cpp:408] Iteration 27000, lr = 0.01, mom = 0.9
I0626 02:33:54.619125  8910 solver.cpp:192] Iteration 28000, loss = 0.00862597
I0626 02:33:54.619212  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00862597 (* 1 = 0.00862597 loss)
I0626 02:33:54.619236  8910 solver.cpp:408] Iteration 28000, lr = 0.01, mom = 0.9
I0626 02:34:06.061554  8910 solver.cpp:192] Iteration 29000, loss = 0.00869469
I0626 02:34:06.061637  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00869469 (* 1 = 0.00869469 loss)
I0626 02:34:06.061661  8910 solver.cpp:408] Iteration 29000, lr = 0.01, mom = 0.9
I0626 02:34:17.469658  8910 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_30000.caffemodel
I0626 02:34:17.508195  8910 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_30000.solverstate
I0626 02:34:17.535158  8910 solver.cpp:248] Iteration 30000, Testing net (#0)
I0626 02:34:17.709374  8910 solver.cpp:299]     Test net output #0: pt_loss1 = 0.0062925 (* 1 = 0.0062925 loss)
I0626 02:34:17.711107  8910 solver.cpp:192] Iteration 30000, loss = 0.00849313
I0626 02:34:17.711174  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00849313 (* 1 = 0.00849313 loss)
I0626 02:34:17.711199  8910 solver.cpp:408] Iteration 30000, lr = 0.01, mom = 0.9
I0626 02:34:29.077402  8910 solver.cpp:192] Iteration 31000, loss = 0.00822922
I0626 02:34:29.077469  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00822922 (* 1 = 0.00822922 loss)
I0626 02:34:29.077483  8910 solver.cpp:408] Iteration 31000, lr = 0.01, mom = 0.9
I0626 02:34:40.405198  8910 solver.cpp:192] Iteration 32000, loss = 0.00778314
I0626 02:34:40.405280  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00778314 (* 1 = 0.00778314 loss)
I0626 02:34:40.405303  8910 solver.cpp:408] Iteration 32000, lr = 0.01, mom = 0.9
I0626 02:34:51.749230  8910 solver.cpp:192] Iteration 33000, loss = 0.00749451
I0626 02:34:51.749332  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00749451 (* 1 = 0.00749451 loss)
I0626 02:34:51.749352  8910 solver.cpp:408] Iteration 33000, lr = 0.01, mom = 0.9
I0626 02:35:03.184703  8910 solver.cpp:192] Iteration 34000, loss = 0.0079193
I0626 02:35:03.184778  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0079193 (* 1 = 0.0079193 loss)
I0626 02:35:03.184792  8910 solver.cpp:408] Iteration 34000, lr = 0.01, mom = 0.9
I0626 02:35:14.498922  8910 solver.cpp:192] Iteration 35000, loss = 0.00762169
I0626 02:35:14.498993  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00762169 (* 1 = 0.00762169 loss)
I0626 02:35:14.499012  8910 solver.cpp:408] Iteration 35000, lr = 0.01, mom = 0.9
I0626 02:35:25.913476  8910 solver.cpp:192] Iteration 36000, loss = 0.00769605
I0626 02:35:25.913584  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00769605 (* 1 = 0.00769605 loss)
I0626 02:35:25.913604  8910 solver.cpp:408] Iteration 36000, lr = 0.01, mom = 0.9
I0626 02:35:37.239431  8910 solver.cpp:192] Iteration 37000, loss = 0.00790363
I0626 02:35:37.239526  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00790363 (* 1 = 0.00790363 loss)
I0626 02:35:37.239540  8910 solver.cpp:408] Iteration 37000, lr = 0.01, mom = 0.9
I0626 02:35:48.515540  8910 solver.cpp:192] Iteration 38000, loss = 0.00830099
I0626 02:35:48.515625  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00830099 (* 1 = 0.00830099 loss)
I0626 02:35:48.515647  8910 solver.cpp:408] Iteration 38000, lr = 0.01, mom = 0.9
I0626 02:35:59.840457  8910 solver.cpp:192] Iteration 39000, loss = 0.00909275
I0626 02:35:59.840616  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00909275 (* 1 = 0.00909275 loss)
I0626 02:35:59.840636  8910 solver.cpp:408] Iteration 39000, lr = 0.01, mom = 0.9
I0626 02:36:11.309767  8910 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_40000.caffemodel
I0626 02:36:11.346674  8910 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_40000.solverstate
I0626 02:36:11.373442  8910 solver.cpp:248] Iteration 40000, Testing net (#0)
I0626 02:36:11.545598  8910 solver.cpp:299]     Test net output #0: pt_loss1 = 0.00624674 (* 1 = 0.00624674 loss)
I0626 02:36:11.547281  8910 solver.cpp:192] Iteration 40000, loss = 0.00943536
I0626 02:36:11.547340  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00943536 (* 1 = 0.00943536 loss)
I0626 02:36:11.547364  8910 solver.cpp:408] Iteration 40000, lr = 0.001, mom = 0.9
I0626 02:36:22.907377  8910 solver.cpp:192] Iteration 41000, loss = 0.00917801
I0626 02:36:22.907488  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00917801 (* 1 = 0.00917801 loss)
I0626 02:36:22.907524  8910 solver.cpp:408] Iteration 41000, lr = 0.001, mom = 0.9
I0626 02:36:34.210582  8910 solver.cpp:192] Iteration 42000, loss = 0.00891284
I0626 02:36:34.210705  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00891284 (* 1 = 0.00891284 loss)
I0626 02:36:34.210726  8910 solver.cpp:408] Iteration 42000, lr = 0.001, mom = 0.9
I0626 02:36:45.472637  8910 solver.cpp:192] Iteration 43000, loss = 0.00896984
I0626 02:36:45.472702  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00896984 (* 1 = 0.00896984 loss)
I0626 02:36:45.472717  8910 solver.cpp:408] Iteration 43000, lr = 0.001, mom = 0.9
I0626 02:36:56.820935  8910 solver.cpp:192] Iteration 44000, loss = 0.00897979
I0626 02:36:56.821013  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00897979 (* 1 = 0.00897979 loss)
I0626 02:36:56.821033  8910 solver.cpp:408] Iteration 44000, lr = 0.001, mom = 0.9
I0626 02:37:08.206069  8910 solver.cpp:192] Iteration 45000, loss = 0.00874696
I0626 02:37:08.206311  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00874696 (* 1 = 0.00874696 loss)
I0626 02:37:08.206339  8910 solver.cpp:408] Iteration 45000, lr = 0.001, mom = 0.9
I0626 02:37:19.564456  8910 solver.cpp:192] Iteration 46000, loss = 0.00878565
I0626 02:37:19.564538  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00878565 (* 1 = 0.00878565 loss)
I0626 02:37:19.564559  8910 solver.cpp:408] Iteration 46000, lr = 0.001, mom = 0.9
I0626 02:37:30.917558  8910 solver.cpp:192] Iteration 47000, loss = 0.0083459
I0626 02:37:30.917646  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.0083459 (* 1 = 0.0083459 loss)
I0626 02:37:30.917688  8910 solver.cpp:408] Iteration 47000, lr = 0.001, mom = 0.9
I0626 02:37:42.298763  8910 solver.cpp:192] Iteration 48000, loss = 0.00845451
I0626 02:37:42.298902  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00845451 (* 1 = 0.00845451 loss)
I0626 02:37:42.298924  8910 solver.cpp:408] Iteration 48000, lr = 0.001, mom = 0.9
I0626 02:37:53.705487  8910 solver.cpp:192] Iteration 49000, loss = 0.00870327
I0626 02:37:53.705559  8910 solver.cpp:207]     Train net output #0: pt_loss1 = 0.00870327 (* 1 = 0.00870327 loss)
I0626 02:37:53.705579  8910 solver.cpp:408] Iteration 49000, lr = 0.001, mom = 0.9
I0626 02:38:05.211822  8910 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_50000.caffemodel
I0626 02:38:05.249248  8910 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_50000.solverstate
I0626 02:38:05.278084  8910 solver.cpp:229] Iteration 50000, loss = 0.00822069
I0626 02:38:05.278129  8910 solver.cpp:248] Iteration 50000, Testing net (#0)
I0626 02:38:05.447916  8910 solver.cpp:299]     Test net output #0: pt_loss1 = 0.00623246 (* 1 = 0.00623246 loss)
I0626 02:38:05.447969  8910 solver.cpp:234] Optimization Done.
I0626 02:38:05.447985  8910 caffe.cpp:121] Optimization Done.
I0626 02:38:05.494976  8898 net.cpp:277] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0626 02:38:05.495115  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/test"
    batch_size: 100
    backend: LEVELDB
  }
  include {
    phase: TEST
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "data_drop"
  name: "data_dropdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "data_drop"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner1"
  top: "d_data"
  name: "d_data"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 3600
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_data"
  bottom: "data"
  top: "pt_loss1"
  name: "pt_loss1"
  type: EUCLIDEAN_LOSS
}
I0626 02:38:05.495184  8898 net.cpp:67] Creating Layer data
I0626 02:38:05.495203  8898 net.cpp:358] data -> data
I0626 02:38:05.495223  8898 net.cpp:96] Setting up data
I0626 02:38:05.495235  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/test
I0626 02:38:05.564278  8898 data_layer.cpp:145] output data size: 100,3600,1,1
I0626 02:38:05.565062  8898 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 02:38:05.565099  8898 net.cpp:67] Creating Layer data_data_0_split
I0626 02:38:05.565116  8898 net.cpp:396] data_data_0_split <- data
I0626 02:38:05.565138  8898 net.cpp:358] data_data_0_split -> data_data_0_split_0
I0626 02:38:05.565165  8898 net.cpp:358] data_data_0_split -> data_data_0_split_1
I0626 02:38:05.565186  8898 net.cpp:96] Setting up data_data_0_split
I0626 02:38:05.565205  8898 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 02:38:05.565223  8898 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 02:38:05.565244  8898 net.cpp:67] Creating Layer data_dropdrop
I0626 02:38:05.565261  8898 net.cpp:396] data_dropdrop <- data_data_0_split_0
I0626 02:38:05.565282  8898 net.cpp:358] data_dropdrop -> data_drop
I0626 02:38:05.565304  8898 net.cpp:96] Setting up data_dropdrop
I0626 02:38:05.565333  8898 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 02:38:05.565356  8898 net.cpp:67] Creating Layer inner1
I0626 02:38:05.565372  8898 net.cpp:396] inner1 <- data_drop
I0626 02:38:05.565403  8898 net.cpp:358] inner1 -> inner1
I0626 02:38:05.565428  8898 net.cpp:96] Setting up inner1
I0626 02:38:05.593705  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:38:05.593756  8898 net.cpp:67] Creating Layer inner1relu
I0626 02:38:05.593766  8898 net.cpp:396] inner1relu <- inner1
I0626 02:38:05.593778  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 02:38:05.593789  8898 net.cpp:96] Setting up inner1relu
I0626 02:38:05.593798  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:38:05.593807  8898 net.cpp:67] Creating Layer inner1drop
I0626 02:38:05.593816  8898 net.cpp:396] inner1drop <- inner1
I0626 02:38:05.593823  8898 net.cpp:347] inner1drop -> inner1 (in-place)
I0626 02:38:05.593833  8898 net.cpp:96] Setting up inner1drop
I0626 02:38:05.593842  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:38:05.593852  8898 net.cpp:67] Creating Layer d_data
I0626 02:38:05.593860  8898 net.cpp:396] d_data <- inner1
I0626 02:38:05.593870  8898 net.cpp:358] d_data -> d_data
I0626 02:38:05.593880  8898 net.cpp:96] Setting up d_data
I0626 02:38:05.614819  8898 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 02:38:05.614868  8898 net.cpp:67] Creating Layer pt_loss1
I0626 02:38:05.614878  8898 net.cpp:396] pt_loss1 <- d_data
I0626 02:38:05.614888  8898 net.cpp:396] pt_loss1 <- data_data_0_split_1
I0626 02:38:05.614897  8898 net.cpp:358] pt_loss1 -> pt_loss1
I0626 02:38:05.614908  8898 net.cpp:96] Setting up pt_loss1
I0626 02:38:05.614918  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 02:38:05.614926  8898 net.cpp:109]     with loss weight 1
I0626 02:38:05.614943  8898 net.cpp:170] pt_loss1 needs backward computation.
I0626 02:38:05.614950  8898 net.cpp:170] d_data needs backward computation.
I0626 02:38:05.614959  8898 net.cpp:170] inner1drop needs backward computation.
I0626 02:38:05.614965  8898 net.cpp:170] inner1relu needs backward computation.
I0626 02:38:05.614972  8898 net.cpp:170] inner1 needs backward computation.
I0626 02:38:05.614980  8898 net.cpp:172] data_dropdrop does not need backward computation.
I0626 02:38:05.614989  8898 net.cpp:172] data_data_0_split does not need backward computation.
I0626 02:38:05.614996  8898 net.cpp:172] data does not need backward computation.
I0626 02:38:05.615003  8898 net.cpp:208] This network produces output pt_loss1
I0626 02:38:05.615015  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 02:38:05.615025  8898 net.cpp:219] Network initialization done.
I0626 02:38:05.615032  8898 net.cpp:220] Memory required for data: 7800004
I0626 02:38:05.632349  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 02:38:05.632452  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/train"
    batch_size: 256
    backend: LEVELDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner1_drop"
  name: "inner1_dropdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner1_drop"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner2"
  top: "d_inner1"
  name: "d_inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner1"
  top: "d_inner1"
  name: "d_inner1relu"
  type: RELU
}
layers {
  bottom: "d_inner1"
  bottom: "inner1"
  top: "pt_loss2"
  name: "pt_loss2"
  type: EUCLIDEAN_LOSS
}
I0626 02:38:05.632513  8898 net.cpp:67] Creating Layer data
I0626 02:38:05.632524  8898 net.cpp:358] data -> data
I0626 02:38:05.632537  8898 net.cpp:96] Setting up data
I0626 02:38:05.632547  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/train
I0626 02:38:05.732573  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 02:38:05.734318  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 02:38:05.734372  8898 net.cpp:67] Creating Layer inner1
I0626 02:38:05.734392  8898 net.cpp:396] inner1 <- data
I0626 02:38:05.734416  8898 net.cpp:358] inner1 -> inner1
I0626 02:38:05.734442  8898 net.cpp:96] Setting up inner1
I0626 02:38:05.762516  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:05.762567  8898 net.cpp:67] Creating Layer inner1relu
I0626 02:38:05.762576  8898 net.cpp:396] inner1relu <- inner1
I0626 02:38:05.762586  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 02:38:05.762598  8898 net.cpp:96] Setting up inner1relu
I0626 02:38:05.762606  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:05.762616  8898 net.cpp:67] Creating Layer inner1_inner1relu_0_split
I0626 02:38:05.762624  8898 net.cpp:396] inner1_inner1relu_0_split <- inner1
I0626 02:38:05.762634  8898 net.cpp:358] inner1_inner1relu_0_split -> inner1_inner1relu_0_split_0
I0626 02:38:05.762645  8898 net.cpp:358] inner1_inner1relu_0_split -> inner1_inner1relu_0_split_1
I0626 02:38:05.762655  8898 net.cpp:96] Setting up inner1_inner1relu_0_split
I0626 02:38:05.762662  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:05.762670  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:05.762681  8898 net.cpp:67] Creating Layer inner1_dropdrop
I0626 02:38:05.762689  8898 net.cpp:396] inner1_dropdrop <- inner1_inner1relu_0_split_0
I0626 02:38:05.762699  8898 net.cpp:358] inner1_dropdrop -> inner1_drop
I0626 02:38:05.762711  8898 net.cpp:96] Setting up inner1_dropdrop
I0626 02:38:05.762719  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:05.762730  8898 net.cpp:67] Creating Layer inner2
I0626 02:38:05.762738  8898 net.cpp:396] inner2 <- inner1_drop
I0626 02:38:05.762748  8898 net.cpp:358] inner2 -> inner2
I0626 02:38:05.762758  8898 net.cpp:96] Setting up inner2
I0626 02:38:05.765615  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:05.765631  8898 net.cpp:67] Creating Layer inner2relu
I0626 02:38:05.765640  8898 net.cpp:396] inner2relu <- inner2
I0626 02:38:05.765650  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 02:38:05.765658  8898 net.cpp:96] Setting up inner2relu
I0626 02:38:05.765666  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:05.765676  8898 net.cpp:67] Creating Layer inner2drop
I0626 02:38:05.765682  8898 net.cpp:396] inner2drop <- inner2
I0626 02:38:05.765691  8898 net.cpp:347] inner2drop -> inner2 (in-place)
I0626 02:38:05.765700  8898 net.cpp:96] Setting up inner2drop
I0626 02:38:05.765708  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:05.765719  8898 net.cpp:67] Creating Layer d_inner1
I0626 02:38:05.765727  8898 net.cpp:396] d_inner1 <- inner2
I0626 02:38:05.765738  8898 net.cpp:358] d_inner1 -> d_inner1
I0626 02:38:05.765746  8898 net.cpp:96] Setting up d_inner1
I0626 02:38:05.768734  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:05.768751  8898 net.cpp:67] Creating Layer d_inner1relu
I0626 02:38:05.768774  8898 net.cpp:396] d_inner1relu <- d_inner1
I0626 02:38:05.768782  8898 net.cpp:347] d_inner1relu -> d_inner1 (in-place)
I0626 02:38:05.768792  8898 net.cpp:96] Setting up d_inner1relu
I0626 02:38:05.768800  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:05.768808  8898 net.cpp:67] Creating Layer pt_loss2
I0626 02:38:05.768816  8898 net.cpp:396] pt_loss2 <- d_inner1
I0626 02:38:05.768824  8898 net.cpp:396] pt_loss2 <- inner1_inner1relu_0_split_1
I0626 02:38:05.768836  8898 net.cpp:358] pt_loss2 -> pt_loss2
I0626 02:38:05.768844  8898 net.cpp:96] Setting up pt_loss2
I0626 02:38:05.768854  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 02:38:05.768862  8898 net.cpp:109]     with loss weight 1
I0626 02:38:05.768878  8898 net.cpp:170] pt_loss2 needs backward computation.
I0626 02:38:05.768887  8898 net.cpp:170] d_inner1relu needs backward computation.
I0626 02:38:05.768894  8898 net.cpp:170] d_inner1 needs backward computation.
I0626 02:38:05.768901  8898 net.cpp:170] inner2drop needs backward computation.
I0626 02:38:05.768908  8898 net.cpp:170] inner2relu needs backward computation.
I0626 02:38:05.768915  8898 net.cpp:170] inner2 needs backward computation.
I0626 02:38:05.768923  8898 net.cpp:172] inner1_dropdrop does not need backward computation.
I0626 02:38:05.768931  8898 net.cpp:172] inner1_inner1relu_0_split does not need backward computation.
I0626 02:38:05.768939  8898 net.cpp:172] inner1relu does not need backward computation.
I0626 02:38:05.768946  8898 net.cpp:172] inner1 does not need backward computation.
I0626 02:38:05.768954  8898 net.cpp:172] data does not need backward computation.
I0626 02:38:05.768960  8898 net.cpp:208] This network produces output pt_loss2
I0626 02:38:05.768972  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 02:38:05.768982  8898 net.cpp:219] Network initialization done.
I0626 02:38:05.768990  8898 net.cpp:220] Memory required for data: 8806404
I0626 02:38:05.940388 27075 caffe.cpp:99] Use GPU with device ID 0
I0626 02:38:06.181254 27075 caffe.cpp:107] Starting Optimization
I0626 02:38:06.181345 27075 solver.cpp:32] Initializing solver from parameters: 
test_iter: 100
test_interval: 10000
base_lr: 0.1
display: 1000
max_iter: 50000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 20000
snapshot: 10000
snapshot_prefix: "modules/image30x30_dim50/exp/save"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/stack_net.prototxt"
snapshot_after_train: true
momentum_burnin: 1000
I0626 02:38:06.181371 27075 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/stack_net.prototxt
I0626 02:38:06.181537 27075 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 02:38:06.181622 27075 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/train"
    batch_size: 256
    backend: LEVELDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner1_drop"
  name: "inner1_dropdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner1_drop"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner2"
  top: "d_inner1"
  name: "d_inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner1"
  top: "d_inner1"
  name: "d_inner1relu"
  type: RELU
}
layers {
  bottom: "d_inner1"
  bottom: "inner1"
  top: "pt_loss2"
  name: "pt_loss2"
  type: EUCLIDEAN_LOSS
}
state {
  phase: TRAIN
}
I0626 02:38:06.181699 27075 net.cpp:67] Creating Layer data
I0626 02:38:06.181712 27075 net.cpp:358] data -> data
I0626 02:38:06.181732 27075 net.cpp:96] Setting up data
I0626 02:38:06.181767 27075 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/train
I0626 02:38:06.275136 27075 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 02:38:06.275354 27075 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 02:38:06.275391 27075 net.cpp:67] Creating Layer inner1
I0626 02:38:06.275409 27075 net.cpp:396] inner1 <- data
I0626 02:38:06.275431 27075 net.cpp:358] inner1 -> inner1
I0626 02:38:06.275462 27075 net.cpp:96] Setting up inner1
I0626 02:38:06.302353 27075 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:06.302407 27075 net.cpp:67] Creating Layer inner1relu
I0626 02:38:06.302417 27075 net.cpp:396] inner1relu <- inner1
I0626 02:38:06.302428 27075 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 02:38:06.302439 27075 net.cpp:96] Setting up inner1relu
I0626 02:38:06.302453 27075 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:06.302464 27075 net.cpp:67] Creating Layer inner1_inner1relu_0_split
I0626 02:38:06.302471 27075 net.cpp:396] inner1_inner1relu_0_split <- inner1
I0626 02:38:06.302480 27075 net.cpp:358] inner1_inner1relu_0_split -> inner1_inner1relu_0_split_0
I0626 02:38:06.302491 27075 net.cpp:358] inner1_inner1relu_0_split -> inner1_inner1relu_0_split_1
I0626 02:38:06.302522 27075 net.cpp:96] Setting up inner1_inner1relu_0_split
I0626 02:38:06.302534 27075 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:06.302542 27075 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:06.302553 27075 net.cpp:67] Creating Layer inner1_dropdrop
I0626 02:38:06.302561 27075 net.cpp:396] inner1_dropdrop <- inner1_inner1relu_0_split_0
I0626 02:38:06.302577 27075 net.cpp:358] inner1_dropdrop -> inner1_drop
I0626 02:38:06.302588 27075 net.cpp:96] Setting up inner1_dropdrop
I0626 02:38:06.302598 27075 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:06.302608 27075 net.cpp:67] Creating Layer inner2
I0626 02:38:06.302616 27075 net.cpp:396] inner2 <- inner1_drop
I0626 02:38:06.302626 27075 net.cpp:358] inner2 -> inner2
I0626 02:38:06.302636 27075 net.cpp:96] Setting up inner2
I0626 02:38:06.305600 27075 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:06.305619 27075 net.cpp:67] Creating Layer inner2relu
I0626 02:38:06.305629 27075 net.cpp:396] inner2relu <- inner2
I0626 02:38:06.305637 27075 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 02:38:06.305646 27075 net.cpp:96] Setting up inner2relu
I0626 02:38:06.305655 27075 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:06.305663 27075 net.cpp:67] Creating Layer inner2drop
I0626 02:38:06.305671 27075 net.cpp:396] inner2drop <- inner2
I0626 02:38:06.305682 27075 net.cpp:347] inner2drop -> inner2 (in-place)
I0626 02:38:06.305691 27075 net.cpp:96] Setting up inner2drop
I0626 02:38:06.305699 27075 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:06.305711 27075 net.cpp:67] Creating Layer d_inner1
I0626 02:38:06.305718 27075 net.cpp:396] d_inner1 <- inner2
I0626 02:38:06.305729 27075 net.cpp:358] d_inner1 -> d_inner1
I0626 02:38:06.305739 27075 net.cpp:96] Setting up d_inner1
I0626 02:38:06.308698 27075 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:06.308715 27075 net.cpp:67] Creating Layer d_inner1relu
I0626 02:38:06.308728 27075 net.cpp:396] d_inner1relu <- d_inner1
I0626 02:38:06.308739 27075 net.cpp:347] d_inner1relu -> d_inner1 (in-place)
I0626 02:38:06.308749 27075 net.cpp:96] Setting up d_inner1relu
I0626 02:38:06.308758 27075 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:38:06.308766 27075 net.cpp:67] Creating Layer pt_loss2
I0626 02:38:06.308773 27075 net.cpp:396] pt_loss2 <- d_inner1
I0626 02:38:06.308782 27075 net.cpp:396] pt_loss2 <- inner1_inner1relu_0_split_1
I0626 02:38:06.308791 27075 net.cpp:358] pt_loss2 -> pt_loss2
I0626 02:38:06.308800 27075 net.cpp:96] Setting up pt_loss2
I0626 02:38:06.308814 27075 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 02:38:06.308821 27075 net.cpp:109]     with loss weight 1
I0626 02:38:06.308846 27075 net.cpp:170] pt_loss2 needs backward computation.
I0626 02:38:06.308854 27075 net.cpp:170] d_inner1relu needs backward computation.
I0626 02:38:06.308862 27075 net.cpp:170] d_inner1 needs backward computation.
I0626 02:38:06.308871 27075 net.cpp:170] inner2drop needs backward computation.
I0626 02:38:06.308877 27075 net.cpp:170] inner2relu needs backward computation.
I0626 02:38:06.308884 27075 net.cpp:170] inner2 needs backward computation.
I0626 02:38:06.308892 27075 net.cpp:172] inner1_dropdrop does not need backward computation.
I0626 02:38:06.308900 27075 net.cpp:172] inner1_inner1relu_0_split does not need backward computation.
I0626 02:38:06.308908 27075 net.cpp:172] inner1relu does not need backward computation.
I0626 02:38:06.308915 27075 net.cpp:172] inner1 does not need backward computation.
I0626 02:38:06.308923 27075 net.cpp:172] data does not need backward computation.
I0626 02:38:06.308930 27075 net.cpp:208] This network produces output pt_loss2
I0626 02:38:06.308945 27075 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 02:38:06.308954 27075 net.cpp:219] Network initialization done.
I0626 02:38:06.308966 27075 net.cpp:220] Memory required for data: 8806404
I0626 02:38:06.309123 27075 solver.cpp:151] Creating test net (#0) specified by net file: modules/image30x30_dim50/stack_net.prototxt
I0626 02:38:06.309154 27075 net.cpp:277] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0626 02:38:06.309238 27075 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/test"
    batch_size: 100
    backend: LEVELDB
  }
  include {
    phase: TEST
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner1_drop"
  name: "inner1_dropdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner1_drop"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner2"
  top: "d_inner1"
  name: "d_inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner1"
  top: "d_inner1"
  name: "d_inner1relu"
  type: RELU
}
layers {
  bottom: "d_inner1"
  bottom: "inner1"
  top: "pt_loss2"
  name: "pt_loss2"
  type: EUCLIDEAN_LOSS
}
state {
  phase: TEST
}
I0626 02:38:06.309294 27075 net.cpp:67] Creating Layer data
I0626 02:38:06.309307 27075 net.cpp:358] data -> data
I0626 02:38:06.309319 27075 net.cpp:96] Setting up data
I0626 02:38:06.309327 27075 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/test
I0626 02:38:06.409073 27075 data_layer.cpp:145] output data size: 100,3600,1,1
I0626 02:38:06.409306 27075 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 02:38:06.409340 27075 net.cpp:67] Creating Layer inner1
I0626 02:38:06.409358 27075 net.cpp:396] inner1 <- data
I0626 02:38:06.409379 27075 net.cpp:358] inner1 -> inner1
I0626 02:38:06.409405 27075 net.cpp:96] Setting up inner1
I0626 02:38:06.437986 27075 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:38:06.438038 27075 net.cpp:67] Creating Layer inner1relu
I0626 02:38:06.438047 27075 net.cpp:396] inner1relu <- inner1
I0626 02:38:06.438057 27075 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 02:38:06.438068 27075 net.cpp:96] Setting up inner1relu
I0626 02:38:06.438077 27075 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:38:06.438086 27075 net.cpp:67] Creating Layer inner1_inner1relu_0_split
I0626 02:38:06.438094 27075 net.cpp:396] inner1_inner1relu_0_split <- inner1
I0626 02:38:06.438103 27075 net.cpp:358] inner1_inner1relu_0_split -> inner1_inner1relu_0_split_0
I0626 02:38:06.438115 27075 net.cpp:358] inner1_inner1relu_0_split -> inner1_inner1relu_0_split_1
I0626 02:38:06.438125 27075 net.cpp:96] Setting up inner1_inner1relu_0_split
I0626 02:38:06.438134 27075 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:38:06.438143 27075 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:38:06.438153 27075 net.cpp:67] Creating Layer inner1_dropdrop
I0626 02:38:06.438160 27075 net.cpp:396] inner1_dropdrop <- inner1_inner1relu_0_split_0
I0626 02:38:06.438170 27075 net.cpp:358] inner1_dropdrop -> inner1_drop
I0626 02:38:06.438184 27075 net.cpp:96] Setting up inner1_dropdrop
I0626 02:38:06.438194 27075 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:38:06.438221 27075 net.cpp:67] Creating Layer inner2
I0626 02:38:06.438230 27075 net.cpp:396] inner2 <- inner1_drop
I0626 02:38:06.438241 27075 net.cpp:358] inner2 -> inner2
I0626 02:38:06.438251 27075 net.cpp:96] Setting up inner2
I0626 02:38:06.441195 27075 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:38:06.441213 27075 net.cpp:67] Creating Layer inner2relu
I0626 02:38:06.441222 27075 net.cpp:396] inner2relu <- inner2
I0626 02:38:06.441231 27075 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 02:38:06.441241 27075 net.cpp:96] Setting up inner2relu
I0626 02:38:06.441248 27075 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:38:06.441257 27075 net.cpp:67] Creating Layer inner2drop
I0626 02:38:06.441265 27075 net.cpp:396] inner2drop <- inner2
I0626 02:38:06.441274 27075 net.cpp:347] inner2drop -> inner2 (in-place)
I0626 02:38:06.441283 27075 net.cpp:96] Setting up inner2drop
I0626 02:38:06.441292 27075 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:38:06.441303 27075 net.cpp:67] Creating Layer d_inner1
I0626 02:38:06.441311 27075 net.cpp:396] d_inner1 <- inner2
I0626 02:38:06.441323 27075 net.cpp:358] d_inner1 -> d_inner1
I0626 02:38:06.441332 27075 net.cpp:96] Setting up d_inner1
I0626 02:38:06.444363 27075 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:38:06.444397 27075 net.cpp:67] Creating Layer d_inner1relu
I0626 02:38:06.444406 27075 net.cpp:396] d_inner1relu <- d_inner1
I0626 02:38:06.444416 27075 net.cpp:347] d_inner1relu -> d_inner1 (in-place)
I0626 02:38:06.444427 27075 net.cpp:96] Setting up d_inner1relu
I0626 02:38:06.444435 27075 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:38:06.444445 27075 net.cpp:67] Creating Layer pt_loss2
I0626 02:38:06.444453 27075 net.cpp:396] pt_loss2 <- d_inner1
I0626 02:38:06.444461 27075 net.cpp:396] pt_loss2 <- inner1_inner1relu_0_split_1
I0626 02:38:06.444473 27075 net.cpp:358] pt_loss2 -> pt_loss2
I0626 02:38:06.444484 27075 net.cpp:96] Setting up pt_loss2
I0626 02:38:06.444494 27075 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 02:38:06.444510 27075 net.cpp:109]     with loss weight 1
I0626 02:38:06.444527 27075 net.cpp:170] pt_loss2 needs backward computation.
I0626 02:38:06.444535 27075 net.cpp:170] d_inner1relu needs backward computation.
I0626 02:38:06.444543 27075 net.cpp:170] d_inner1 needs backward computation.
I0626 02:38:06.444551 27075 net.cpp:170] inner2drop needs backward computation.
I0626 02:38:06.444559 27075 net.cpp:170] inner2relu needs backward computation.
I0626 02:38:06.444567 27075 net.cpp:170] inner2 needs backward computation.
I0626 02:38:06.444574 27075 net.cpp:172] inner1_dropdrop does not need backward computation.
I0626 02:38:06.444583 27075 net.cpp:172] inner1_inner1relu_0_split does not need backward computation.
I0626 02:38:06.444591 27075 net.cpp:172] inner1relu does not need backward computation.
I0626 02:38:06.444598 27075 net.cpp:172] inner1 does not need backward computation.
I0626 02:38:06.444607 27075 net.cpp:172] data does not need backward computation.
I0626 02:38:06.444614 27075 net.cpp:208] This network produces output pt_loss2
I0626 02:38:06.444630 27075 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 02:38:06.444641 27075 net.cpp:219] Network initialization done.
I0626 02:38:06.444649 27075 net.cpp:220] Memory required for data: 3440004
I0626 02:38:06.444684 27075 solver.cpp:41] Solver scaffolding done.
I0626 02:38:06.444694 27075 caffe.cpp:115] Finetuning from modules/image30x30_dim50/stack_init.caffemodel
I0626 02:38:06.452790 27075 solver.cpp:160] Solving net
I0626 02:38:06.452841 27075 solver.cpp:248] Iteration 0, Testing net (#0)
I0626 02:38:06.646669 27075 solver.cpp:299]     Test net output #0: pt_loss2 = 0.0747638 (* 1 = 0.0747638 loss)
I0626 02:38:06.654517 27075 solver.cpp:192] Iteration 0, loss = 0.0665001
I0626 02:38:06.654582 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.0665001 (* 1 = 0.0665001 loss)
I0626 02:38:06.654604 27075 solver.cpp:408] Iteration 0, lr = 0.1, mom = 0
I0626 02:38:18.172741 27075 solver.cpp:192] Iteration 1000, loss = 0.0283135
I0626 02:38:18.172850 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.0283135 (* 1 = 0.0283135 loss)
I0626 02:38:18.172873 27075 solver.cpp:408] Iteration 1000, lr = 0.1, mom = 0.9
I0626 02:38:29.640805 27075 solver.cpp:192] Iteration 2000, loss = 0.0135123
I0626 02:38:29.640872 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.0135123 (* 1 = 0.0135123 loss)
I0626 02:38:29.640885 27075 solver.cpp:408] Iteration 2000, lr = 0.1, mom = 0.9
I0626 02:38:41.118516 27075 solver.cpp:192] Iteration 3000, loss = 0.00931884
I0626 02:38:41.118652 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00931884 (* 1 = 0.00931884 loss)
I0626 02:38:41.118676 27075 solver.cpp:408] Iteration 3000, lr = 0.1, mom = 0.9
I0626 02:38:52.637056 27075 solver.cpp:192] Iteration 4000, loss = 0.00786672
I0626 02:38:52.637142 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00786672 (* 1 = 0.00786672 loss)
I0626 02:38:52.637166 27075 solver.cpp:408] Iteration 4000, lr = 0.1, mom = 0.9
I0626 02:39:04.101166 27075 solver.cpp:192] Iteration 5000, loss = 0.00737897
I0626 02:39:04.101246 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00737897 (* 1 = 0.00737897 loss)
I0626 02:39:04.101269 27075 solver.cpp:408] Iteration 5000, lr = 0.1, mom = 0.9
I0626 02:39:15.551241 27075 solver.cpp:192] Iteration 6000, loss = 0.00705696
I0626 02:39:15.551358 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00705696 (* 1 = 0.00705696 loss)
I0626 02:39:15.551380 27075 solver.cpp:408] Iteration 6000, lr = 0.1, mom = 0.9
I0626 02:39:27.032724 27075 solver.cpp:192] Iteration 7000, loss = 0.00700428
I0626 02:39:27.032799 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00700428 (* 1 = 0.00700428 loss)
I0626 02:39:27.032819 27075 solver.cpp:408] Iteration 7000, lr = 0.1, mom = 0.9
I0626 02:39:38.524122 27075 solver.cpp:192] Iteration 8000, loss = 0.00702988
I0626 02:39:38.524185 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00702988 (* 1 = 0.00702988 loss)
I0626 02:39:38.524214 27075 solver.cpp:408] Iteration 8000, lr = 0.1, mom = 0.9
I0626 02:39:49.979455 27075 solver.cpp:192] Iteration 9000, loss = 0.00713208
I0626 02:39:49.979590 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00713208 (* 1 = 0.00713208 loss)
I0626 02:39:49.979611 27075 solver.cpp:408] Iteration 9000, lr = 0.1, mom = 0.9
I0626 02:40:01.482710 27075 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_10000.caffemodel
I0626 02:40:01.514394 27075 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_10000.solverstate
I0626 02:40:01.534178 27075 solver.cpp:248] Iteration 10000, Testing net (#0)
I0626 02:40:01.717152 27075 solver.cpp:299]     Test net output #0: pt_loss2 = 0.00220745 (* 1 = 0.00220745 loss)
I0626 02:40:01.718703 27075 solver.cpp:192] Iteration 10000, loss = 0.0073044
I0626 02:40:01.718760 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.0073044 (* 1 = 0.0073044 loss)
I0626 02:40:01.718781 27075 solver.cpp:408] Iteration 10000, lr = 0.1, mom = 0.9
I0626 02:40:13.112763 27075 solver.cpp:192] Iteration 11000, loss = 0.00728288
I0626 02:40:13.112835 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00728288 (* 1 = 0.00728288 loss)
I0626 02:40:13.112854 27075 solver.cpp:408] Iteration 11000, lr = 0.1, mom = 0.9
I0626 02:40:24.509680 27075 solver.cpp:192] Iteration 12000, loss = 0.00719998
I0626 02:40:24.509804 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00719998 (* 1 = 0.00719998 loss)
I0626 02:40:24.509827 27075 solver.cpp:408] Iteration 12000, lr = 0.1, mom = 0.9
I0626 02:40:35.945853 27075 solver.cpp:192] Iteration 13000, loss = 0.00715483
I0626 02:40:35.945940 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00715483 (* 1 = 0.00715483 loss)
I0626 02:40:35.945963 27075 solver.cpp:408] Iteration 13000, lr = 0.1, mom = 0.9
I0626 02:40:47.357734 27075 solver.cpp:192] Iteration 14000, loss = 0.00693028
I0626 02:40:47.357808 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00693028 (* 1 = 0.00693028 loss)
I0626 02:40:47.357825 27075 solver.cpp:408] Iteration 14000, lr = 0.1, mom = 0.9
I0626 02:40:58.847865 27075 solver.cpp:192] Iteration 15000, loss = 0.00662504
I0626 02:40:58.848176 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00662504 (* 1 = 0.00662504 loss)
I0626 02:40:58.848206 27075 solver.cpp:408] Iteration 15000, lr = 0.1, mom = 0.9
I0626 02:41:10.268194 27075 solver.cpp:192] Iteration 16000, loss = 0.00615426
I0626 02:41:10.268257 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00615426 (* 1 = 0.00615426 loss)
I0626 02:41:10.268271 27075 solver.cpp:408] Iteration 16000, lr = 0.1, mom = 0.9
I0626 02:41:21.711191 27075 solver.cpp:192] Iteration 17000, loss = 0.00596718
I0626 02:41:21.711268 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00596718 (* 1 = 0.00596718 loss)
I0626 02:41:21.711288 27075 solver.cpp:408] Iteration 17000, lr = 0.1, mom = 0.9
I0626 02:41:33.206773 27075 solver.cpp:192] Iteration 18000, loss = 0.00573708
I0626 02:41:33.206926 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00573708 (* 1 = 0.00573708 loss)
I0626 02:41:33.206950 27075 solver.cpp:408] Iteration 18000, lr = 0.1, mom = 0.9
I0626 02:41:44.608556 27075 solver.cpp:192] Iteration 19000, loss = 0.00566488
I0626 02:41:44.608628 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00566488 (* 1 = 0.00566488 loss)
I0626 02:41:44.608645 27075 solver.cpp:408] Iteration 19000, lr = 0.1, mom = 0.9
I0626 02:41:56.090122 27075 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_20000.caffemodel
I0626 02:41:56.120450 27075 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_20000.solverstate
I0626 02:41:56.140509 27075 solver.cpp:248] Iteration 20000, Testing net (#0)
I0626 02:41:56.324744 27075 solver.cpp:299]     Test net output #0: pt_loss2 = 0.00179895 (* 1 = 0.00179895 loss)
I0626 02:41:56.326313 27075 solver.cpp:192] Iteration 20000, loss = 0.00571258
I0626 02:41:56.326373 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00571258 (* 1 = 0.00571258 loss)
I0626 02:41:56.326414 27075 solver.cpp:408] Iteration 20000, lr = 0.01, mom = 0.9
I0626 02:42:07.892447 27075 solver.cpp:192] Iteration 21000, loss = 0.00537978
I0626 02:42:07.892539 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00537978 (* 1 = 0.00537978 loss)
I0626 02:42:07.892554 27075 solver.cpp:408] Iteration 21000, lr = 0.01, mom = 0.9
I0626 02:42:19.330723 27075 solver.cpp:192] Iteration 22000, loss = 0.00515549
I0626 02:42:19.330811 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00515549 (* 1 = 0.00515549 loss)
I0626 02:42:19.330835 27075 solver.cpp:408] Iteration 22000, lr = 0.01, mom = 0.9
I0626 02:42:30.760686 27075 solver.cpp:192] Iteration 23000, loss = 0.00529268
I0626 02:42:30.760766 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00529268 (* 1 = 0.00529268 loss)
I0626 02:42:30.760787 27075 solver.cpp:408] Iteration 23000, lr = 0.01, mom = 0.9
I0626 02:42:42.286633 27075 solver.cpp:192] Iteration 24000, loss = 0.00526018
I0626 02:42:42.286829 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00526018 (* 1 = 0.00526018 loss)
I0626 02:42:42.286856 27075 solver.cpp:408] Iteration 24000, lr = 0.01, mom = 0.9
I0626 02:42:53.808604 27075 solver.cpp:192] Iteration 25000, loss = 0.00535293
I0626 02:42:53.808668 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00535293 (* 1 = 0.00535293 loss)
I0626 02:42:53.808681 27075 solver.cpp:408] Iteration 25000, lr = 0.01, mom = 0.9
I0626 02:43:05.365437 27075 solver.cpp:192] Iteration 26000, loss = 0.00528976
I0626 02:43:05.365509 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00528976 (* 1 = 0.00528976 loss)
I0626 02:43:05.365527 27075 solver.cpp:408] Iteration 26000, lr = 0.01, mom = 0.9
I0626 02:43:16.822871 27075 solver.cpp:192] Iteration 27000, loss = 0.00496022
I0626 02:43:16.823043 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00496022 (* 1 = 0.00496022 loss)
I0626 02:43:16.823065 27075 solver.cpp:408] Iteration 27000, lr = 0.01, mom = 0.9
I0626 02:43:28.344980 27075 solver.cpp:192] Iteration 28000, loss = 0.00519353
I0626 02:43:28.345064 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00519353 (* 1 = 0.00519353 loss)
I0626 02:43:28.345093 27075 solver.cpp:408] Iteration 28000, lr = 0.01, mom = 0.9
I0626 02:43:39.824818 27075 solver.cpp:192] Iteration 29000, loss = 0.00533529
I0626 02:43:39.824895 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00533529 (* 1 = 0.00533529 loss)
I0626 02:43:39.824914 27075 solver.cpp:408] Iteration 29000, lr = 0.01, mom = 0.9
I0626 02:43:51.272264 27075 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_30000.caffemodel
I0626 02:43:51.300112 27075 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_30000.solverstate
I0626 02:43:51.319958 27075 solver.cpp:248] Iteration 30000, Testing net (#0)
I0626 02:43:51.496098 27075 solver.cpp:299]     Test net output #0: pt_loss2 = 0.00177172 (* 1 = 0.00177172 loss)
I0626 02:43:51.497687 27075 solver.cpp:192] Iteration 30000, loss = 0.00528366
I0626 02:43:51.497754 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00528366 (* 1 = 0.00528366 loss)
I0626 02:43:51.497779 27075 solver.cpp:408] Iteration 30000, lr = 0.01, mom = 0.9
I0626 02:44:02.963356 27075 solver.cpp:192] Iteration 31000, loss = 0.00502256
I0626 02:44:02.963429 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00502256 (* 1 = 0.00502256 loss)
I0626 02:44:02.963448 27075 solver.cpp:408] Iteration 31000, lr = 0.01, mom = 0.9
I0626 02:44:14.491655 27075 solver.cpp:192] Iteration 32000, loss = 0.00472236
I0626 02:44:14.491721 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00472236 (* 1 = 0.00472236 loss)
I0626 02:44:14.491734 27075 solver.cpp:408] Iteration 32000, lr = 0.01, mom = 0.9
I0626 02:44:25.985443 27075 solver.cpp:192] Iteration 33000, loss = 0.00456874
I0626 02:44:25.985570 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00456874 (* 1 = 0.00456874 loss)
I0626 02:44:25.985605 27075 solver.cpp:408] Iteration 33000, lr = 0.01, mom = 0.9
I0626 02:44:37.445606 27075 solver.cpp:192] Iteration 34000, loss = 0.00454813
I0626 02:44:37.445691 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00454813 (* 1 = 0.00454813 loss)
I0626 02:44:37.445715 27075 solver.cpp:408] Iteration 34000, lr = 0.01, mom = 0.9
I0626 02:44:48.895051 27075 solver.cpp:192] Iteration 35000, loss = 0.00437621
I0626 02:44:48.895140 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00437621 (* 1 = 0.00437621 loss)
I0626 02:44:48.895164 27075 solver.cpp:408] Iteration 35000, lr = 0.01, mom = 0.9
I0626 02:45:00.357098 27075 solver.cpp:192] Iteration 36000, loss = 0.00442912
I0626 02:45:00.357213 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00442912 (* 1 = 0.00442912 loss)
I0626 02:45:00.357236 27075 solver.cpp:408] Iteration 36000, lr = 0.01, mom = 0.9
I0626 02:45:11.941078 27075 solver.cpp:192] Iteration 37000, loss = 0.00460898
I0626 02:45:11.941165 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00460898 (* 1 = 0.00460898 loss)
I0626 02:45:11.941187 27075 solver.cpp:408] Iteration 37000, lr = 0.01, mom = 0.9
I0626 02:45:23.429265 27075 solver.cpp:192] Iteration 38000, loss = 0.00494178
I0626 02:45:23.429352 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00494178 (* 1 = 0.00494178 loss)
I0626 02:45:23.429374 27075 solver.cpp:408] Iteration 38000, lr = 0.01, mom = 0.9
I0626 02:45:34.982622 27075 solver.cpp:192] Iteration 39000, loss = 0.00538089
I0626 02:45:34.982822 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00538089 (* 1 = 0.00538089 loss)
I0626 02:45:34.982849 27075 solver.cpp:408] Iteration 39000, lr = 0.01, mom = 0.9
I0626 02:45:46.455482 27075 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_40000.caffemodel
I0626 02:45:46.484076 27075 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_40000.solverstate
I0626 02:45:46.503938 27075 solver.cpp:248] Iteration 40000, Testing net (#0)
I0626 02:45:46.683713 27075 solver.cpp:299]     Test net output #0: pt_loss2 = 0.00174658 (* 1 = 0.00174658 loss)
I0626 02:45:46.685166 27075 solver.cpp:192] Iteration 40000, loss = 0.00548022
I0626 02:45:46.685217 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00548022 (* 1 = 0.00548022 loss)
I0626 02:45:46.685235 27075 solver.cpp:408] Iteration 40000, lr = 0.001, mom = 0.9
I0626 02:45:58.225512 27075 solver.cpp:192] Iteration 41000, loss = 0.0056537
I0626 02:45:58.225591 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.0056537 (* 1 = 0.0056537 loss)
I0626 02:45:58.225615 27075 solver.cpp:408] Iteration 41000, lr = 0.001, mom = 0.9
I0626 02:46:09.660923 27075 solver.cpp:192] Iteration 42000, loss = 0.00541388
I0626 02:46:09.661099 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00541388 (* 1 = 0.00541388 loss)
I0626 02:46:09.661123 27075 solver.cpp:408] Iteration 42000, lr = 0.001, mom = 0.9
I0626 02:46:21.206315 27075 solver.cpp:192] Iteration 43000, loss = 0.00560509
I0626 02:46:21.206403 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00560509 (* 1 = 0.00560509 loss)
I0626 02:46:21.206423 27075 solver.cpp:408] Iteration 43000, lr = 0.001, mom = 0.9
I0626 02:46:32.583742 27075 solver.cpp:192] Iteration 44000, loss = 0.00562519
I0626 02:46:32.583817 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00562519 (* 1 = 0.00562519 loss)
I0626 02:46:32.583837 27075 solver.cpp:408] Iteration 44000, lr = 0.001, mom = 0.9
I0626 02:46:43.936369 27075 solver.cpp:192] Iteration 45000, loss = 0.00537346
I0626 02:46:43.936534 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00537346 (* 1 = 0.00537346 loss)
I0626 02:46:43.936553 27075 solver.cpp:408] Iteration 45000, lr = 0.001, mom = 0.9
I0626 02:46:55.057570 27075 solver.cpp:192] Iteration 46000, loss = 0.00528382
I0626 02:46:55.057658 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00528382 (* 1 = 0.00528382 loss)
I0626 02:46:55.057682 27075 solver.cpp:408] Iteration 46000, lr = 0.001, mom = 0.9
I0626 02:47:06.506980 27075 solver.cpp:192] Iteration 47000, loss = 0.00522635
I0626 02:47:06.507045 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00522635 (* 1 = 0.00522635 loss)
I0626 02:47:06.507061 27075 solver.cpp:408] Iteration 47000, lr = 0.001, mom = 0.9
I0626 02:47:17.971117 27075 solver.cpp:192] Iteration 48000, loss = 0.00526861
I0626 02:47:17.971302 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.00526861 (* 1 = 0.00526861 loss)
I0626 02:47:17.971325 27075 solver.cpp:408] Iteration 48000, lr = 0.001, mom = 0.9
I0626 02:47:29.382370 27075 solver.cpp:192] Iteration 49000, loss = 0.0051416
I0626 02:47:29.382458 27075 solver.cpp:207]     Train net output #0: pt_loss2 = 0.0051416 (* 1 = 0.0051416 loss)
I0626 02:47:29.382483 27075 solver.cpp:408] Iteration 49000, lr = 0.001, mom = 0.9
I0626 02:47:40.654357 27075 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_50000.caffemodel
I0626 02:47:40.682924 27075 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_50000.solverstate
I0626 02:47:40.703991 27075 solver.cpp:229] Iteration 50000, loss = 0.00473809
I0626 02:47:40.704037 27075 solver.cpp:248] Iteration 50000, Testing net (#0)
I0626 02:47:40.882166 27075 solver.cpp:299]     Test net output #0: pt_loss2 = 0.00174735 (* 1 = 0.00174735 loss)
I0626 02:47:40.882228 27075 solver.cpp:234] Optimization Done.
I0626 02:47:40.882247 27075 caffe.cpp:121] Optimization Done.
I0626 02:47:40.931342  8898 net.cpp:277] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0626 02:47:40.931470  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/test"
    batch_size: 100
    backend: LEVELDB
  }
  include {
    phase: TEST
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner1_drop"
  name: "inner1_dropdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner1_drop"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner2"
  top: "d_inner1"
  name: "d_inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner1"
  top: "d_inner1"
  name: "d_inner1relu"
  type: RELU
}
layers {
  bottom: "d_inner1"
  bottom: "inner1"
  top: "pt_loss2"
  name: "pt_loss2"
  type: EUCLIDEAN_LOSS
}
I0626 02:47:40.931547  8898 net.cpp:67] Creating Layer data
I0626 02:47:40.931560  8898 net.cpp:358] data -> data
I0626 02:47:40.931574  8898 net.cpp:96] Setting up data
I0626 02:47:40.931586  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/test
I0626 02:47:41.021270  8898 data_layer.cpp:145] output data size: 100,3600,1,1
I0626 02:47:41.022017  8898 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 02:47:41.022056  8898 net.cpp:67] Creating Layer inner1
I0626 02:47:41.022075  8898 net.cpp:396] inner1 <- data
I0626 02:47:41.022110  8898 net.cpp:358] inner1 -> inner1
I0626 02:47:41.022137  8898 net.cpp:96] Setting up inner1
I0626 02:47:41.049968  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.050019  8898 net.cpp:67] Creating Layer inner1relu
I0626 02:47:41.050029  8898 net.cpp:396] inner1relu <- inner1
I0626 02:47:41.050040  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 02:47:41.050050  8898 net.cpp:96] Setting up inner1relu
I0626 02:47:41.050060  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.050068  8898 net.cpp:67] Creating Layer inner1_inner1relu_0_split
I0626 02:47:41.050076  8898 net.cpp:396] inner1_inner1relu_0_split <- inner1
I0626 02:47:41.050086  8898 net.cpp:358] inner1_inner1relu_0_split -> inner1_inner1relu_0_split_0
I0626 02:47:41.050096  8898 net.cpp:358] inner1_inner1relu_0_split -> inner1_inner1relu_0_split_1
I0626 02:47:41.050107  8898 net.cpp:96] Setting up inner1_inner1relu_0_split
I0626 02:47:41.050115  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.050123  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.050134  8898 net.cpp:67] Creating Layer inner1_dropdrop
I0626 02:47:41.050143  8898 net.cpp:396] inner1_dropdrop <- inner1_inner1relu_0_split_0
I0626 02:47:41.050153  8898 net.cpp:358] inner1_dropdrop -> inner1_drop
I0626 02:47:41.050163  8898 net.cpp:96] Setting up inner1_dropdrop
I0626 02:47:41.050173  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.050184  8898 net.cpp:67] Creating Layer inner2
I0626 02:47:41.050191  8898 net.cpp:396] inner2 <- inner1_drop
I0626 02:47:41.050204  8898 net.cpp:358] inner2 -> inner2
I0626 02:47:41.050213  8898 net.cpp:96] Setting up inner2
I0626 02:47:41.053202  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.053220  8898 net.cpp:67] Creating Layer inner2relu
I0626 02:47:41.053228  8898 net.cpp:396] inner2relu <- inner2
I0626 02:47:41.053237  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 02:47:41.053246  8898 net.cpp:96] Setting up inner2relu
I0626 02:47:41.053254  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.053263  8898 net.cpp:67] Creating Layer inner2drop
I0626 02:47:41.053270  8898 net.cpp:396] inner2drop <- inner2
I0626 02:47:41.053280  8898 net.cpp:347] inner2drop -> inner2 (in-place)
I0626 02:47:41.053289  8898 net.cpp:96] Setting up inner2drop
I0626 02:47:41.053297  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.053308  8898 net.cpp:67] Creating Layer d_inner1
I0626 02:47:41.053316  8898 net.cpp:396] d_inner1 <- inner2
I0626 02:47:41.053326  8898 net.cpp:358] d_inner1 -> d_inner1
I0626 02:47:41.053335  8898 net.cpp:96] Setting up d_inner1
I0626 02:47:41.056267  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.056284  8898 net.cpp:67] Creating Layer d_inner1relu
I0626 02:47:41.056293  8898 net.cpp:396] d_inner1relu <- d_inner1
I0626 02:47:41.056303  8898 net.cpp:347] d_inner1relu -> d_inner1 (in-place)
I0626 02:47:41.056311  8898 net.cpp:96] Setting up d_inner1relu
I0626 02:47:41.056318  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.056329  8898 net.cpp:67] Creating Layer pt_loss2
I0626 02:47:41.056336  8898 net.cpp:396] pt_loss2 <- d_inner1
I0626 02:47:41.056344  8898 net.cpp:396] pt_loss2 <- inner1_inner1relu_0_split_1
I0626 02:47:41.056352  8898 net.cpp:358] pt_loss2 -> pt_loss2
I0626 02:47:41.056361  8898 net.cpp:96] Setting up pt_loss2
I0626 02:47:41.056371  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 02:47:41.056380  8898 net.cpp:109]     with loss weight 1
I0626 02:47:41.056396  8898 net.cpp:170] pt_loss2 needs backward computation.
I0626 02:47:41.056403  8898 net.cpp:170] d_inner1relu needs backward computation.
I0626 02:47:41.056411  8898 net.cpp:170] d_inner1 needs backward computation.
I0626 02:47:41.056418  8898 net.cpp:170] inner2drop needs backward computation.
I0626 02:47:41.056426  8898 net.cpp:170] inner2relu needs backward computation.
I0626 02:47:41.056432  8898 net.cpp:170] inner2 needs backward computation.
I0626 02:47:41.056439  8898 net.cpp:172] inner1_dropdrop does not need backward computation.
I0626 02:47:41.056458  8898 net.cpp:172] inner1_inner1relu_0_split does not need backward computation.
I0626 02:47:41.056466  8898 net.cpp:172] inner1relu does not need backward computation.
I0626 02:47:41.056473  8898 net.cpp:172] inner1 does not need backward computation.
I0626 02:47:41.056481  8898 net.cpp:172] data does not need backward computation.
I0626 02:47:41.056488  8898 net.cpp:208] This network produces output pt_loss2
I0626 02:47:41.056501  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 02:47:41.056510  8898 net.cpp:219] Network initialization done.
I0626 02:47:41.056517  8898 net.cpp:220] Memory required for data: 3440004
I0626 02:47:41.067049  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 02:47:41.067162  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/train"
    batch_size: 256
    backend: LEVELDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner2_drop"
  name: "inner2_dropdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner2_drop"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner3"
  top: "d_inner2"
  name: "d_inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner2"
  top: "d_inner2"
  name: "d_inner2relu"
  type: RELU
}
layers {
  bottom: "d_inner2"
  bottom: "inner2"
  top: "pt_loss3"
  name: "pt_loss3"
  type: EUCLIDEAN_LOSS
}
I0626 02:47:41.067217  8898 net.cpp:67] Creating Layer data
I0626 02:47:41.067229  8898 net.cpp:358] data -> data
I0626 02:47:41.067243  8898 net.cpp:96] Setting up data
I0626 02:47:41.067252  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/train
I0626 02:47:41.163672  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 02:47:41.165410  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 02:47:41.165460  8898 net.cpp:67] Creating Layer inner1
I0626 02:47:41.165479  8898 net.cpp:396] inner1 <- data
I0626 02:47:41.165503  8898 net.cpp:358] inner1 -> inner1
I0626 02:47:41.165529  8898 net.cpp:96] Setting up inner1
I0626 02:47:41.191884  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.191932  8898 net.cpp:67] Creating Layer inner1relu
I0626 02:47:41.191942  8898 net.cpp:396] inner1relu <- inner1
I0626 02:47:41.191952  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 02:47:41.191974  8898 net.cpp:96] Setting up inner1relu
I0626 02:47:41.191983  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.191994  8898 net.cpp:67] Creating Layer inner2
I0626 02:47:41.192001  8898 net.cpp:396] inner2 <- inner1
I0626 02:47:41.192010  8898 net.cpp:358] inner2 -> inner2
I0626 02:47:41.192021  8898 net.cpp:96] Setting up inner2
I0626 02:47:41.194779  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.194795  8898 net.cpp:67] Creating Layer inner2relu
I0626 02:47:41.194803  8898 net.cpp:396] inner2relu <- inner2
I0626 02:47:41.194813  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 02:47:41.194821  8898 net.cpp:96] Setting up inner2relu
I0626 02:47:41.194828  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.194838  8898 net.cpp:67] Creating Layer inner2_inner2relu_0_split
I0626 02:47:41.194844  8898 net.cpp:396] inner2_inner2relu_0_split <- inner2
I0626 02:47:41.194854  8898 net.cpp:358] inner2_inner2relu_0_split -> inner2_inner2relu_0_split_0
I0626 02:47:41.194864  8898 net.cpp:358] inner2_inner2relu_0_split -> inner2_inner2relu_0_split_1
I0626 02:47:41.194874  8898 net.cpp:96] Setting up inner2_inner2relu_0_split
I0626 02:47:41.194882  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.194890  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.194900  8898 net.cpp:67] Creating Layer inner2_dropdrop
I0626 02:47:41.194907  8898 net.cpp:396] inner2_dropdrop <- inner2_inner2relu_0_split_0
I0626 02:47:41.194917  8898 net.cpp:358] inner2_dropdrop -> inner2_drop
I0626 02:47:41.194926  8898 net.cpp:96] Setting up inner2_dropdrop
I0626 02:47:41.194936  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.194944  8898 net.cpp:67] Creating Layer inner3
I0626 02:47:41.194952  8898 net.cpp:396] inner3 <- inner2_drop
I0626 02:47:41.194962  8898 net.cpp:358] inner3 -> inner3
I0626 02:47:41.194972  8898 net.cpp:96] Setting up inner3
I0626 02:47:41.206534  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:47:41.206569  8898 net.cpp:67] Creating Layer inner3relu
I0626 02:47:41.206578  8898 net.cpp:396] inner3relu <- inner3
I0626 02:47:41.206588  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 02:47:41.206598  8898 net.cpp:96] Setting up inner3relu
I0626 02:47:41.206605  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:47:41.206614  8898 net.cpp:67] Creating Layer inner3drop
I0626 02:47:41.206621  8898 net.cpp:396] inner3drop <- inner3
I0626 02:47:41.206630  8898 net.cpp:347] inner3drop -> inner3 (in-place)
I0626 02:47:41.206640  8898 net.cpp:96] Setting up inner3drop
I0626 02:47:41.206647  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:47:41.206657  8898 net.cpp:67] Creating Layer d_inner2
I0626 02:47:41.206665  8898 net.cpp:396] d_inner2 <- inner3
I0626 02:47:41.206673  8898 net.cpp:358] d_inner2 -> d_inner2
I0626 02:47:41.206683  8898 net.cpp:96] Setting up d_inner2
I0626 02:47:41.218304  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.218338  8898 net.cpp:67] Creating Layer d_inner2relu
I0626 02:47:41.218346  8898 net.cpp:396] d_inner2relu <- d_inner2
I0626 02:47:41.218356  8898 net.cpp:347] d_inner2relu -> d_inner2 (in-place)
I0626 02:47:41.218366  8898 net.cpp:96] Setting up d_inner2relu
I0626 02:47:41.218374  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.218384  8898 net.cpp:67] Creating Layer pt_loss3
I0626 02:47:41.218390  8898 net.cpp:396] pt_loss3 <- d_inner2
I0626 02:47:41.218400  8898 net.cpp:396] pt_loss3 <- inner2_inner2relu_0_split_1
I0626 02:47:41.218408  8898 net.cpp:358] pt_loss3 -> pt_loss3
I0626 02:47:41.218420  8898 net.cpp:96] Setting up pt_loss3
I0626 02:47:41.218430  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 02:47:41.218437  8898 net.cpp:109]     with loss weight 1
I0626 02:47:41.218454  8898 net.cpp:170] pt_loss3 needs backward computation.
I0626 02:47:41.218462  8898 net.cpp:170] d_inner2relu needs backward computation.
I0626 02:47:41.218469  8898 net.cpp:170] d_inner2 needs backward computation.
I0626 02:47:41.218477  8898 net.cpp:170] inner3drop needs backward computation.
I0626 02:47:41.218495  8898 net.cpp:170] inner3relu needs backward computation.
I0626 02:47:41.218503  8898 net.cpp:170] inner3 needs backward computation.
I0626 02:47:41.218510  8898 net.cpp:172] inner2_dropdrop does not need backward computation.
I0626 02:47:41.218518  8898 net.cpp:172] inner2_inner2relu_0_split does not need backward computation.
I0626 02:47:41.218526  8898 net.cpp:172] inner2relu does not need backward computation.
I0626 02:47:41.218533  8898 net.cpp:172] inner2 does not need backward computation.
I0626 02:47:41.218541  8898 net.cpp:172] inner1relu does not need backward computation.
I0626 02:47:41.218549  8898 net.cpp:172] inner1 does not need backward computation.
I0626 02:47:41.218556  8898 net.cpp:172] data does not need backward computation.
I0626 02:47:41.218564  8898 net.cpp:208] This network produces output pt_loss3
I0626 02:47:41.218575  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 02:47:41.218585  8898 net.cpp:219] Network initialization done.
I0626 02:47:41.218592  8898 net.cpp:220] Memory required for data: 14438404
I0626 02:47:41.408319 12773 caffe.cpp:99] Use GPU with device ID 0
I0626 02:47:41.649231 12773 caffe.cpp:107] Starting Optimization
I0626 02:47:41.649327 12773 solver.cpp:32] Initializing solver from parameters: 
test_iter: 100
test_interval: 10000
base_lr: 0.1
display: 1000
max_iter: 50000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 20000
snapshot: 10000
snapshot_prefix: "modules/image30x30_dim50/exp/save"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/stack_net.prototxt"
snapshot_after_train: true
momentum_burnin: 1000
I0626 02:47:41.649353 12773 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/stack_net.prototxt
I0626 02:47:41.649525 12773 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 02:47:41.649615 12773 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/train"
    batch_size: 256
    backend: LEVELDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner2_drop"
  name: "inner2_dropdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner2_drop"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner3"
  top: "d_inner2"
  name: "d_inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner2"
  top: "d_inner2"
  name: "d_inner2relu"
  type: RELU
}
layers {
  bottom: "d_inner2"
  bottom: "inner2"
  top: "pt_loss3"
  name: "pt_loss3"
  type: EUCLIDEAN_LOSS
}
state {
  phase: TRAIN
}
I0626 02:47:41.649700 12773 net.cpp:67] Creating Layer data
I0626 02:47:41.649713 12773 net.cpp:358] data -> data
I0626 02:47:41.649730 12773 net.cpp:96] Setting up data
I0626 02:47:41.649765 12773 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/train
I0626 02:47:41.756192 12773 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 02:47:41.756366 12773 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 02:47:41.756398 12773 net.cpp:67] Creating Layer inner1
I0626 02:47:41.756415 12773 net.cpp:396] inner1 <- data
I0626 02:47:41.756434 12773 net.cpp:358] inner1 -> inner1
I0626 02:47:41.756461 12773 net.cpp:96] Setting up inner1
I0626 02:47:41.783819 12773 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.783874 12773 net.cpp:67] Creating Layer inner1relu
I0626 02:47:41.783885 12773 net.cpp:396] inner1relu <- inner1
I0626 02:47:41.783895 12773 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 02:47:41.783906 12773 net.cpp:96] Setting up inner1relu
I0626 02:47:41.783941 12773 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.783951 12773 net.cpp:67] Creating Layer inner2
I0626 02:47:41.783959 12773 net.cpp:396] inner2 <- inner1
I0626 02:47:41.783969 12773 net.cpp:358] inner2 -> inner2
I0626 02:47:41.783980 12773 net.cpp:96] Setting up inner2
I0626 02:47:41.786975 12773 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.786993 12773 net.cpp:67] Creating Layer inner2relu
I0626 02:47:41.787000 12773 net.cpp:396] inner2relu <- inner2
I0626 02:47:41.787009 12773 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 02:47:41.787019 12773 net.cpp:96] Setting up inner2relu
I0626 02:47:41.787026 12773 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.787035 12773 net.cpp:67] Creating Layer inner2_inner2relu_0_split
I0626 02:47:41.787044 12773 net.cpp:396] inner2_inner2relu_0_split <- inner2
I0626 02:47:41.787052 12773 net.cpp:358] inner2_inner2relu_0_split -> inner2_inner2relu_0_split_0
I0626 02:47:41.787063 12773 net.cpp:358] inner2_inner2relu_0_split -> inner2_inner2relu_0_split_1
I0626 02:47:41.787073 12773 net.cpp:96] Setting up inner2_inner2relu_0_split
I0626 02:47:41.787086 12773 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.787096 12773 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.787106 12773 net.cpp:67] Creating Layer inner2_dropdrop
I0626 02:47:41.787112 12773 net.cpp:396] inner2_dropdrop <- inner2_inner2relu_0_split_0
I0626 02:47:41.787130 12773 net.cpp:358] inner2_dropdrop -> inner2_drop
I0626 02:47:41.787142 12773 net.cpp:96] Setting up inner2_dropdrop
I0626 02:47:41.787151 12773 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.787161 12773 net.cpp:67] Creating Layer inner3
I0626 02:47:41.787169 12773 net.cpp:396] inner3 <- inner2_drop
I0626 02:47:41.787180 12773 net.cpp:358] inner3 -> inner3
I0626 02:47:41.787190 12773 net.cpp:96] Setting up inner3
I0626 02:47:41.799114 12773 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:47:41.799149 12773 net.cpp:67] Creating Layer inner3relu
I0626 02:47:41.799157 12773 net.cpp:396] inner3relu <- inner3
I0626 02:47:41.799168 12773 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 02:47:41.799180 12773 net.cpp:96] Setting up inner3relu
I0626 02:47:41.799187 12773 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:47:41.799196 12773 net.cpp:67] Creating Layer inner3drop
I0626 02:47:41.799204 12773 net.cpp:396] inner3drop <- inner3
I0626 02:47:41.799213 12773 net.cpp:347] inner3drop -> inner3 (in-place)
I0626 02:47:41.799222 12773 net.cpp:96] Setting up inner3drop
I0626 02:47:41.799230 12773 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:47:41.799242 12773 net.cpp:67] Creating Layer d_inner2
I0626 02:47:41.799250 12773 net.cpp:396] d_inner2 <- inner3
I0626 02:47:41.799259 12773 net.cpp:358] d_inner2 -> d_inner2
I0626 02:47:41.799269 12773 net.cpp:96] Setting up d_inner2
I0626 02:47:41.811031 12773 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.811058 12773 net.cpp:67] Creating Layer d_inner2relu
I0626 02:47:41.811066 12773 net.cpp:396] d_inner2relu <- d_inner2
I0626 02:47:41.811077 12773 net.cpp:347] d_inner2relu -> d_inner2 (in-place)
I0626 02:47:41.811087 12773 net.cpp:96] Setting up d_inner2relu
I0626 02:47:41.811095 12773 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:47:41.811105 12773 net.cpp:67] Creating Layer pt_loss3
I0626 02:47:41.811112 12773 net.cpp:396] pt_loss3 <- d_inner2
I0626 02:47:41.811120 12773 net.cpp:396] pt_loss3 <- inner2_inner2relu_0_split_1
I0626 02:47:41.811131 12773 net.cpp:358] pt_loss3 -> pt_loss3
I0626 02:47:41.811142 12773 net.cpp:96] Setting up pt_loss3
I0626 02:47:41.811157 12773 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 02:47:41.811167 12773 net.cpp:109]     with loss weight 1
I0626 02:47:41.811195 12773 net.cpp:170] pt_loss3 needs backward computation.
I0626 02:47:41.811203 12773 net.cpp:170] d_inner2relu needs backward computation.
I0626 02:47:41.811211 12773 net.cpp:170] d_inner2 needs backward computation.
I0626 02:47:41.811219 12773 net.cpp:170] inner3drop needs backward computation.
I0626 02:47:41.811237 12773 net.cpp:170] inner3relu needs backward computation.
I0626 02:47:41.811244 12773 net.cpp:170] inner3 needs backward computation.
I0626 02:47:41.811254 12773 net.cpp:172] inner2_dropdrop does not need backward computation.
I0626 02:47:41.811261 12773 net.cpp:172] inner2_inner2relu_0_split does not need backward computation.
I0626 02:47:41.811269 12773 net.cpp:172] inner2relu does not need backward computation.
I0626 02:47:41.811276 12773 net.cpp:172] inner2 does not need backward computation.
I0626 02:47:41.811285 12773 net.cpp:172] inner1relu does not need backward computation.
I0626 02:47:41.811291 12773 net.cpp:172] inner1 does not need backward computation.
I0626 02:47:41.811300 12773 net.cpp:172] data does not need backward computation.
I0626 02:47:41.811306 12773 net.cpp:208] This network produces output pt_loss3
I0626 02:47:41.811322 12773 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 02:47:41.811332 12773 net.cpp:219] Network initialization done.
I0626 02:47:41.811345 12773 net.cpp:220] Memory required for data: 14438404
I0626 02:47:41.811522 12773 solver.cpp:151] Creating test net (#0) specified by net file: modules/image30x30_dim50/stack_net.prototxt
I0626 02:47:41.811547 12773 net.cpp:277] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0626 02:47:41.811642 12773 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/test"
    batch_size: 100
    backend: LEVELDB
  }
  include {
    phase: TEST
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner2_drop"
  name: "inner2_dropdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner2_drop"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner3"
  top: "d_inner2"
  name: "d_inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner2"
  top: "d_inner2"
  name: "d_inner2relu"
  type: RELU
}
layers {
  bottom: "d_inner2"
  bottom: "inner2"
  top: "pt_loss3"
  name: "pt_loss3"
  type: EUCLIDEAN_LOSS
}
state {
  phase: TEST
}
I0626 02:47:41.811702 12773 net.cpp:67] Creating Layer data
I0626 02:47:41.811712 12773 net.cpp:358] data -> data
I0626 02:47:41.811723 12773 net.cpp:96] Setting up data
I0626 02:47:41.811733 12773 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/test
I0626 02:47:41.906770 12773 data_layer.cpp:145] output data size: 100,3600,1,1
I0626 02:47:41.907006 12773 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 02:47:41.907058 12773 net.cpp:67] Creating Layer inner1
I0626 02:47:41.907081 12773 net.cpp:396] inner1 <- data
I0626 02:47:41.907099 12773 net.cpp:358] inner1 -> inner1
I0626 02:47:41.907126 12773 net.cpp:96] Setting up inner1
I0626 02:47:41.936657 12773 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.936707 12773 net.cpp:67] Creating Layer inner1relu
I0626 02:47:41.936715 12773 net.cpp:396] inner1relu <- inner1
I0626 02:47:41.936725 12773 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 02:47:41.936736 12773 net.cpp:96] Setting up inner1relu
I0626 02:47:41.936745 12773 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.936755 12773 net.cpp:67] Creating Layer inner2
I0626 02:47:41.936764 12773 net.cpp:396] inner2 <- inner1
I0626 02:47:41.936772 12773 net.cpp:358] inner2 -> inner2
I0626 02:47:41.936784 12773 net.cpp:96] Setting up inner2
I0626 02:47:41.939741 12773 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.939759 12773 net.cpp:67] Creating Layer inner2relu
I0626 02:47:41.939769 12773 net.cpp:396] inner2relu <- inner2
I0626 02:47:41.939776 12773 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 02:47:41.939786 12773 net.cpp:96] Setting up inner2relu
I0626 02:47:41.939793 12773 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.939805 12773 net.cpp:67] Creating Layer inner2_inner2relu_0_split
I0626 02:47:41.939812 12773 net.cpp:396] inner2_inner2relu_0_split <- inner2
I0626 02:47:41.939822 12773 net.cpp:358] inner2_inner2relu_0_split -> inner2_inner2relu_0_split_0
I0626 02:47:41.939832 12773 net.cpp:358] inner2_inner2relu_0_split -> inner2_inner2relu_0_split_1
I0626 02:47:41.939844 12773 net.cpp:96] Setting up inner2_inner2relu_0_split
I0626 02:47:41.939853 12773 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.939862 12773 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.939872 12773 net.cpp:67] Creating Layer inner2_dropdrop
I0626 02:47:41.939879 12773 net.cpp:396] inner2_dropdrop <- inner2_inner2relu_0_split_0
I0626 02:47:41.939889 12773 net.cpp:358] inner2_dropdrop -> inner2_drop
I0626 02:47:41.939898 12773 net.cpp:96] Setting up inner2_dropdrop
I0626 02:47:41.939908 12773 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.939918 12773 net.cpp:67] Creating Layer inner3
I0626 02:47:41.939926 12773 net.cpp:396] inner3 <- inner2_drop
I0626 02:47:41.939936 12773 net.cpp:358] inner3 -> inner3
I0626 02:47:41.939947 12773 net.cpp:96] Setting up inner3
I0626 02:47:41.951675 12773 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 02:47:41.951710 12773 net.cpp:67] Creating Layer inner3relu
I0626 02:47:41.951719 12773 net.cpp:396] inner3relu <- inner3
I0626 02:47:41.951730 12773 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 02:47:41.951740 12773 net.cpp:96] Setting up inner3relu
I0626 02:47:41.951748 12773 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 02:47:41.951767 12773 net.cpp:67] Creating Layer inner3drop
I0626 02:47:41.951776 12773 net.cpp:396] inner3drop <- inner3
I0626 02:47:41.951784 12773 net.cpp:347] inner3drop -> inner3 (in-place)
I0626 02:47:41.951793 12773 net.cpp:96] Setting up inner3drop
I0626 02:47:41.951802 12773 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 02:47:41.951814 12773 net.cpp:67] Creating Layer d_inner2
I0626 02:47:41.951822 12773 net.cpp:396] d_inner2 <- inner3
I0626 02:47:41.951831 12773 net.cpp:358] d_inner2 -> d_inner2
I0626 02:47:41.951843 12773 net.cpp:96] Setting up d_inner2
I0626 02:47:41.963739 12773 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.963766 12773 net.cpp:67] Creating Layer d_inner2relu
I0626 02:47:41.963774 12773 net.cpp:396] d_inner2relu <- d_inner2
I0626 02:47:41.963785 12773 net.cpp:347] d_inner2relu -> d_inner2 (in-place)
I0626 02:47:41.963796 12773 net.cpp:96] Setting up d_inner2relu
I0626 02:47:41.963804 12773 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:47:41.963814 12773 net.cpp:67] Creating Layer pt_loss3
I0626 02:47:41.963820 12773 net.cpp:396] pt_loss3 <- d_inner2
I0626 02:47:41.963829 12773 net.cpp:396] pt_loss3 <- inner2_inner2relu_0_split_1
I0626 02:47:41.963850 12773 net.cpp:358] pt_loss3 -> pt_loss3
I0626 02:47:41.963863 12773 net.cpp:96] Setting up pt_loss3
I0626 02:47:41.963874 12773 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 02:47:41.963882 12773 net.cpp:109]     with loss weight 1
I0626 02:47:41.963898 12773 net.cpp:170] pt_loss3 needs backward computation.
I0626 02:47:41.963907 12773 net.cpp:170] d_inner2relu needs backward computation.
I0626 02:47:41.963914 12773 net.cpp:170] d_inner2 needs backward computation.
I0626 02:47:41.963922 12773 net.cpp:170] inner3drop needs backward computation.
I0626 02:47:41.963929 12773 net.cpp:170] inner3relu needs backward computation.
I0626 02:47:41.963937 12773 net.cpp:170] inner3 needs backward computation.
I0626 02:47:41.963944 12773 net.cpp:172] inner2_dropdrop does not need backward computation.
I0626 02:47:41.963953 12773 net.cpp:172] inner2_inner2relu_0_split does not need backward computation.
I0626 02:47:41.963960 12773 net.cpp:172] inner2relu does not need backward computation.
I0626 02:47:41.963968 12773 net.cpp:172] inner2 does not need backward computation.
I0626 02:47:41.963975 12773 net.cpp:172] inner1relu does not need backward computation.
I0626 02:47:41.963982 12773 net.cpp:172] inner1 does not need backward computation.
I0626 02:47:41.963990 12773 net.cpp:172] data does not need backward computation.
I0626 02:47:41.963997 12773 net.cpp:208] This network produces output pt_loss3
I0626 02:47:41.964012 12773 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 02:47:41.964023 12773 net.cpp:219] Network initialization done.
I0626 02:47:41.964030 12773 net.cpp:220] Memory required for data: 5640004
I0626 02:47:41.964067 12773 solver.cpp:41] Solver scaffolding done.
I0626 02:47:41.964076 12773 caffe.cpp:115] Finetuning from modules/image30x30_dim50/stack_init.caffemodel
I0626 02:47:41.977715 12773 solver.cpp:160] Solving net
I0626 02:47:41.977767 12773 solver.cpp:248] Iteration 0, Testing net (#0)
I0626 02:47:42.167382 12773 solver.cpp:299]     Test net output #0: pt_loss3 = 0.0356798 (* 1 = 0.0356798 loss)
I0626 02:47:42.176872 12773 solver.cpp:192] Iteration 0, loss = 0.0322319
I0626 02:47:42.176935 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.0322319 (* 1 = 0.0322319 loss)
I0626 02:47:42.176959 12773 solver.cpp:408] Iteration 0, lr = 0.1, mom = 0
I0626 02:47:53.520337 12773 solver.cpp:192] Iteration 1000, loss = 0.017234
I0626 02:47:53.520406 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.017234 (* 1 = 0.017234 loss)
I0626 02:47:53.520422 12773 solver.cpp:408] Iteration 1000, lr = 0.1, mom = 0.9
I0626 02:48:04.919101 12773 solver.cpp:192] Iteration 2000, loss = 0.00802336
I0626 02:48:04.919188 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00802336 (* 1 = 0.00802336 loss)
I0626 02:48:04.919210 12773 solver.cpp:408] Iteration 2000, lr = 0.1, mom = 0.9
I0626 02:48:16.316658 12773 solver.cpp:192] Iteration 3000, loss = 0.00443549
I0626 02:48:16.316784 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00443549 (* 1 = 0.00443549 loss)
I0626 02:48:16.316803 12773 solver.cpp:408] Iteration 3000, lr = 0.1, mom = 0.9
I0626 02:48:27.756466 12773 solver.cpp:192] Iteration 4000, loss = 0.00325842
I0626 02:48:27.756530 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00325842 (* 1 = 0.00325842 loss)
I0626 02:48:27.756543 12773 solver.cpp:408] Iteration 4000, lr = 0.1, mom = 0.9
I0626 02:48:39.135177 12773 solver.cpp:192] Iteration 5000, loss = 0.00272802
I0626 02:48:39.135258 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00272802 (* 1 = 0.00272802 loss)
I0626 02:48:39.135282 12773 solver.cpp:408] Iteration 5000, lr = 0.1, mom = 0.9
I0626 02:48:50.559523 12773 solver.cpp:192] Iteration 6000, loss = 0.00243581
I0626 02:48:50.559653 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00243581 (* 1 = 0.00243581 loss)
I0626 02:48:50.559677 12773 solver.cpp:408] Iteration 6000, lr = 0.1, mom = 0.9
I0626 02:49:01.923297 12773 solver.cpp:192] Iteration 7000, loss = 0.00234573
I0626 02:49:01.923380 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00234573 (* 1 = 0.00234573 loss)
I0626 02:49:01.923404 12773 solver.cpp:408] Iteration 7000, lr = 0.1, mom = 0.9
I0626 02:49:13.289793 12773 solver.cpp:192] Iteration 8000, loss = 0.00225486
I0626 02:49:13.289873 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00225486 (* 1 = 0.00225486 loss)
I0626 02:49:13.289894 12773 solver.cpp:408] Iteration 8000, lr = 0.1, mom = 0.9
I0626 02:49:24.616353 12773 solver.cpp:192] Iteration 9000, loss = 0.00218325
I0626 02:49:24.616487 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00218325 (* 1 = 0.00218325 loss)
I0626 02:49:24.616502 12773 solver.cpp:408] Iteration 9000, lr = 0.1, mom = 0.9
I0626 02:49:35.938207 12773 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_10000.caffemodel
I0626 02:49:35.990592 12773 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_10000.solverstate
I0626 02:49:36.023504 12773 solver.cpp:248] Iteration 10000, Testing net (#0)
I0626 02:49:36.188741 12773 solver.cpp:299]     Test net output #0: pt_loss3 = 0.000447421 (* 1 = 0.000447421 loss)
I0626 02:49:36.190543 12773 solver.cpp:192] Iteration 10000, loss = 0.00215524
I0626 02:49:36.190605 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00215524 (* 1 = 0.00215524 loss)
I0626 02:49:36.190629 12773 solver.cpp:408] Iteration 10000, lr = 0.1, mom = 0.9
I0626 02:49:47.545915 12773 solver.cpp:192] Iteration 11000, loss = 0.00221155
I0626 02:49:47.545980 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00221155 (* 1 = 0.00221155 loss)
I0626 02:49:47.545994 12773 solver.cpp:408] Iteration 11000, lr = 0.1, mom = 0.9
I0626 02:49:58.851828 12773 solver.cpp:192] Iteration 12000, loss = 0.0020821
I0626 02:49:58.851933 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.0020821 (* 1 = 0.0020821 loss)
I0626 02:49:58.851953 12773 solver.cpp:408] Iteration 12000, lr = 0.1, mom = 0.9
I0626 02:50:10.256767 12773 solver.cpp:192] Iteration 13000, loss = 0.0019898
I0626 02:50:10.256839 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.0019898 (* 1 = 0.0019898 loss)
I0626 02:50:10.256855 12773 solver.cpp:408] Iteration 13000, lr = 0.1, mom = 0.9
I0626 02:50:21.655643 12773 solver.cpp:192] Iteration 14000, loss = 0.00188529
I0626 02:50:21.655720 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00188529 (* 1 = 0.00188529 loss)
I0626 02:50:21.655741 12773 solver.cpp:408] Iteration 14000, lr = 0.1, mom = 0.9
I0626 02:50:33.090553 12773 solver.cpp:192] Iteration 15000, loss = 0.00182228
I0626 02:50:33.090754 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00182228 (* 1 = 0.00182228 loss)
I0626 02:50:33.090787 12773 solver.cpp:408] Iteration 15000, lr = 0.1, mom = 0.9
I0626 02:50:44.447108 12773 solver.cpp:192] Iteration 16000, loss = 0.00172033
I0626 02:50:44.447217 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00172033 (* 1 = 0.00172033 loss)
I0626 02:50:44.447243 12773 solver.cpp:408] Iteration 16000, lr = 0.1, mom = 0.9
I0626 02:50:55.835918 12773 solver.cpp:192] Iteration 17000, loss = 0.00169101
I0626 02:50:55.835983 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00169101 (* 1 = 0.00169101 loss)
I0626 02:50:55.835995 12773 solver.cpp:408] Iteration 17000, lr = 0.1, mom = 0.9
I0626 02:51:07.120487 12773 solver.cpp:192] Iteration 18000, loss = 0.00168202
I0626 02:51:07.120654 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00168202 (* 1 = 0.00168202 loss)
I0626 02:51:07.120674 12773 solver.cpp:408] Iteration 18000, lr = 0.1, mom = 0.9
I0626 02:51:18.453866 12773 solver.cpp:192] Iteration 19000, loss = 0.00161322
I0626 02:51:18.453960 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00161322 (* 1 = 0.00161322 loss)
I0626 02:51:18.453995 12773 solver.cpp:408] Iteration 19000, lr = 0.1, mom = 0.9
I0626 02:51:29.972455 12773 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_20000.caffemodel
I0626 02:51:30.017242 12773 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_20000.solverstate
I0626 02:51:30.050397 12773 solver.cpp:248] Iteration 20000, Testing net (#0)
I0626 02:51:30.215343 12773 solver.cpp:299]     Test net output #0: pt_loss3 = 0.000316534 (* 1 = 0.000316534 loss)
I0626 02:51:30.216982 12773 solver.cpp:192] Iteration 20000, loss = 0.00161251
I0626 02:51:30.217036 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00161251 (* 1 = 0.00161251 loss)
I0626 02:51:30.217053 12773 solver.cpp:408] Iteration 20000, lr = 0.01, mom = 0.9
I0626 02:51:41.468173 12773 solver.cpp:192] Iteration 21000, loss = 0.00151684
I0626 02:51:41.468452 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00151684 (* 1 = 0.00151684 loss)
I0626 02:51:41.468475 12773 solver.cpp:408] Iteration 21000, lr = 0.01, mom = 0.9
I0626 02:51:53.008469 12773 solver.cpp:192] Iteration 22000, loss = 0.00144722
I0626 02:51:53.008553 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00144722 (* 1 = 0.00144722 loss)
I0626 02:51:53.008575 12773 solver.cpp:408] Iteration 22000, lr = 0.01, mom = 0.9
I0626 02:52:04.352427 12773 solver.cpp:192] Iteration 23000, loss = 0.00146601
I0626 02:52:04.352501 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00146601 (* 1 = 0.00146601 loss)
I0626 02:52:04.352520 12773 solver.cpp:408] Iteration 23000, lr = 0.01, mom = 0.9
I0626 02:52:15.814846 12773 solver.cpp:192] Iteration 24000, loss = 0.00147253
I0626 02:52:15.815019 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00147253 (* 1 = 0.00147253 loss)
I0626 02:52:15.815039 12773 solver.cpp:408] Iteration 24000, lr = 0.01, mom = 0.9
I0626 02:52:27.182790 12773 solver.cpp:192] Iteration 25000, loss = 0.00151858
I0626 02:52:27.182860 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00151858 (* 1 = 0.00151858 loss)
I0626 02:52:27.182874 12773 solver.cpp:408] Iteration 25000, lr = 0.01, mom = 0.9
I0626 02:52:38.516719 12773 solver.cpp:192] Iteration 26000, loss = 0.00148801
I0626 02:52:38.516782 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00148801 (* 1 = 0.00148801 loss)
I0626 02:52:38.516796 12773 solver.cpp:408] Iteration 26000, lr = 0.01, mom = 0.9
I0626 02:52:49.909462 12773 solver.cpp:192] Iteration 27000, loss = 0.00138723
I0626 02:52:49.909550 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00138723 (* 1 = 0.00138723 loss)
I0626 02:52:49.909564 12773 solver.cpp:408] Iteration 27000, lr = 0.01, mom = 0.9
I0626 02:53:01.364280 12773 solver.cpp:192] Iteration 28000, loss = 0.00147041
I0626 02:53:01.364368 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00147041 (* 1 = 0.00147041 loss)
I0626 02:53:01.364392 12773 solver.cpp:408] Iteration 28000, lr = 0.01, mom = 0.9
I0626 02:53:12.981611 12773 solver.cpp:192] Iteration 29000, loss = 0.00149821
I0626 02:53:12.981689 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00149821 (* 1 = 0.00149821 loss)
I0626 02:53:12.981729 12773 solver.cpp:408] Iteration 29000, lr = 0.01, mom = 0.9
I0626 02:53:24.398484 12773 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_30000.caffemodel
I0626 02:53:24.443739 12773 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_30000.solverstate
I0626 02:53:24.477020 12773 solver.cpp:248] Iteration 30000, Testing net (#0)
I0626 02:53:24.645323 12773 solver.cpp:299]     Test net output #0: pt_loss3 = 0.000310786 (* 1 = 0.000310786 loss)
I0626 02:53:24.646997 12773 solver.cpp:192] Iteration 30000, loss = 0.00152856
I0626 02:53:24.647049 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00152856 (* 1 = 0.00152856 loss)
I0626 02:53:24.647069 12773 solver.cpp:408] Iteration 30000, lr = 0.01, mom = 0.9
I0626 02:53:35.958015 12773 solver.cpp:192] Iteration 31000, loss = 0.00139173
I0626 02:53:35.958103 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00139173 (* 1 = 0.00139173 loss)
I0626 02:53:35.958127 12773 solver.cpp:408] Iteration 31000, lr = 0.01, mom = 0.9
I0626 02:53:47.222618 12773 solver.cpp:192] Iteration 32000, loss = 0.00135616
I0626 02:53:47.222712 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00135616 (* 1 = 0.00135616 loss)
I0626 02:53:47.222736 12773 solver.cpp:408] Iteration 32000, lr = 0.01, mom = 0.9
I0626 02:53:58.597319 12773 solver.cpp:192] Iteration 33000, loss = 0.00130344
I0626 02:53:58.597478 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00130344 (* 1 = 0.00130344 loss)
I0626 02:53:58.597493 12773 solver.cpp:408] Iteration 33000, lr = 0.01, mom = 0.9
I0626 02:54:10.034101 12773 solver.cpp:192] Iteration 34000, loss = 0.0012848
I0626 02:54:10.034168 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.0012848 (* 1 = 0.0012848 loss)
I0626 02:54:10.034181 12773 solver.cpp:408] Iteration 34000, lr = 0.01, mom = 0.9
I0626 02:54:21.428158 12773 solver.cpp:192] Iteration 35000, loss = 0.00124351
I0626 02:54:21.428233 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00124351 (* 1 = 0.00124351 loss)
I0626 02:54:21.428253 12773 solver.cpp:408] Iteration 35000, lr = 0.01, mom = 0.9
I0626 02:54:32.733705 12773 solver.cpp:192] Iteration 36000, loss = 0.00125314
I0626 02:54:32.733829 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00125314 (* 1 = 0.00125314 loss)
I0626 02:54:32.733851 12773 solver.cpp:408] Iteration 36000, lr = 0.01, mom = 0.9
I0626 02:54:44.030201 12773 solver.cpp:192] Iteration 37000, loss = 0.00134284
I0626 02:54:44.030287 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00134284 (* 1 = 0.00134284 loss)
I0626 02:54:44.030310 12773 solver.cpp:408] Iteration 37000, lr = 0.01, mom = 0.9
I0626 02:54:55.387351 12773 solver.cpp:192] Iteration 38000, loss = 0.00138638
I0626 02:54:55.387434 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00138638 (* 1 = 0.00138638 loss)
I0626 02:54:55.387456 12773 solver.cpp:408] Iteration 38000, lr = 0.01, mom = 0.9
I0626 02:55:06.810868 12773 solver.cpp:192] Iteration 39000, loss = 0.00149287
I0626 02:55:06.811019 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00149287 (* 1 = 0.00149287 loss)
I0626 02:55:06.811043 12773 solver.cpp:408] Iteration 39000, lr = 0.01, mom = 0.9
I0626 02:55:18.278451 12773 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_40000.caffemodel
I0626 02:55:18.323894 12773 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_40000.solverstate
I0626 02:55:18.356775 12773 solver.cpp:248] Iteration 40000, Testing net (#0)
I0626 02:55:18.518630 12773 solver.cpp:299]     Test net output #0: pt_loss3 = 0.000304108 (* 1 = 0.000304108 loss)
I0626 02:55:18.520387 12773 solver.cpp:192] Iteration 40000, loss = 0.00150422
I0626 02:55:18.520449 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00150422 (* 1 = 0.00150422 loss)
I0626 02:55:18.520473 12773 solver.cpp:408] Iteration 40000, lr = 0.001, mom = 0.9
I0626 02:55:29.949304 12773 solver.cpp:192] Iteration 41000, loss = 0.00154764
I0626 02:55:29.949414 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00154764 (* 1 = 0.00154764 loss)
I0626 02:55:29.949568 12773 solver.cpp:408] Iteration 41000, lr = 0.001, mom = 0.9
I0626 02:55:41.257527 12773 solver.cpp:192] Iteration 42000, loss = 0.0015647
I0626 02:55:41.257658 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.0015647 (* 1 = 0.0015647 loss)
I0626 02:55:41.257683 12773 solver.cpp:408] Iteration 42000, lr = 0.001, mom = 0.9
I0626 02:55:52.579500 12773 solver.cpp:192] Iteration 43000, loss = 0.00156105
I0626 02:55:52.579569 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00156105 (* 1 = 0.00156105 loss)
I0626 02:55:52.579582 12773 solver.cpp:408] Iteration 43000, lr = 0.001, mom = 0.9
I0626 02:56:04.038097 12773 solver.cpp:192] Iteration 44000, loss = 0.001589
I0626 02:56:04.038177 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.001589 (* 1 = 0.001589 loss)
I0626 02:56:04.038197 12773 solver.cpp:408] Iteration 44000, lr = 0.001, mom = 0.9
I0626 02:56:15.520164 12773 solver.cpp:192] Iteration 45000, loss = 0.00147331
I0626 02:56:15.520390 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00147331 (* 1 = 0.00147331 loss)
I0626 02:56:15.520414 12773 solver.cpp:408] Iteration 45000, lr = 0.001, mom = 0.9
I0626 02:56:26.690492 12773 solver.cpp:192] Iteration 46000, loss = 0.00152173
I0626 02:56:26.690557 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00152173 (* 1 = 0.00152173 loss)
I0626 02:56:26.690572 12773 solver.cpp:408] Iteration 46000, lr = 0.001, mom = 0.9
I0626 02:56:37.937829 12773 solver.cpp:192] Iteration 47000, loss = 0.00147939
I0626 02:56:37.937896 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00147939 (* 1 = 0.00147939 loss)
I0626 02:56:37.937911 12773 solver.cpp:408] Iteration 47000, lr = 0.001, mom = 0.9
I0626 02:56:49.148947 12773 solver.cpp:192] Iteration 48000, loss = 0.00150785
I0626 02:56:49.149190 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00150785 (* 1 = 0.00150785 loss)
I0626 02:56:49.149224 12773 solver.cpp:408] Iteration 48000, lr = 0.001, mom = 0.9
I0626 02:57:00.503994 12773 solver.cpp:192] Iteration 49000, loss = 0.00147845
I0626 02:57:00.504060 12773 solver.cpp:207]     Train net output #0: pt_loss3 = 0.00147845 (* 1 = 0.00147845 loss)
I0626 02:57:00.504074 12773 solver.cpp:408] Iteration 49000, lr = 0.001, mom = 0.9
I0626 02:57:12.021664 12773 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_50000.caffemodel
I0626 02:57:12.066426 12773 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_50000.solverstate
I0626 02:57:12.100630 12773 solver.cpp:229] Iteration 50000, loss = 0.00138095
I0626 02:57:12.100675 12773 solver.cpp:248] Iteration 50000, Testing net (#0)
I0626 02:57:12.255913 12773 solver.cpp:299]     Test net output #0: pt_loss3 = 0.000303825 (* 1 = 0.000303825 loss)
I0626 02:57:12.255966 12773 solver.cpp:234] Optimization Done.
I0626 02:57:12.255981 12773 caffe.cpp:121] Optimization Done.
I0626 02:57:12.307292  8898 net.cpp:277] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0626 02:57:12.307493  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/test"
    batch_size: 100
    backend: LEVELDB
  }
  include {
    phase: TEST
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner2_drop"
  name: "inner2_dropdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner2_drop"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner3"
  top: "d_inner2"
  name: "d_inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner2"
  top: "d_inner2"
  name: "d_inner2relu"
  type: RELU
}
layers {
  bottom: "d_inner2"
  bottom: "inner2"
  top: "pt_loss3"
  name: "pt_loss3"
  type: EUCLIDEAN_LOSS
}
I0626 02:57:12.307600  8898 net.cpp:67] Creating Layer data
I0626 02:57:12.307621  8898 net.cpp:358] data -> data
I0626 02:57:12.307642  8898 net.cpp:96] Setting up data
I0626 02:57:12.307657  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/test
I0626 02:57:12.395162  8898 data_layer.cpp:145] output data size: 100,3600,1,1
I0626 02:57:12.395934  8898 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 02:57:12.395973  8898 net.cpp:67] Creating Layer inner1
I0626 02:57:12.395992  8898 net.cpp:396] inner1 <- data
I0626 02:57:12.396015  8898 net.cpp:358] inner1 -> inner1
I0626 02:57:12.396039  8898 net.cpp:96] Setting up inner1
I0626 02:57:12.425019  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:57:12.425070  8898 net.cpp:67] Creating Layer inner1relu
I0626 02:57:12.425079  8898 net.cpp:396] inner1relu <- inner1
I0626 02:57:12.425089  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 02:57:12.425101  8898 net.cpp:96] Setting up inner1relu
I0626 02:57:12.425109  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:57:12.425119  8898 net.cpp:67] Creating Layer inner2
I0626 02:57:12.425127  8898 net.cpp:396] inner2 <- inner1
I0626 02:57:12.425137  8898 net.cpp:358] inner2 -> inner2
I0626 02:57:12.425146  8898 net.cpp:96] Setting up inner2
I0626 02:57:12.428061  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:57:12.428079  8898 net.cpp:67] Creating Layer inner2relu
I0626 02:57:12.428087  8898 net.cpp:396] inner2relu <- inner2
I0626 02:57:12.428097  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 02:57:12.428107  8898 net.cpp:96] Setting up inner2relu
I0626 02:57:12.428113  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:57:12.428123  8898 net.cpp:67] Creating Layer inner2_inner2relu_0_split
I0626 02:57:12.428130  8898 net.cpp:396] inner2_inner2relu_0_split <- inner2
I0626 02:57:12.428140  8898 net.cpp:358] inner2_inner2relu_0_split -> inner2_inner2relu_0_split_0
I0626 02:57:12.428150  8898 net.cpp:358] inner2_inner2relu_0_split -> inner2_inner2relu_0_split_1
I0626 02:57:12.428160  8898 net.cpp:96] Setting up inner2_inner2relu_0_split
I0626 02:57:12.428169  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:57:12.428176  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:57:12.428186  8898 net.cpp:67] Creating Layer inner2_dropdrop
I0626 02:57:12.428194  8898 net.cpp:396] inner2_dropdrop <- inner2_inner2relu_0_split_0
I0626 02:57:12.428205  8898 net.cpp:358] inner2_dropdrop -> inner2_drop
I0626 02:57:12.428215  8898 net.cpp:96] Setting up inner2_dropdrop
I0626 02:57:12.428222  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:57:12.428246  8898 net.cpp:67] Creating Layer inner3
I0626 02:57:12.428253  8898 net.cpp:396] inner3 <- inner2_drop
I0626 02:57:12.428264  8898 net.cpp:358] inner3 -> inner3
I0626 02:57:12.428274  8898 net.cpp:96] Setting up inner3
I0626 02:57:12.439914  8898 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 02:57:12.439951  8898 net.cpp:67] Creating Layer inner3relu
I0626 02:57:12.439960  8898 net.cpp:396] inner3relu <- inner3
I0626 02:57:12.439971  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 02:57:12.439981  8898 net.cpp:96] Setting up inner3relu
I0626 02:57:12.439990  8898 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 02:57:12.439998  8898 net.cpp:67] Creating Layer inner3drop
I0626 02:57:12.440006  8898 net.cpp:396] inner3drop <- inner3
I0626 02:57:12.440016  8898 net.cpp:347] inner3drop -> inner3 (in-place)
I0626 02:57:12.440024  8898 net.cpp:96] Setting up inner3drop
I0626 02:57:12.440032  8898 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 02:57:12.440042  8898 net.cpp:67] Creating Layer d_inner2
I0626 02:57:12.440050  8898 net.cpp:396] d_inner2 <- inner3
I0626 02:57:12.440060  8898 net.cpp:358] d_inner2 -> d_inner2
I0626 02:57:12.440070  8898 net.cpp:96] Setting up d_inner2
I0626 02:57:12.451902  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:57:12.451943  8898 net.cpp:67] Creating Layer d_inner2relu
I0626 02:57:12.451952  8898 net.cpp:396] d_inner2relu <- d_inner2
I0626 02:57:12.451963  8898 net.cpp:347] d_inner2relu -> d_inner2 (in-place)
I0626 02:57:12.451973  8898 net.cpp:96] Setting up d_inner2relu
I0626 02:57:12.451982  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:57:12.451990  8898 net.cpp:67] Creating Layer pt_loss3
I0626 02:57:12.451998  8898 net.cpp:396] pt_loss3 <- d_inner2
I0626 02:57:12.452006  8898 net.cpp:396] pt_loss3 <- inner2_inner2relu_0_split_1
I0626 02:57:12.452018  8898 net.cpp:358] pt_loss3 -> pt_loss3
I0626 02:57:12.452029  8898 net.cpp:96] Setting up pt_loss3
I0626 02:57:12.452039  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 02:57:12.452047  8898 net.cpp:109]     with loss weight 1
I0626 02:57:12.452064  8898 net.cpp:170] pt_loss3 needs backward computation.
I0626 02:57:12.452071  8898 net.cpp:170] d_inner2relu needs backward computation.
I0626 02:57:12.452078  8898 net.cpp:170] d_inner2 needs backward computation.
I0626 02:57:12.452086  8898 net.cpp:170] inner3drop needs backward computation.
I0626 02:57:12.452093  8898 net.cpp:170] inner3relu needs backward computation.
I0626 02:57:12.452100  8898 net.cpp:170] inner3 needs backward computation.
I0626 02:57:12.452108  8898 net.cpp:172] inner2_dropdrop does not need backward computation.
I0626 02:57:12.452116  8898 net.cpp:172] inner2_inner2relu_0_split does not need backward computation.
I0626 02:57:12.452123  8898 net.cpp:172] inner2relu does not need backward computation.
I0626 02:57:12.452131  8898 net.cpp:172] inner2 does not need backward computation.
I0626 02:57:12.452138  8898 net.cpp:172] inner1relu does not need backward computation.
I0626 02:57:12.452147  8898 net.cpp:172] inner1 does not need backward computation.
I0626 02:57:12.452153  8898 net.cpp:172] data does not need backward computation.
I0626 02:57:12.452162  8898 net.cpp:208] This network produces output pt_loss3
I0626 02:57:12.452173  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 02:57:12.452183  8898 net.cpp:219] Network initialization done.
I0626 02:57:12.452190  8898 net.cpp:220] Memory required for data: 5640004
I0626 02:57:12.469874  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 02:57:12.469991  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/train"
    batch_size: 256
    backend: LEVELDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "inner3_drop"
  name: "inner3_dropdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner3_drop"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  top: "d_inner3"
  name: "d_inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner3"
  top: "d_inner3"
  name: "d_inner3relu"
  type: RELU
}
layers {
  bottom: "d_inner3"
  bottom: "inner3"
  top: "pt_loss4"
  name: "pt_loss4"
  type: EUCLIDEAN_LOSS
}
I0626 02:57:12.470054  8898 net.cpp:67] Creating Layer data
I0626 02:57:12.470067  8898 net.cpp:358] data -> data
I0626 02:57:12.470080  8898 net.cpp:96] Setting up data
I0626 02:57:12.470089  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/train
I0626 02:57:12.554208  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 02:57:12.555979  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 02:57:12.556031  8898 net.cpp:67] Creating Layer inner1
I0626 02:57:12.556057  8898 net.cpp:396] inner1 <- data
I0626 02:57:12.556079  8898 net.cpp:358] inner1 -> inner1
I0626 02:57:12.556104  8898 net.cpp:96] Setting up inner1
I0626 02:57:12.580832  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:57:12.580884  8898 net.cpp:67] Creating Layer inner1relu
I0626 02:57:12.580893  8898 net.cpp:396] inner1relu <- inner1
I0626 02:57:12.580904  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 02:57:12.580914  8898 net.cpp:96] Setting up inner1relu
I0626 02:57:12.580922  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:57:12.580934  8898 net.cpp:67] Creating Layer inner2
I0626 02:57:12.580940  8898 net.cpp:396] inner2 <- inner1
I0626 02:57:12.580950  8898 net.cpp:358] inner2 -> inner2
I0626 02:57:12.580960  8898 net.cpp:96] Setting up inner2
I0626 02:57:12.583925  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:57:12.583943  8898 net.cpp:67] Creating Layer inner2relu
I0626 02:57:12.583951  8898 net.cpp:396] inner2relu <- inner2
I0626 02:57:12.583961  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 02:57:12.583968  8898 net.cpp:96] Setting up inner2relu
I0626 02:57:12.583976  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:57:12.583986  8898 net.cpp:67] Creating Layer inner3
I0626 02:57:12.583993  8898 net.cpp:396] inner3 <- inner2
I0626 02:57:12.584002  8898 net.cpp:358] inner3 -> inner3
I0626 02:57:12.584012  8898 net.cpp:96] Setting up inner3
I0626 02:57:12.595696  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:57:12.595743  8898 net.cpp:67] Creating Layer inner3relu
I0626 02:57:12.595752  8898 net.cpp:396] inner3relu <- inner3
I0626 02:57:12.595763  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 02:57:12.595773  8898 net.cpp:96] Setting up inner3relu
I0626 02:57:12.595780  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:57:12.595789  8898 net.cpp:67] Creating Layer inner3_inner3relu_0_split
I0626 02:57:12.595798  8898 net.cpp:396] inner3_inner3relu_0_split <- inner3
I0626 02:57:12.595805  8898 net.cpp:358] inner3_inner3relu_0_split -> inner3_inner3relu_0_split_0
I0626 02:57:12.595818  8898 net.cpp:358] inner3_inner3relu_0_split -> inner3_inner3relu_0_split_1
I0626 02:57:12.595827  8898 net.cpp:96] Setting up inner3_inner3relu_0_split
I0626 02:57:12.595835  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:57:12.595844  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:57:12.595854  8898 net.cpp:67] Creating Layer inner3_dropdrop
I0626 02:57:12.595861  8898 net.cpp:396] inner3_dropdrop <- inner3_inner3relu_0_split_0
I0626 02:57:12.595871  8898 net.cpp:358] inner3_dropdrop -> inner3_drop
I0626 02:57:12.595881  8898 net.cpp:96] Setting up inner3_dropdrop
I0626 02:57:12.595890  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:57:12.595899  8898 net.cpp:67] Creating Layer output
I0626 02:57:12.595907  8898 net.cpp:396] output <- inner3_drop
I0626 02:57:12.595916  8898 net.cpp:358] output -> output
I0626 02:57:12.595927  8898 net.cpp:96] Setting up output
I0626 02:57:12.597092  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 02:57:12.597107  8898 net.cpp:67] Creating Layer d_inner3
I0626 02:57:12.597115  8898 net.cpp:396] d_inner3 <- output
I0626 02:57:12.597126  8898 net.cpp:358] d_inner3 -> d_inner3
I0626 02:57:12.597136  8898 net.cpp:96] Setting up d_inner3
I0626 02:57:12.598306  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:57:12.598322  8898 net.cpp:67] Creating Layer d_inner3relu
I0626 02:57:12.598330  8898 net.cpp:396] d_inner3relu <- d_inner3
I0626 02:57:12.598340  8898 net.cpp:347] d_inner3relu -> d_inner3 (in-place)
I0626 02:57:12.598348  8898 net.cpp:96] Setting up d_inner3relu
I0626 02:57:12.598356  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:57:12.598366  8898 net.cpp:67] Creating Layer pt_loss4
I0626 02:57:12.598372  8898 net.cpp:396] pt_loss4 <- d_inner3
I0626 02:57:12.598381  8898 net.cpp:396] pt_loss4 <- inner3_inner3relu_0_split_1
I0626 02:57:12.598389  8898 net.cpp:358] pt_loss4 -> pt_loss4
I0626 02:57:12.598398  8898 net.cpp:96] Setting up pt_loss4
I0626 02:57:12.598407  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 02:57:12.598415  8898 net.cpp:109]     with loss weight 1
I0626 02:57:12.598431  8898 net.cpp:170] pt_loss4 needs backward computation.
I0626 02:57:12.598439  8898 net.cpp:170] d_inner3relu needs backward computation.
I0626 02:57:12.598446  8898 net.cpp:170] d_inner3 needs backward computation.
I0626 02:57:12.598453  8898 net.cpp:170] output needs backward computation.
I0626 02:57:12.598461  8898 net.cpp:172] inner3_dropdrop does not need backward computation.
I0626 02:57:12.598469  8898 net.cpp:172] inner3_inner3relu_0_split does not need backward computation.
I0626 02:57:12.598475  8898 net.cpp:172] inner3relu does not need backward computation.
I0626 02:57:12.598484  8898 net.cpp:172] inner3 does not need backward computation.
I0626 02:57:12.598490  8898 net.cpp:172] inner2relu does not need backward computation.
I0626 02:57:12.598497  8898 net.cpp:172] inner2 does not need backward computation.
I0626 02:57:12.598505  8898 net.cpp:172] inner1relu does not need backward computation.
I0626 02:57:12.598512  8898 net.cpp:172] inner1 does not need backward computation.
I0626 02:57:12.598520  8898 net.cpp:172] data does not need backward computation.
I0626 02:57:12.598526  8898 net.cpp:208] This network produces output pt_loss4
I0626 02:57:12.598538  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 02:57:12.598548  8898 net.cpp:219] Network initialization done.
I0626 02:57:12.598556  8898 net.cpp:220] Memory required for data: 20121604
I0626 02:57:12.781683 30935 caffe.cpp:99] Use GPU with device ID 0
I0626 02:57:13.023221 30935 caffe.cpp:107] Starting Optimization
I0626 02:57:13.023316 30935 solver.cpp:32] Initializing solver from parameters: 
test_iter: 100
test_interval: 10000
base_lr: 0.1
display: 1000
max_iter: 50000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 20000
snapshot: 10000
snapshot_prefix: "modules/image30x30_dim50/exp/save"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/stack_net.prototxt"
snapshot_after_train: true
momentum_burnin: 1000
I0626 02:57:13.023344 30935 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/stack_net.prototxt
I0626 02:57:13.023540 30935 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 02:57:13.023634 30935 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/train"
    batch_size: 256
    backend: LEVELDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "inner3_drop"
  name: "inner3_dropdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner3_drop"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  top: "d_inner3"
  name: "d_inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner3"
  top: "d_inner3"
  name: "d_inner3relu"
  type: RELU
}
layers {
  bottom: "d_inner3"
  bottom: "inner3"
  top: "pt_loss4"
  name: "pt_loss4"
  type: EUCLIDEAN_LOSS
}
state {
  phase: TRAIN
}
I0626 02:57:13.023702 30935 net.cpp:67] Creating Layer data
I0626 02:57:13.023715 30935 net.cpp:358] data -> data
I0626 02:57:13.023732 30935 net.cpp:96] Setting up data
I0626 02:57:13.023768 30935 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/train
I0626 02:57:13.105070 30935 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 02:57:13.105244 30935 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 02:57:13.105278 30935 net.cpp:67] Creating Layer inner1
I0626 02:57:13.105295 30935 net.cpp:396] inner1 <- data
I0626 02:57:13.105316 30935 net.cpp:358] inner1 -> inner1
I0626 02:57:13.105345 30935 net.cpp:96] Setting up inner1
I0626 02:57:13.133808 30935 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:57:13.133864 30935 net.cpp:67] Creating Layer inner1relu
I0626 02:57:13.133893 30935 net.cpp:396] inner1relu <- inner1
I0626 02:57:13.133905 30935 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 02:57:13.133918 30935 net.cpp:96] Setting up inner1relu
I0626 02:57:13.133931 30935 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:57:13.133942 30935 net.cpp:67] Creating Layer inner2
I0626 02:57:13.133950 30935 net.cpp:396] inner2 <- inner1
I0626 02:57:13.133960 30935 net.cpp:358] inner2 -> inner2
I0626 02:57:13.133971 30935 net.cpp:96] Setting up inner2
I0626 02:57:13.136937 30935 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:57:13.136956 30935 net.cpp:67] Creating Layer inner2relu
I0626 02:57:13.136965 30935 net.cpp:396] inner2relu <- inner2
I0626 02:57:13.136975 30935 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 02:57:13.136983 30935 net.cpp:96] Setting up inner2relu
I0626 02:57:13.136991 30935 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 02:57:13.137001 30935 net.cpp:67] Creating Layer inner3
I0626 02:57:13.137009 30935 net.cpp:396] inner3 <- inner2
I0626 02:57:13.137019 30935 net.cpp:358] inner3 -> inner3
I0626 02:57:13.137029 30935 net.cpp:96] Setting up inner3
I0626 02:57:13.148779 30935 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:57:13.148813 30935 net.cpp:67] Creating Layer inner3relu
I0626 02:57:13.148821 30935 net.cpp:396] inner3relu <- inner3
I0626 02:57:13.148831 30935 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 02:57:13.148842 30935 net.cpp:96] Setting up inner3relu
I0626 02:57:13.148851 30935 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:57:13.148860 30935 net.cpp:67] Creating Layer inner3_inner3relu_0_split
I0626 02:57:13.148869 30935 net.cpp:396] inner3_inner3relu_0_split <- inner3
I0626 02:57:13.148880 30935 net.cpp:358] inner3_inner3relu_0_split -> inner3_inner3relu_0_split_0
I0626 02:57:13.148891 30935 net.cpp:358] inner3_inner3relu_0_split -> inner3_inner3relu_0_split_1
I0626 02:57:13.148903 30935 net.cpp:96] Setting up inner3_inner3relu_0_split
I0626 02:57:13.148917 30935 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:57:13.148926 30935 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:57:13.148937 30935 net.cpp:67] Creating Layer inner3_dropdrop
I0626 02:57:13.148946 30935 net.cpp:396] inner3_dropdrop <- inner3_inner3relu_0_split_0
I0626 02:57:13.148963 30935 net.cpp:358] inner3_dropdrop -> inner3_drop
I0626 02:57:13.148974 30935 net.cpp:96] Setting up inner3_dropdrop
I0626 02:57:13.148983 30935 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:57:13.148995 30935 net.cpp:67] Creating Layer output
I0626 02:57:13.149004 30935 net.cpp:396] output <- inner3_drop
I0626 02:57:13.149015 30935 net.cpp:358] output -> output
I0626 02:57:13.149025 30935 net.cpp:96] Setting up output
I0626 02:57:13.150213 30935 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 02:57:13.150229 30935 net.cpp:67] Creating Layer d_inner3
I0626 02:57:13.150238 30935 net.cpp:396] d_inner3 <- output
I0626 02:57:13.150249 30935 net.cpp:358] d_inner3 -> d_inner3
I0626 02:57:13.150261 30935 net.cpp:96] Setting up d_inner3
I0626 02:57:13.151458 30935 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:57:13.151484 30935 net.cpp:67] Creating Layer d_inner3relu
I0626 02:57:13.151492 30935 net.cpp:396] d_inner3relu <- d_inner3
I0626 02:57:13.151502 30935 net.cpp:347] d_inner3relu -> d_inner3 (in-place)
I0626 02:57:13.151520 30935 net.cpp:96] Setting up d_inner3relu
I0626 02:57:13.151527 30935 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 02:57:13.151537 30935 net.cpp:67] Creating Layer pt_loss4
I0626 02:57:13.151546 30935 net.cpp:396] pt_loss4 <- d_inner3
I0626 02:57:13.151553 30935 net.cpp:396] pt_loss4 <- inner3_inner3relu_0_split_1
I0626 02:57:13.151562 30935 net.cpp:358] pt_loss4 -> pt_loss4
I0626 02:57:13.151572 30935 net.cpp:96] Setting up pt_loss4
I0626 02:57:13.151585 30935 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 02:57:13.151594 30935 net.cpp:109]     with loss weight 1
I0626 02:57:13.151626 30935 net.cpp:170] pt_loss4 needs backward computation.
I0626 02:57:13.151635 30935 net.cpp:170] d_inner3relu needs backward computation.
I0626 02:57:13.151650 30935 net.cpp:170] d_inner3 needs backward computation.
I0626 02:57:13.151659 30935 net.cpp:170] output needs backward computation.
I0626 02:57:13.151667 30935 net.cpp:172] inner3_dropdrop does not need backward computation.
I0626 02:57:13.151675 30935 net.cpp:172] inner3_inner3relu_0_split does not need backward computation.
I0626 02:57:13.151684 30935 net.cpp:172] inner3relu does not need backward computation.
I0626 02:57:13.151690 30935 net.cpp:172] inner3 does not need backward computation.
I0626 02:57:13.151700 30935 net.cpp:172] inner2relu does not need backward computation.
I0626 02:57:13.151708 30935 net.cpp:172] inner2 does not need backward computation.
I0626 02:57:13.151716 30935 net.cpp:172] inner1relu does not need backward computation.
I0626 02:57:13.151724 30935 net.cpp:172] inner1 does not need backward computation.
I0626 02:57:13.151732 30935 net.cpp:172] data does not need backward computation.
I0626 02:57:13.151739 30935 net.cpp:208] This network produces output pt_loss4
I0626 02:57:13.151756 30935 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 02:57:13.151767 30935 net.cpp:219] Network initialization done.
I0626 02:57:13.151778 30935 net.cpp:220] Memory required for data: 20121604
I0626 02:57:13.151950 30935 solver.cpp:151] Creating test net (#0) specified by net file: modules/image30x30_dim50/stack_net.prototxt
I0626 02:57:13.151973 30935 net.cpp:277] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0626 02:57:13.152072 30935 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/test"
    batch_size: 100
    backend: LEVELDB
  }
  include {
    phase: TEST
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "inner3_drop"
  name: "inner3_dropdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner3_drop"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  top: "d_inner3"
  name: "d_inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner3"
  top: "d_inner3"
  name: "d_inner3relu"
  type: RELU
}
layers {
  bottom: "d_inner3"
  bottom: "inner3"
  top: "pt_loss4"
  name: "pt_loss4"
  type: EUCLIDEAN_LOSS
}
state {
  phase: TEST
}
I0626 02:57:13.152137 30935 net.cpp:67] Creating Layer data
I0626 02:57:13.152148 30935 net.cpp:358] data -> data
I0626 02:57:13.152159 30935 net.cpp:96] Setting up data
I0626 02:57:13.152169 30935 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/test
I0626 02:57:13.230602 30935 data_layer.cpp:145] output data size: 100,3600,1,1
I0626 02:57:13.230826 30935 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 02:57:13.230859 30935 net.cpp:67] Creating Layer inner1
I0626 02:57:13.230877 30935 net.cpp:396] inner1 <- data
I0626 02:57:13.230897 30935 net.cpp:358] inner1 -> inner1
I0626 02:57:13.230921 30935 net.cpp:96] Setting up inner1
I0626 02:57:13.258718 30935 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:57:13.258767 30935 net.cpp:67] Creating Layer inner1relu
I0626 02:57:13.258777 30935 net.cpp:396] inner1relu <- inner1
I0626 02:57:13.258787 30935 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 02:57:13.258800 30935 net.cpp:96] Setting up inner1relu
I0626 02:57:13.258808 30935 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:57:13.258819 30935 net.cpp:67] Creating Layer inner2
I0626 02:57:13.258828 30935 net.cpp:396] inner2 <- inner1
I0626 02:57:13.258838 30935 net.cpp:358] inner2 -> inner2
I0626 02:57:13.258849 30935 net.cpp:96] Setting up inner2
I0626 02:57:13.261833 30935 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:57:13.261853 30935 net.cpp:67] Creating Layer inner2relu
I0626 02:57:13.261862 30935 net.cpp:396] inner2relu <- inner2
I0626 02:57:13.261871 30935 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 02:57:13.261880 30935 net.cpp:96] Setting up inner2relu
I0626 02:57:13.261888 30935 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 02:57:13.261900 30935 net.cpp:67] Creating Layer inner3
I0626 02:57:13.261909 30935 net.cpp:396] inner3 <- inner2
I0626 02:57:13.261917 30935 net.cpp:358] inner3 -> inner3
I0626 02:57:13.261929 30935 net.cpp:96] Setting up inner3
I0626 02:57:13.273635 30935 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 02:57:13.273666 30935 net.cpp:67] Creating Layer inner3relu
I0626 02:57:13.273675 30935 net.cpp:396] inner3relu <- inner3
I0626 02:57:13.273686 30935 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 02:57:13.273697 30935 net.cpp:96] Setting up inner3relu
I0626 02:57:13.273705 30935 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 02:57:13.273715 30935 net.cpp:67] Creating Layer inner3_inner3relu_0_split
I0626 02:57:13.273723 30935 net.cpp:396] inner3_inner3relu_0_split <- inner3
I0626 02:57:13.273732 30935 net.cpp:358] inner3_inner3relu_0_split -> inner3_inner3relu_0_split_0
I0626 02:57:13.273743 30935 net.cpp:358] inner3_inner3relu_0_split -> inner3_inner3relu_0_split_1
I0626 02:57:13.273756 30935 net.cpp:96] Setting up inner3_inner3relu_0_split
I0626 02:57:13.273766 30935 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 02:57:13.273773 30935 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 02:57:13.273784 30935 net.cpp:67] Creating Layer inner3_dropdrop
I0626 02:57:13.273792 30935 net.cpp:396] inner3_dropdrop <- inner3_inner3relu_0_split_0
I0626 02:57:13.273804 30935 net.cpp:358] inner3_dropdrop -> inner3_drop
I0626 02:57:13.273815 30935 net.cpp:96] Setting up inner3_dropdrop
I0626 02:57:13.273824 30935 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 02:57:13.273834 30935 net.cpp:67] Creating Layer output
I0626 02:57:13.273842 30935 net.cpp:396] output <- inner3_drop
I0626 02:57:13.273854 30935 net.cpp:358] output -> output
I0626 02:57:13.273864 30935 net.cpp:96] Setting up output
I0626 02:57:13.275053 30935 net.cpp:103] Top shape: 100 50 1 1 (5000)
I0626 02:57:13.275069 30935 net.cpp:67] Creating Layer d_inner3
I0626 02:57:13.275079 30935 net.cpp:396] d_inner3 <- output
I0626 02:57:13.275090 30935 net.cpp:358] d_inner3 -> d_inner3
I0626 02:57:13.275101 30935 net.cpp:96] Setting up d_inner3
I0626 02:57:13.276294 30935 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 02:57:13.276314 30935 net.cpp:67] Creating Layer d_inner3relu
I0626 02:57:13.276332 30935 net.cpp:396] d_inner3relu <- d_inner3
I0626 02:57:13.276342 30935 net.cpp:347] d_inner3relu -> d_inner3 (in-place)
I0626 02:57:13.276363 30935 net.cpp:96] Setting up d_inner3relu
I0626 02:57:13.276371 30935 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 02:57:13.276383 30935 net.cpp:67] Creating Layer pt_loss4
I0626 02:57:13.276391 30935 net.cpp:396] pt_loss4 <- d_inner3
I0626 02:57:13.276401 30935 net.cpp:396] pt_loss4 <- inner3_inner3relu_0_split_1
I0626 02:57:13.276409 30935 net.cpp:358] pt_loss4 -> pt_loss4
I0626 02:57:13.276419 30935 net.cpp:96] Setting up pt_loss4
I0626 02:57:13.276429 30935 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 02:57:13.276437 30935 net.cpp:109]     with loss weight 1
I0626 02:57:13.276453 30935 net.cpp:170] pt_loss4 needs backward computation.
I0626 02:57:13.276461 30935 net.cpp:170] d_inner3relu needs backward computation.
I0626 02:57:13.276469 30935 net.cpp:170] d_inner3 needs backward computation.
I0626 02:57:13.276477 30935 net.cpp:170] output needs backward computation.
I0626 02:57:13.276484 30935 net.cpp:172] inner3_dropdrop does not need backward computation.
I0626 02:57:13.276494 30935 net.cpp:172] inner3_inner3relu_0_split does not need backward computation.
I0626 02:57:13.276501 30935 net.cpp:172] inner3relu does not need backward computation.
I0626 02:57:13.276509 30935 net.cpp:172] inner3 does not need backward computation.
I0626 02:57:13.276516 30935 net.cpp:172] inner2relu does not need backward computation.
I0626 02:57:13.276525 30935 net.cpp:172] inner2 does not need backward computation.
I0626 02:57:13.276531 30935 net.cpp:172] inner1relu does not need backward computation.
I0626 02:57:13.276540 30935 net.cpp:172] inner1 does not need backward computation.
I0626 02:57:13.276547 30935 net.cpp:172] data does not need backward computation.
I0626 02:57:13.276554 30935 net.cpp:208] This network produces output pt_loss4
I0626 02:57:13.276569 30935 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 02:57:13.276579 30935 net.cpp:219] Network initialization done.
I0626 02:57:13.276587 30935 net.cpp:220] Memory required for data: 7860004
I0626 02:57:13.276624 30935 solver.cpp:41] Solver scaffolding done.
I0626 02:57:13.276633 30935 caffe.cpp:115] Finetuning from modules/image30x30_dim50/stack_init.caffemodel
I0626 02:57:13.287798 30935 solver.cpp:160] Solving net
I0626 02:57:13.287853 30935 solver.cpp:248] Iteration 0, Testing net (#0)
I0626 02:57:13.477583 30935 solver.cpp:299]     Test net output #0: pt_loss4 = 0.0073868 (* 1 = 0.0073868 loss)
I0626 02:57:13.487733 30935 solver.cpp:192] Iteration 0, loss = 0.00664014
I0626 02:57:13.487793 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00664014 (* 1 = 0.00664014 loss)
I0626 02:57:13.487818 30935 solver.cpp:408] Iteration 0, lr = 0.1, mom = 0
I0626 02:57:24.793010 30935 solver.cpp:192] Iteration 1000, loss = 0.00526182
I0626 02:57:24.793084 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00526182 (* 1 = 0.00526182 loss)
I0626 02:57:24.793103 30935 solver.cpp:408] Iteration 1000, lr = 0.1, mom = 0.9
I0626 02:57:36.158816 30935 solver.cpp:192] Iteration 2000, loss = 0.00429231
I0626 02:57:36.158896 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00429231 (* 1 = 0.00429231 loss)
I0626 02:57:36.158916 30935 solver.cpp:408] Iteration 2000, lr = 0.1, mom = 0.9
I0626 02:57:47.596551 30935 solver.cpp:192] Iteration 3000, loss = 0.00417853
I0626 02:57:47.596683 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00417853 (* 1 = 0.00417853 loss)
I0626 02:57:47.596704 30935 solver.cpp:408] Iteration 3000, lr = 0.1, mom = 0.9
I0626 02:57:59.153579 30935 solver.cpp:192] Iteration 4000, loss = 0.00421203
I0626 02:57:59.153643 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00421203 (* 1 = 0.00421203 loss)
I0626 02:57:59.153658 30935 solver.cpp:408] Iteration 4000, lr = 0.1, mom = 0.9
I0626 02:58:10.536561 30935 solver.cpp:192] Iteration 5000, loss = 0.00437507
I0626 02:58:10.536638 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00437507 (* 1 = 0.00437507 loss)
I0626 02:58:10.536672 30935 solver.cpp:408] Iteration 5000, lr = 0.1, mom = 0.9
I0626 02:58:21.869583 30935 solver.cpp:192] Iteration 6000, loss = 0.00421695
I0626 02:58:21.869841 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00421695 (* 1 = 0.00421695 loss)
I0626 02:58:21.869860 30935 solver.cpp:408] Iteration 6000, lr = 0.1, mom = 0.9
I0626 02:58:33.178079 30935 solver.cpp:192] Iteration 7000, loss = 0.00425372
I0626 02:58:33.178170 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00425372 (* 1 = 0.00425372 loss)
I0626 02:58:33.178200 30935 solver.cpp:408] Iteration 7000, lr = 0.1, mom = 0.9
I0626 02:58:44.527597 30935 solver.cpp:192] Iteration 8000, loss = 0.00428369
I0626 02:58:44.527671 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00428369 (* 1 = 0.00428369 loss)
I0626 02:58:44.527689 30935 solver.cpp:408] Iteration 8000, lr = 0.1, mom = 0.9
I0626 02:58:55.937270 30935 solver.cpp:192] Iteration 9000, loss = 0.00420429
I0626 02:58:55.937391 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00420429 (* 1 = 0.00420429 loss)
I0626 02:58:55.937413 30935 solver.cpp:408] Iteration 9000, lr = 0.1, mom = 0.9
I0626 02:59:07.409016 30935 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_10000.caffemodel
I0626 02:59:07.451486 30935 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_10000.solverstate
I0626 02:59:07.479179 30935 solver.cpp:248] Iteration 10000, Testing net (#0)
I0626 02:59:07.627154 30935 solver.cpp:299]     Test net output #0: pt_loss4 = 0.00366688 (* 1 = 0.00366688 loss)
I0626 02:59:07.628836 30935 solver.cpp:192] Iteration 10000, loss = 0.00405002
I0626 02:59:07.628891 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00405002 (* 1 = 0.00405002 loss)
I0626 02:59:07.628911 30935 solver.cpp:408] Iteration 10000, lr = 0.1, mom = 0.9
I0626 02:59:18.985400 30935 solver.cpp:192] Iteration 11000, loss = 0.00399276
I0626 02:59:18.985486 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00399276 (* 1 = 0.00399276 loss)
I0626 02:59:18.985509 30935 solver.cpp:408] Iteration 11000, lr = 0.1, mom = 0.9
I0626 02:59:30.409876 30935 solver.cpp:192] Iteration 12000, loss = 0.00355959
I0626 02:59:30.410017 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00355959 (* 1 = 0.00355959 loss)
I0626 02:59:30.410043 30935 solver.cpp:408] Iteration 12000, lr = 0.1, mom = 0.9
I0626 02:59:41.839022 30935 solver.cpp:192] Iteration 13000, loss = 0.00330621
I0626 02:59:41.839108 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00330621 (* 1 = 0.00330621 loss)
I0626 02:59:41.839138 30935 solver.cpp:408] Iteration 13000, lr = 0.1, mom = 0.9
I0626 02:59:53.321940 30935 solver.cpp:192] Iteration 14000, loss = 0.0031252
I0626 02:59:53.322005 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.0031252 (* 1 = 0.0031252 loss)
I0626 02:59:53.322018 30935 solver.cpp:408] Iteration 14000, lr = 0.1, mom = 0.9
I0626 03:00:04.872100 30935 solver.cpp:192] Iteration 15000, loss = 0.00280831
I0626 03:00:04.872319 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00280831 (* 1 = 0.00280831 loss)
I0626 03:00:04.872349 30935 solver.cpp:408] Iteration 15000, lr = 0.1, mom = 0.9
I0626 03:00:16.336597 30935 solver.cpp:192] Iteration 16000, loss = 0.0025714
I0626 03:00:16.336679 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.0025714 (* 1 = 0.0025714 loss)
I0626 03:00:16.336701 30935 solver.cpp:408] Iteration 16000, lr = 0.1, mom = 0.9
I0626 03:00:27.740769 30935 solver.cpp:192] Iteration 17000, loss = 0.00234182
I0626 03:00:27.740854 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00234182 (* 1 = 0.00234182 loss)
I0626 03:00:27.740877 30935 solver.cpp:408] Iteration 17000, lr = 0.1, mom = 0.9
I0626 03:00:39.210757 30935 solver.cpp:192] Iteration 18000, loss = 0.00214792
I0626 03:00:39.210867 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00214792 (* 1 = 0.00214792 loss)
I0626 03:00:39.210888 30935 solver.cpp:408] Iteration 18000, lr = 0.1, mom = 0.9
I0626 03:00:50.692207 30935 solver.cpp:192] Iteration 19000, loss = 0.00198421
I0626 03:00:50.692276 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00198421 (* 1 = 0.00198421 loss)
I0626 03:00:50.692291 30935 solver.cpp:408] Iteration 19000, lr = 0.1, mom = 0.9
I0626 03:01:02.072523 30935 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_20000.caffemodel
I0626 03:01:02.111026 30935 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_20000.solverstate
I0626 03:01:02.138754 30935 solver.cpp:248] Iteration 20000, Testing net (#0)
I0626 03:01:02.306959 30935 solver.cpp:299]     Test net output #0: pt_loss4 = 0.00140725 (* 1 = 0.00140725 loss)
I0626 03:01:02.308631 30935 solver.cpp:192] Iteration 20000, loss = 0.00184471
I0626 03:01:02.308686 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00184471 (* 1 = 0.00184471 loss)
I0626 03:01:02.308706 30935 solver.cpp:408] Iteration 20000, lr = 0.01, mom = 0.9
I0626 03:01:13.444320 30935 solver.cpp:192] Iteration 21000, loss = 0.00169517
I0626 03:01:13.444475 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00169517 (* 1 = 0.00169517 loss)
I0626 03:01:13.444490 30935 solver.cpp:408] Iteration 21000, lr = 0.01, mom = 0.9
I0626 03:01:24.680403 30935 solver.cpp:192] Iteration 22000, loss = 0.00157101
I0626 03:01:24.680487 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00157101 (* 1 = 0.00157101 loss)
I0626 03:01:24.680510 30935 solver.cpp:408] Iteration 22000, lr = 0.01, mom = 0.9
I0626 03:01:35.999706 30935 solver.cpp:192] Iteration 23000, loss = 0.00155052
I0626 03:01:35.999768 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00155052 (* 1 = 0.00155052 loss)
I0626 03:01:35.999781 30935 solver.cpp:408] Iteration 23000, lr = 0.01, mom = 0.9
I0626 03:01:46.825846 30935 solver.cpp:192] Iteration 24000, loss = 0.00151649
I0626 03:01:46.825969 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00151649 (* 1 = 0.00151649 loss)
I0626 03:01:46.825989 30935 solver.cpp:408] Iteration 24000, lr = 0.01, mom = 0.9
I0626 03:01:57.802939 30935 solver.cpp:192] Iteration 25000, loss = 0.00144486
I0626 03:01:57.803027 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00144486 (* 1 = 0.00144486 loss)
I0626 03:01:57.803048 30935 solver.cpp:408] Iteration 25000, lr = 0.01, mom = 0.9
I0626 03:02:09.201792 30935 solver.cpp:192] Iteration 26000, loss = 0.00138626
I0626 03:02:09.201880 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00138626 (* 1 = 0.00138626 loss)
I0626 03:02:09.201905 30935 solver.cpp:408] Iteration 26000, lr = 0.01, mom = 0.9
I0626 03:02:20.330044 30935 solver.cpp:192] Iteration 27000, loss = 0.00127361
I0626 03:02:20.330153 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00127361 (* 1 = 0.00127361 loss)
I0626 03:02:20.330174 30935 solver.cpp:408] Iteration 27000, lr = 0.01, mom = 0.9
I0626 03:02:31.645736 30935 solver.cpp:192] Iteration 28000, loss = 0.00132081
I0626 03:02:31.645822 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00132081 (* 1 = 0.00132081 loss)
I0626 03:02:31.645845 30935 solver.cpp:408] Iteration 28000, lr = 0.01, mom = 0.9
I0626 03:02:42.738507 30935 solver.cpp:192] Iteration 29000, loss = 0.00138555
I0626 03:02:42.738574 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00138555 (* 1 = 0.00138555 loss)
I0626 03:02:42.738589 30935 solver.cpp:408] Iteration 29000, lr = 0.01, mom = 0.9
I0626 03:02:53.816323 30935 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_30000.caffemodel
I0626 03:02:53.854276 30935 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_30000.solverstate
I0626 03:02:53.883365 30935 solver.cpp:248] Iteration 30000, Testing net (#0)
I0626 03:02:54.022980 30935 solver.cpp:299]     Test net output #0: pt_loss4 = 0.0012962 (* 1 = 0.0012962 loss)
I0626 03:02:54.024477 30935 solver.cpp:192] Iteration 30000, loss = 0.00138335
I0626 03:02:54.024523 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00138335 (* 1 = 0.00138335 loss)
I0626 03:02:54.024551 30935 solver.cpp:408] Iteration 30000, lr = 0.01, mom = 0.9
I0626 03:03:05.239127 30935 solver.cpp:192] Iteration 31000, loss = 0.00133749
I0626 03:03:05.239203 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00133749 (* 1 = 0.00133749 loss)
I0626 03:03:05.239223 30935 solver.cpp:408] Iteration 31000, lr = 0.01, mom = 0.9
I0626 03:03:16.565065 30935 solver.cpp:192] Iteration 32000, loss = 0.00127079
I0626 03:03:16.565146 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00127079 (* 1 = 0.00127079 loss)
I0626 03:03:16.565168 30935 solver.cpp:408] Iteration 32000, lr = 0.01, mom = 0.9
I0626 03:03:27.743669 30935 solver.cpp:192] Iteration 33000, loss = 0.001236
I0626 03:03:27.743857 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.001236 (* 1 = 0.001236 loss)
I0626 03:03:27.743881 30935 solver.cpp:408] Iteration 33000, lr = 0.01, mom = 0.9
I0626 03:03:38.596220 30935 solver.cpp:192] Iteration 34000, loss = 0.0011997
I0626 03:03:38.596295 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.0011997 (* 1 = 0.0011997 loss)
I0626 03:03:38.596315 30935 solver.cpp:408] Iteration 34000, lr = 0.01, mom = 0.9
I0626 03:03:49.752431 30935 solver.cpp:192] Iteration 35000, loss = 0.00115819
I0626 03:03:49.752511 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00115819 (* 1 = 0.00115819 loss)
I0626 03:03:49.752533 30935 solver.cpp:408] Iteration 35000, lr = 0.01, mom = 0.9
I0626 03:04:00.956612 30935 solver.cpp:192] Iteration 36000, loss = 0.00116907
I0626 03:04:00.956701 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00116907 (* 1 = 0.00116907 loss)
I0626 03:04:00.956717 30935 solver.cpp:408] Iteration 36000, lr = 0.01, mom = 0.9
I0626 03:04:12.280879 30935 solver.cpp:192] Iteration 37000, loss = 0.00118614
I0626 03:04:12.280953 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00118614 (* 1 = 0.00118614 loss)
I0626 03:04:12.280972 30935 solver.cpp:408] Iteration 37000, lr = 0.01, mom = 0.9
I0626 03:04:22.424953 30935 solver.cpp:192] Iteration 38000, loss = 0.00123249
I0626 03:04:22.425017 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00123249 (* 1 = 0.00123249 loss)
I0626 03:04:22.425031 30935 solver.cpp:408] Iteration 38000, lr = 0.01, mom = 0.9
I0626 03:04:33.150430 30935 solver.cpp:192] Iteration 39000, loss = 0.00130804
I0626 03:04:33.150610 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00130804 (* 1 = 0.00130804 loss)
I0626 03:04:33.150630 30935 solver.cpp:408] Iteration 39000, lr = 0.01, mom = 0.9
I0626 03:04:43.929893 30935 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_40000.caffemodel
I0626 03:04:43.967326 30935 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_40000.solverstate
I0626 03:04:43.994591 30935 solver.cpp:248] Iteration 40000, Testing net (#0)
I0626 03:04:44.148162 30935 solver.cpp:299]     Test net output #0: pt_loss4 = 0.00119428 (* 1 = 0.00119428 loss)
I0626 03:04:44.149762 30935 solver.cpp:192] Iteration 40000, loss = 0.00135018
I0626 03:04:44.149807 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00135018 (* 1 = 0.00135018 loss)
I0626 03:04:44.149826 30935 solver.cpp:408] Iteration 40000, lr = 0.001, mom = 0.9
I0626 03:04:54.862107 30935 solver.cpp:192] Iteration 41000, loss = 0.00137518
I0626 03:04:54.862184 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00137518 (* 1 = 0.00137518 loss)
I0626 03:04:54.862201 30935 solver.cpp:408] Iteration 41000, lr = 0.001, mom = 0.9
I0626 03:05:05.544345 30935 solver.cpp:192] Iteration 42000, loss = 0.00142647
I0626 03:05:05.544482 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00142647 (* 1 = 0.00142647 loss)
I0626 03:05:05.544505 30935 solver.cpp:408] Iteration 42000, lr = 0.001, mom = 0.9
I0626 03:05:16.113464 30935 solver.cpp:192] Iteration 43000, loss = 0.00143272
I0626 03:05:16.113541 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00143272 (* 1 = 0.00143272 loss)
I0626 03:05:16.113561 30935 solver.cpp:408] Iteration 43000, lr = 0.001, mom = 0.9
I0626 03:05:27.001557 30935 solver.cpp:192] Iteration 44000, loss = 0.00141893
I0626 03:05:27.001634 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00141893 (* 1 = 0.00141893 loss)
I0626 03:05:27.001652 30935 solver.cpp:408] Iteration 44000, lr = 0.001, mom = 0.9
I0626 03:05:37.875028 30935 solver.cpp:192] Iteration 45000, loss = 0.00135022
I0626 03:05:37.875282 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00135022 (* 1 = 0.00135022 loss)
I0626 03:05:37.875303 30935 solver.cpp:408] Iteration 45000, lr = 0.001, mom = 0.9
I0626 03:05:48.584172 30935 solver.cpp:192] Iteration 46000, loss = 0.00136199
I0626 03:05:48.584236 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00136199 (* 1 = 0.00136199 loss)
I0626 03:05:48.584251 30935 solver.cpp:408] Iteration 46000, lr = 0.001, mom = 0.9
I0626 03:05:59.452111 30935 solver.cpp:192] Iteration 47000, loss = 0.00135811
I0626 03:05:59.452195 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00135811 (* 1 = 0.00135811 loss)
I0626 03:05:59.452217 30935 solver.cpp:408] Iteration 47000, lr = 0.001, mom = 0.9
I0626 03:06:10.261924 30935 solver.cpp:192] Iteration 48000, loss = 0.00138417
I0626 03:06:10.262107 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00138417 (* 1 = 0.00138417 loss)
I0626 03:06:10.262130 30935 solver.cpp:408] Iteration 48000, lr = 0.001, mom = 0.9
I0626 03:06:21.053037 30935 solver.cpp:192] Iteration 49000, loss = 0.00138926
I0626 03:06:21.053108 30935 solver.cpp:207]     Train net output #0: pt_loss4 = 0.00138926 (* 1 = 0.00138926 loss)
I0626 03:06:21.053126 30935 solver.cpp:408] Iteration 49000, lr = 0.001, mom = 0.9
I0626 03:06:31.825911 30935 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_50000.caffemodel
I0626 03:06:31.863325 30935 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_50000.solverstate
I0626 03:06:31.893240 30935 solver.cpp:229] Iteration 50000, loss = 0.00132993
I0626 03:06:31.893286 30935 solver.cpp:248] Iteration 50000, Testing net (#0)
I0626 03:06:32.046594 30935 solver.cpp:299]     Test net output #0: pt_loss4 = 0.00118345 (* 1 = 0.00118345 loss)
I0626 03:06:32.046645 30935 solver.cpp:234] Optimization Done.
I0626 03:06:32.046659 30935 caffe.cpp:121] Optimization Done.
I0626 03:06:32.096962  8898 net.cpp:277] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0626 03:06:32.097102  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/test"
    batch_size: 100
    backend: LEVELDB
  }
  include {
    phase: TEST
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 0
  blobs_lr: 0
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "inner3_drop"
  name: "inner3_dropdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.2
  }
}
layers {
  bottom: "inner3_drop"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  top: "d_inner3"
  name: "d_inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner3"
  top: "d_inner3"
  name: "d_inner3relu"
  type: RELU
}
layers {
  bottom: "d_inner3"
  bottom: "inner3"
  top: "pt_loss4"
  name: "pt_loss4"
  type: EUCLIDEAN_LOSS
}
I0626 03:06:32.097177  8898 net.cpp:67] Creating Layer data
I0626 03:06:32.097192  8898 net.cpp:358] data -> data
I0626 03:06:32.097206  8898 net.cpp:96] Setting up data
I0626 03:06:32.097216  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/test
I0626 03:06:32.183064  8898 data_layer.cpp:145] output data size: 100,3600,1,1
I0626 03:06:32.183848  8898 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 03:06:32.183892  8898 net.cpp:67] Creating Layer inner1
I0626 03:06:32.183910  8898 net.cpp:396] inner1 <- data
I0626 03:06:32.183934  8898 net.cpp:358] inner1 -> inner1
I0626 03:06:32.183961  8898 net.cpp:96] Setting up inner1
I0626 03:06:32.212819  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:32.212872  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:06:32.212880  8898 net.cpp:396] inner1relu <- inner1
I0626 03:06:32.212890  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:06:32.212903  8898 net.cpp:96] Setting up inner1relu
I0626 03:06:32.212910  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:32.212920  8898 net.cpp:67] Creating Layer inner2
I0626 03:06:32.212929  8898 net.cpp:396] inner2 <- inner1
I0626 03:06:32.212937  8898 net.cpp:358] inner2 -> inner2
I0626 03:06:32.212949  8898 net.cpp:96] Setting up inner2
I0626 03:06:32.215857  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:32.215874  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:06:32.215883  8898 net.cpp:396] inner2relu <- inner2
I0626 03:06:32.215890  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:06:32.215899  8898 net.cpp:96] Setting up inner2relu
I0626 03:06:32.215906  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:32.215915  8898 net.cpp:67] Creating Layer inner3
I0626 03:06:32.215924  8898 net.cpp:396] inner3 <- inner2
I0626 03:06:32.215932  8898 net.cpp:358] inner3 -> inner3
I0626 03:06:32.215941  8898 net.cpp:96] Setting up inner3
I0626 03:06:32.227669  8898 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:32.227713  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:06:32.227722  8898 net.cpp:396] inner3relu <- inner3
I0626 03:06:32.227732  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:06:32.227742  8898 net.cpp:96] Setting up inner3relu
I0626 03:06:32.227751  8898 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:32.227759  8898 net.cpp:67] Creating Layer inner3_inner3relu_0_split
I0626 03:06:32.227766  8898 net.cpp:396] inner3_inner3relu_0_split <- inner3
I0626 03:06:32.227777  8898 net.cpp:358] inner3_inner3relu_0_split -> inner3_inner3relu_0_split_0
I0626 03:06:32.227787  8898 net.cpp:358] inner3_inner3relu_0_split -> inner3_inner3relu_0_split_1
I0626 03:06:32.227797  8898 net.cpp:96] Setting up inner3_inner3relu_0_split
I0626 03:06:32.227807  8898 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:32.227814  8898 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:32.227825  8898 net.cpp:67] Creating Layer inner3_dropdrop
I0626 03:06:32.227833  8898 net.cpp:396] inner3_dropdrop <- inner3_inner3relu_0_split_0
I0626 03:06:32.227843  8898 net.cpp:358] inner3_dropdrop -> inner3_drop
I0626 03:06:32.227852  8898 net.cpp:96] Setting up inner3_dropdrop
I0626 03:06:32.227874  8898 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:32.227883  8898 net.cpp:67] Creating Layer output
I0626 03:06:32.227891  8898 net.cpp:396] output <- inner3_drop
I0626 03:06:32.227901  8898 net.cpp:358] output -> output
I0626 03:06:32.227912  8898 net.cpp:96] Setting up output
I0626 03:06:32.229270  8898 net.cpp:103] Top shape: 100 50 1 1 (5000)
I0626 03:06:32.229293  8898 net.cpp:67] Creating Layer d_inner3
I0626 03:06:32.229302  8898 net.cpp:396] d_inner3 <- output
I0626 03:06:32.229313  8898 net.cpp:358] d_inner3 -> d_inner3
I0626 03:06:32.229326  8898 net.cpp:96] Setting up d_inner3
I0626 03:06:32.230597  8898 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:32.230618  8898 net.cpp:67] Creating Layer d_inner3relu
I0626 03:06:32.230628  8898 net.cpp:396] d_inner3relu <- d_inner3
I0626 03:06:32.230638  8898 net.cpp:347] d_inner3relu -> d_inner3 (in-place)
I0626 03:06:32.230648  8898 net.cpp:96] Setting up d_inner3relu
I0626 03:06:32.230656  8898 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:32.230665  8898 net.cpp:67] Creating Layer pt_loss4
I0626 03:06:32.230674  8898 net.cpp:396] pt_loss4 <- d_inner3
I0626 03:06:32.230681  8898 net.cpp:396] pt_loss4 <- inner3_inner3relu_0_split_1
I0626 03:06:32.230692  8898 net.cpp:358] pt_loss4 -> pt_loss4
I0626 03:06:32.230702  8898 net.cpp:96] Setting up pt_loss4
I0626 03:06:32.230720  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:32.230726  8898 net.cpp:109]     with loss weight 1
I0626 03:06:32.230742  8898 net.cpp:170] pt_loss4 needs backward computation.
I0626 03:06:32.230751  8898 net.cpp:170] d_inner3relu needs backward computation.
I0626 03:06:32.230758  8898 net.cpp:170] d_inner3 needs backward computation.
I0626 03:06:32.230767  8898 net.cpp:170] output needs backward computation.
I0626 03:06:32.230773  8898 net.cpp:172] inner3_dropdrop does not need backward computation.
I0626 03:06:32.230782  8898 net.cpp:172] inner3_inner3relu_0_split does not need backward computation.
I0626 03:06:32.230789  8898 net.cpp:172] inner3relu does not need backward computation.
I0626 03:06:32.230798  8898 net.cpp:172] inner3 does not need backward computation.
I0626 03:06:32.230813  8898 net.cpp:172] inner2relu does not need backward computation.
I0626 03:06:32.230820  8898 net.cpp:172] inner2 does not need backward computation.
I0626 03:06:32.230829  8898 net.cpp:172] inner1relu does not need backward computation.
I0626 03:06:32.230835  8898 net.cpp:172] inner1 does not need backward computation.
I0626 03:06:32.230844  8898 net.cpp:172] data does not need backward computation.
I0626 03:06:32.230851  8898 net.cpp:208] This network produces output pt_loss4
I0626 03:06:32.230872  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:06:32.230883  8898 net.cpp:219] Network initialization done.
I0626 03:06:32.230897  8898 net.cpp:220] Memory required for data: 7860004
I0626 03:06:32.245978  8898 net.cpp:277] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0626 03:06:32.246157  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/test"
    batch_size: 100
    backend: LEVELDB
  }
  include {
    phase: TEST
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.016666668
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.044721361
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.044721361
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.022360681
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  top: "output"
  name: "outputdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "output"
  top: "d_inner3"
  name: "d_inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.14142136
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner3"
  top: "d_inner3"
  name: "d_inner3relu"
  type: RELU
}
layers {
  bottom: "d_inner3"
  top: "d_inner3"
  name: "d_inner3drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "d_inner3"
  top: "d_inner2"
  name: "d_inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.022360681
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner2"
  top: "d_inner2"
  name: "d_inner2relu"
  type: RELU
}
layers {
  bottom: "d_inner2"
  top: "d_inner2"
  name: "d_inner2drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "d_inner2"
  top: "d_inner1"
  name: "d_inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.044721361
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner1"
  top: "d_inner1"
  name: "d_inner1relu"
  type: RELU
}
layers {
  bottom: "d_inner1"
  top: "d_inner1"
  name: "d_inner1drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "d_inner1"
  top: "d_data"
  name: "d_data"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 3600
    weight_filler {
      type: "gaussian"
      std: 0.044721361
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_data"
  bottom: "data"
  top: "pt_loss"
  name: "pt_loss"
  type: EUCLIDEAN_LOSS
}
I0626 03:06:32.246245  8898 net.cpp:67] Creating Layer data
I0626 03:06:32.246259  8898 net.cpp:358] data -> data
I0626 03:06:32.246273  8898 net.cpp:96] Setting up data
I0626 03:06:32.246284  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/test
I0626 03:06:32.334673  8898 data_layer.cpp:145] output data size: 100,3600,1,1
I0626 03:06:32.334961  8898 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 03:06:32.335000  8898 net.cpp:67] Creating Layer data_data_0_split
I0626 03:06:32.335019  8898 net.cpp:396] data_data_0_split <- data
I0626 03:06:32.335041  8898 net.cpp:358] data_data_0_split -> data_data_0_split_0
I0626 03:06:32.335083  8898 net.cpp:358] data_data_0_split -> data_data_0_split_1
I0626 03:06:32.335105  8898 net.cpp:96] Setting up data_data_0_split
I0626 03:06:32.335124  8898 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 03:06:32.335141  8898 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 03:06:32.335165  8898 net.cpp:67] Creating Layer inner1
I0626 03:06:32.335181  8898 net.cpp:396] inner1 <- data_data_0_split_0
I0626 03:06:32.335202  8898 net.cpp:358] inner1 -> inner1
I0626 03:06:32.335224  8898 net.cpp:96] Setting up inner1
I0626 03:06:32.362232  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:32.362280  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:06:32.362290  8898 net.cpp:396] inner1relu <- inner1
I0626 03:06:32.362301  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:06:32.362313  8898 net.cpp:96] Setting up inner1relu
I0626 03:06:32.362321  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:32.362334  8898 net.cpp:67] Creating Layer inner1drop
I0626 03:06:32.362341  8898 net.cpp:396] inner1drop <- inner1
I0626 03:06:32.362350  8898 net.cpp:347] inner1drop -> inner1 (in-place)
I0626 03:06:32.362360  8898 net.cpp:96] Setting up inner1drop
I0626 03:06:32.362368  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:32.362378  8898 net.cpp:67] Creating Layer inner2
I0626 03:06:32.362386  8898 net.cpp:396] inner2 <- inner1
I0626 03:06:32.362396  8898 net.cpp:358] inner2 -> inner2
I0626 03:06:32.362407  8898 net.cpp:96] Setting up inner2
I0626 03:06:32.365192  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:32.365209  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:06:32.365217  8898 net.cpp:396] inner2relu <- inner2
I0626 03:06:32.365226  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:06:32.365236  8898 net.cpp:96] Setting up inner2relu
I0626 03:06:32.365242  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:32.365252  8898 net.cpp:67] Creating Layer inner2drop
I0626 03:06:32.365258  8898 net.cpp:396] inner2drop <- inner2
I0626 03:06:32.365267  8898 net.cpp:347] inner2drop -> inner2 (in-place)
I0626 03:06:32.365276  8898 net.cpp:96] Setting up inner2drop
I0626 03:06:32.365283  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:32.365294  8898 net.cpp:67] Creating Layer inner3
I0626 03:06:32.365301  8898 net.cpp:396] inner3 <- inner2
I0626 03:06:32.365310  8898 net.cpp:358] inner3 -> inner3
I0626 03:06:32.365320  8898 net.cpp:96] Setting up inner3
I0626 03:06:32.376965  8898 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:32.377001  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:06:32.377009  8898 net.cpp:396] inner3relu <- inner3
I0626 03:06:32.377020  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:06:32.377030  8898 net.cpp:96] Setting up inner3relu
I0626 03:06:32.377039  8898 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:32.377048  8898 net.cpp:67] Creating Layer inner3drop
I0626 03:06:32.377056  8898 net.cpp:396] inner3drop <- inner3
I0626 03:06:32.377065  8898 net.cpp:347] inner3drop -> inner3 (in-place)
I0626 03:06:32.377074  8898 net.cpp:96] Setting up inner3drop
I0626 03:06:32.377082  8898 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:32.377092  8898 net.cpp:67] Creating Layer output
I0626 03:06:32.377100  8898 net.cpp:396] output <- inner3
I0626 03:06:32.377110  8898 net.cpp:358] output -> output
I0626 03:06:32.377120  8898 net.cpp:96] Setting up output
I0626 03:06:32.378319  8898 net.cpp:103] Top shape: 100 50 1 1 (5000)
I0626 03:06:32.378334  8898 net.cpp:67] Creating Layer outputdrop
I0626 03:06:32.378341  8898 net.cpp:396] outputdrop <- output
I0626 03:06:32.378350  8898 net.cpp:347] outputdrop -> output (in-place)
I0626 03:06:32.378360  8898 net.cpp:96] Setting up outputdrop
I0626 03:06:32.378367  8898 net.cpp:103] Top shape: 100 50 1 1 (5000)
I0626 03:06:32.378377  8898 net.cpp:67] Creating Layer d_inner3
I0626 03:06:32.378384  8898 net.cpp:396] d_inner3 <- output
I0626 03:06:32.378394  8898 net.cpp:358] d_inner3 -> d_inner3
I0626 03:06:32.378415  8898 net.cpp:96] Setting up d_inner3
I0626 03:06:32.379562  8898 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:32.379580  8898 net.cpp:67] Creating Layer d_inner3relu
I0626 03:06:32.379587  8898 net.cpp:396] d_inner3relu <- d_inner3
I0626 03:06:32.379596  8898 net.cpp:347] d_inner3relu -> d_inner3 (in-place)
I0626 03:06:32.379606  8898 net.cpp:96] Setting up d_inner3relu
I0626 03:06:32.379613  8898 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:32.379622  8898 net.cpp:67] Creating Layer d_inner3drop
I0626 03:06:32.379629  8898 net.cpp:396] d_inner3drop <- d_inner3
I0626 03:06:32.379638  8898 net.cpp:347] d_inner3drop -> d_inner3 (in-place)
I0626 03:06:32.379647  8898 net.cpp:96] Setting up d_inner3drop
I0626 03:06:32.379655  8898 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:32.379667  8898 net.cpp:67] Creating Layer d_inner2
I0626 03:06:32.379674  8898 net.cpp:396] d_inner2 <- d_inner3
I0626 03:06:32.379684  8898 net.cpp:358] d_inner2 -> d_inner2
I0626 03:06:32.379694  8898 net.cpp:96] Setting up d_inner2
I0626 03:06:32.390817  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:32.390846  8898 net.cpp:67] Creating Layer d_inner2relu
I0626 03:06:32.390854  8898 net.cpp:396] d_inner2relu <- d_inner2
I0626 03:06:32.390863  8898 net.cpp:347] d_inner2relu -> d_inner2 (in-place)
I0626 03:06:32.390873  8898 net.cpp:96] Setting up d_inner2relu
I0626 03:06:32.390882  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:32.390890  8898 net.cpp:67] Creating Layer d_inner2drop
I0626 03:06:32.390897  8898 net.cpp:396] d_inner2drop <- d_inner2
I0626 03:06:32.390907  8898 net.cpp:347] d_inner2drop -> d_inner2 (in-place)
I0626 03:06:32.390915  8898 net.cpp:96] Setting up d_inner2drop
I0626 03:06:32.390923  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:32.390933  8898 net.cpp:67] Creating Layer d_inner1
I0626 03:06:32.390940  8898 net.cpp:396] d_inner1 <- d_inner2
I0626 03:06:32.390949  8898 net.cpp:358] d_inner1 -> d_inner1
I0626 03:06:32.390959  8898 net.cpp:96] Setting up d_inner1
I0626 03:06:32.393733  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:32.393748  8898 net.cpp:67] Creating Layer d_inner1relu
I0626 03:06:32.393755  8898 net.cpp:396] d_inner1relu <- d_inner1
I0626 03:06:32.393764  8898 net.cpp:347] d_inner1relu -> d_inner1 (in-place)
I0626 03:06:32.393774  8898 net.cpp:96] Setting up d_inner1relu
I0626 03:06:32.393780  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:32.393790  8898 net.cpp:67] Creating Layer d_inner1drop
I0626 03:06:32.393797  8898 net.cpp:396] d_inner1drop <- d_inner1
I0626 03:06:32.393805  8898 net.cpp:347] d_inner1drop -> d_inner1 (in-place)
I0626 03:06:32.393815  8898 net.cpp:96] Setting up d_inner1drop
I0626 03:06:32.393822  8898 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:32.393831  8898 net.cpp:67] Creating Layer d_data
I0626 03:06:32.393838  8898 net.cpp:396] d_data <- d_inner1
I0626 03:06:32.393847  8898 net.cpp:358] d_data -> d_data
I0626 03:06:32.393857  8898 net.cpp:96] Setting up d_data
I0626 03:06:32.413784  8898 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 03:06:32.413830  8898 net.cpp:67] Creating Layer pt_loss
I0626 03:06:32.413839  8898 net.cpp:396] pt_loss <- d_data
I0626 03:06:32.413849  8898 net.cpp:396] pt_loss <- data_data_0_split_1
I0626 03:06:32.413859  8898 net.cpp:358] pt_loss -> pt_loss
I0626 03:06:32.413871  8898 net.cpp:96] Setting up pt_loss
I0626 03:06:32.413882  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:32.413888  8898 net.cpp:109]     with loss weight 1
I0626 03:06:32.413904  8898 net.cpp:170] pt_loss needs backward computation.
I0626 03:06:32.413913  8898 net.cpp:170] d_data needs backward computation.
I0626 03:06:32.413920  8898 net.cpp:170] d_inner1drop needs backward computation.
I0626 03:06:32.413928  8898 net.cpp:170] d_inner1relu needs backward computation.
I0626 03:06:32.413935  8898 net.cpp:170] d_inner1 needs backward computation.
I0626 03:06:32.413942  8898 net.cpp:170] d_inner2drop needs backward computation.
I0626 03:06:32.413964  8898 net.cpp:170] d_inner2relu needs backward computation.
I0626 03:06:32.413971  8898 net.cpp:170] d_inner2 needs backward computation.
I0626 03:06:32.413980  8898 net.cpp:170] d_inner3drop needs backward computation.
I0626 03:06:32.413986  8898 net.cpp:170] d_inner3relu needs backward computation.
I0626 03:06:32.413993  8898 net.cpp:170] d_inner3 needs backward computation.
I0626 03:06:32.414001  8898 net.cpp:170] outputdrop needs backward computation.
I0626 03:06:32.414008  8898 net.cpp:170] output needs backward computation.
I0626 03:06:32.414016  8898 net.cpp:170] inner3drop needs backward computation.
I0626 03:06:32.414023  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:06:32.414031  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:06:32.414038  8898 net.cpp:170] inner2drop needs backward computation.
I0626 03:06:32.414047  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:06:32.414053  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:06:32.414062  8898 net.cpp:170] inner1drop needs backward computation.
I0626 03:06:32.414068  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:06:32.414075  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:06:32.414083  8898 net.cpp:172] data_data_0_split does not need backward computation.
I0626 03:06:32.414091  8898 net.cpp:172] data does not need backward computation.
I0626 03:06:32.414098  8898 net.cpp:208] This network produces output pt_loss
I0626 03:06:32.414113  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:06:32.414124  8898 net.cpp:219] Network initialization done.
I0626 03:06:32.414131  8898 net.cpp:220] Memory required for data: 13000004
I0626 03:06:32.559305 16647 caffe.cpp:99] Use GPU with device ID 0
I0626 03:06:32.803270 16647 caffe.cpp:107] Starting Optimization
I0626 03:06:32.803362 16647 solver.cpp:32] Initializing solver from parameters: 
test_iter: 100
test_interval: 10000
base_lr: 0.1
display: 1000
max_iter: 1000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 20000
snapshot: 10000
snapshot_prefix: "modules/image30x30_dim50/exp/save"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/pt_net.prototxt"
snapshot_after_train: true
momentum_burnin: 1000
I0626 03:06:32.803388 16647 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/pt_net.prototxt
I0626 03:06:32.803640 16647 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:06:32.803795 16647 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/train"
    batch_size: 256
    backend: LEVELDB
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.016666668
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.044721361
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.044721361
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.022360681
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  top: "output"
  name: "outputdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "output"
  top: "d_inner3"
  name: "d_inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.14142136
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner3"
  top: "d_inner3"
  name: "d_inner3relu"
  type: RELU
}
layers {
  bottom: "d_inner3"
  top: "d_inner3"
  name: "d_inner3drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "d_inner3"
  top: "d_inner2"
  name: "d_inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.022360681
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner2"
  top: "d_inner2"
  name: "d_inner2relu"
  type: RELU
}
layers {
  bottom: "d_inner2"
  top: "d_inner2"
  name: "d_inner2drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "d_inner2"
  top: "d_inner1"
  name: "d_inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.044721361
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner1"
  top: "d_inner1"
  name: "d_inner1relu"
  type: RELU
}
layers {
  bottom: "d_inner1"
  top: "d_inner1"
  name: "d_inner1drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "d_inner1"
  top: "d_data"
  name: "d_data"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 3600
    weight_filler {
      type: "gaussian"
      std: 0.044721361
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_data"
  bottom: "data"
  top: "pt_loss"
  name: "pt_loss"
  type: EUCLIDEAN_LOSS
}
state {
  phase: TRAIN
}
I0626 03:06:32.803911 16647 net.cpp:67] Creating Layer data
I0626 03:06:32.803925 16647 net.cpp:358] data -> data
I0626 03:06:32.803941 16647 net.cpp:96] Setting up data
I0626 03:06:32.803974 16647 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/train
I0626 03:06:32.869089 16647 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:06:32.869258 16647 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:06:32.869293 16647 net.cpp:67] Creating Layer data_data_0_split
I0626 03:06:32.869310 16647 net.cpp:396] data_data_0_split <- data
I0626 03:06:32.869331 16647 net.cpp:358] data_data_0_split -> data_data_0_split_0
I0626 03:06:32.869361 16647 net.cpp:358] data_data_0_split -> data_data_0_split_1
I0626 03:06:32.869385 16647 net.cpp:96] Setting up data_data_0_split
I0626 03:06:32.869410 16647 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:06:32.869426 16647 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:06:32.869449 16647 net.cpp:67] Creating Layer inner1
I0626 03:06:32.869465 16647 net.cpp:396] inner1 <- data_data_0_split_0
I0626 03:06:32.869503 16647 net.cpp:358] inner1 -> inner1
I0626 03:06:32.869526 16647 net.cpp:96] Setting up inner1
I0626 03:06:32.895547 16647 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:32.895606 16647 net.cpp:67] Creating Layer inner1relu
I0626 03:06:32.895615 16647 net.cpp:396] inner1relu <- inner1
I0626 03:06:32.895627 16647 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:06:32.895639 16647 net.cpp:96] Setting up inner1relu
I0626 03:06:32.895654 16647 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:32.895668 16647 net.cpp:67] Creating Layer inner1drop
I0626 03:06:32.895675 16647 net.cpp:396] inner1drop <- inner1
I0626 03:06:32.895685 16647 net.cpp:347] inner1drop -> inner1 (in-place)
I0626 03:06:32.895694 16647 net.cpp:96] Setting up inner1drop
I0626 03:06:32.895704 16647 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:32.895715 16647 net.cpp:67] Creating Layer inner2
I0626 03:06:32.895723 16647 net.cpp:396] inner2 <- inner1
I0626 03:06:32.895733 16647 net.cpp:358] inner2 -> inner2
I0626 03:06:32.895745 16647 net.cpp:96] Setting up inner2
I0626 03:06:32.898828 16647 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:32.898844 16647 net.cpp:67] Creating Layer inner2relu
I0626 03:06:32.898854 16647 net.cpp:396] inner2relu <- inner2
I0626 03:06:32.898862 16647 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:06:32.898872 16647 net.cpp:96] Setting up inner2relu
I0626 03:06:32.898880 16647 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:32.898891 16647 net.cpp:67] Creating Layer inner2drop
I0626 03:06:32.898900 16647 net.cpp:396] inner2drop <- inner2
I0626 03:06:32.898908 16647 net.cpp:347] inner2drop -> inner2 (in-place)
I0626 03:06:32.898917 16647 net.cpp:96] Setting up inner2drop
I0626 03:06:32.898926 16647 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:32.898957 16647 net.cpp:67] Creating Layer inner3
I0626 03:06:32.898965 16647 net.cpp:396] inner3 <- inner2
I0626 03:06:32.898975 16647 net.cpp:358] inner3 -> inner3
I0626 03:06:32.898985 16647 net.cpp:96] Setting up inner3
I0626 03:06:32.910851 16647 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:32.910886 16647 net.cpp:67] Creating Layer inner3relu
I0626 03:06:32.910895 16647 net.cpp:396] inner3relu <- inner3
I0626 03:06:32.910905 16647 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:06:32.910915 16647 net.cpp:96] Setting up inner3relu
I0626 03:06:32.910923 16647 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:32.910933 16647 net.cpp:67] Creating Layer inner3drop
I0626 03:06:32.910941 16647 net.cpp:396] inner3drop <- inner3
I0626 03:06:32.910949 16647 net.cpp:347] inner3drop -> inner3 (in-place)
I0626 03:06:32.910959 16647 net.cpp:96] Setting up inner3drop
I0626 03:06:32.910966 16647 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:32.910979 16647 net.cpp:67] Creating Layer output
I0626 03:06:32.910986 16647 net.cpp:396] output <- inner3
I0626 03:06:32.910996 16647 net.cpp:358] output -> output
I0626 03:06:32.911006 16647 net.cpp:96] Setting up output
I0626 03:06:32.912221 16647 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:06:32.912240 16647 net.cpp:67] Creating Layer outputdrop
I0626 03:06:32.912247 16647 net.cpp:396] outputdrop <- output
I0626 03:06:32.912256 16647 net.cpp:347] outputdrop -> output (in-place)
I0626 03:06:32.912266 16647 net.cpp:96] Setting up outputdrop
I0626 03:06:32.912273 16647 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:06:32.912283 16647 net.cpp:67] Creating Layer d_inner3
I0626 03:06:32.912292 16647 net.cpp:396] d_inner3 <- output
I0626 03:06:32.912302 16647 net.cpp:358] d_inner3 -> d_inner3
I0626 03:06:32.912312 16647 net.cpp:96] Setting up d_inner3
I0626 03:06:32.913496 16647 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:32.913513 16647 net.cpp:67] Creating Layer d_inner3relu
I0626 03:06:32.913522 16647 net.cpp:396] d_inner3relu <- d_inner3
I0626 03:06:32.913530 16647 net.cpp:347] d_inner3relu -> d_inner3 (in-place)
I0626 03:06:32.913540 16647 net.cpp:96] Setting up d_inner3relu
I0626 03:06:32.913547 16647 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:32.913564 16647 net.cpp:67] Creating Layer d_inner3drop
I0626 03:06:32.913573 16647 net.cpp:396] d_inner3drop <- d_inner3
I0626 03:06:32.913583 16647 net.cpp:347] d_inner3drop -> d_inner3 (in-place)
I0626 03:06:32.913592 16647 net.cpp:96] Setting up d_inner3drop
I0626 03:06:32.913600 16647 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:32.913614 16647 net.cpp:67] Creating Layer d_inner2
I0626 03:06:32.913621 16647 net.cpp:396] d_inner2 <- d_inner3
I0626 03:06:32.913631 16647 net.cpp:358] d_inner2 -> d_inner2
I0626 03:06:32.913642 16647 net.cpp:96] Setting up d_inner2
I0626 03:06:32.925412 16647 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:32.925437 16647 net.cpp:67] Creating Layer d_inner2relu
I0626 03:06:32.925446 16647 net.cpp:396] d_inner2relu <- d_inner2
I0626 03:06:32.925458 16647 net.cpp:347] d_inner2relu -> d_inner2 (in-place)
I0626 03:06:32.925468 16647 net.cpp:96] Setting up d_inner2relu
I0626 03:06:32.925477 16647 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:32.925485 16647 net.cpp:67] Creating Layer d_inner2drop
I0626 03:06:32.925493 16647 net.cpp:396] d_inner2drop <- d_inner2
I0626 03:06:32.925503 16647 net.cpp:347] d_inner2drop -> d_inner2 (in-place)
I0626 03:06:32.925510 16647 net.cpp:96] Setting up d_inner2drop
I0626 03:06:32.925519 16647 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:32.925530 16647 net.cpp:67] Creating Layer d_inner1
I0626 03:06:32.925539 16647 net.cpp:396] d_inner1 <- d_inner2
I0626 03:06:32.925549 16647 net.cpp:358] d_inner1 -> d_inner1
I0626 03:06:32.925559 16647 net.cpp:96] Setting up d_inner1
I0626 03:06:32.928512 16647 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:32.928530 16647 net.cpp:67] Creating Layer d_inner1relu
I0626 03:06:32.928539 16647 net.cpp:396] d_inner1relu <- d_inner1
I0626 03:06:32.928558 16647 net.cpp:347] d_inner1relu -> d_inner1 (in-place)
I0626 03:06:32.928567 16647 net.cpp:96] Setting up d_inner1relu
I0626 03:06:32.928575 16647 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:32.928584 16647 net.cpp:67] Creating Layer d_inner1drop
I0626 03:06:32.928592 16647 net.cpp:396] d_inner1drop <- d_inner1
I0626 03:06:32.928601 16647 net.cpp:347] d_inner1drop -> d_inner1 (in-place)
I0626 03:06:32.928611 16647 net.cpp:96] Setting up d_inner1drop
I0626 03:06:32.928619 16647 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:32.928629 16647 net.cpp:67] Creating Layer d_data
I0626 03:06:32.928637 16647 net.cpp:396] d_data <- d_inner1
I0626 03:06:32.928648 16647 net.cpp:358] d_data -> d_data
I0626 03:06:32.928658 16647 net.cpp:96] Setting up d_data
I0626 03:06:32.949875 16647 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:06:32.949923 16647 net.cpp:67] Creating Layer pt_loss
I0626 03:06:32.949931 16647 net.cpp:396] pt_loss <- d_data
I0626 03:06:32.949941 16647 net.cpp:396] pt_loss <- data_data_0_split_1
I0626 03:06:32.949954 16647 net.cpp:358] pt_loss -> pt_loss
I0626 03:06:32.949965 16647 net.cpp:96] Setting up pt_loss
I0626 03:06:32.949981 16647 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:32.949990 16647 net.cpp:109]     with loss weight 1
I0626 03:06:32.950026 16647 net.cpp:170] pt_loss needs backward computation.
I0626 03:06:32.950034 16647 net.cpp:170] d_data needs backward computation.
I0626 03:06:32.950042 16647 net.cpp:170] d_inner1drop needs backward computation.
I0626 03:06:32.950050 16647 net.cpp:170] d_inner1relu needs backward computation.
I0626 03:06:32.950057 16647 net.cpp:170] d_inner1 needs backward computation.
I0626 03:06:32.950065 16647 net.cpp:170] d_inner2drop needs backward computation.
I0626 03:06:32.950073 16647 net.cpp:170] d_inner2relu needs backward computation.
I0626 03:06:32.950081 16647 net.cpp:170] d_inner2 needs backward computation.
I0626 03:06:32.950088 16647 net.cpp:170] d_inner3drop needs backward computation.
I0626 03:06:32.950096 16647 net.cpp:170] d_inner3relu needs backward computation.
I0626 03:06:32.950103 16647 net.cpp:170] d_inner3 needs backward computation.
I0626 03:06:32.950111 16647 net.cpp:170] outputdrop needs backward computation.
I0626 03:06:32.950127 16647 net.cpp:170] output needs backward computation.
I0626 03:06:32.950136 16647 net.cpp:170] inner3drop needs backward computation.
I0626 03:06:32.950143 16647 net.cpp:170] inner3relu needs backward computation.
I0626 03:06:32.950151 16647 net.cpp:170] inner3 needs backward computation.
I0626 03:06:32.950160 16647 net.cpp:170] inner2drop needs backward computation.
I0626 03:06:32.950166 16647 net.cpp:170] inner2relu needs backward computation.
I0626 03:06:32.950175 16647 net.cpp:170] inner2 needs backward computation.
I0626 03:06:32.950182 16647 net.cpp:170] inner1drop needs backward computation.
I0626 03:06:32.950189 16647 net.cpp:170] inner1relu needs backward computation.
I0626 03:06:32.950197 16647 net.cpp:170] inner1 needs backward computation.
I0626 03:06:32.950206 16647 net.cpp:172] data_data_0_split does not need backward computation.
I0626 03:06:32.950213 16647 net.cpp:172] data does not need backward computation.
I0626 03:06:32.950220 16647 net.cpp:208] This network produces output pt_loss
I0626 03:06:32.950239 16647 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:06:32.950250 16647 net.cpp:219] Network initialization done.
I0626 03:06:32.950263 16647 net.cpp:220] Memory required for data: 33280004
I0626 03:06:32.950498 16647 solver.cpp:151] Creating test net (#0) specified by net file: modules/image30x30_dim50/pt_net.prototxt
I0626 03:06:32.950527 16647 net.cpp:277] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0626 03:06:32.950675 16647 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/test"
    batch_size: 100
    backend: LEVELDB
  }
  include {
    phase: TEST
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.016666668
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.044721361
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.044721361
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.022360681
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  top: "output"
  name: "outputdrop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "output"
  top: "d_inner3"
  name: "d_inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.14142136
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner3"
  top: "d_inner3"
  name: "d_inner3relu"
  type: RELU
}
layers {
  bottom: "d_inner3"
  top: "d_inner3"
  name: "d_inner3drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "d_inner3"
  top: "d_inner2"
  name: "d_inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.022360681
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner2"
  top: "d_inner2"
  name: "d_inner2relu"
  type: RELU
}
layers {
  bottom: "d_inner2"
  top: "d_inner2"
  name: "d_inner2drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "d_inner2"
  top: "d_inner1"
  name: "d_inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.044721361
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_inner1"
  top: "d_inner1"
  name: "d_inner1relu"
  type: RELU
}
layers {
  bottom: "d_inner1"
  top: "d_inner1"
  name: "d_inner1drop"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0
  }
}
layers {
  bottom: "d_inner1"
  top: "d_data"
  name: "d_data"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 3600
    weight_filler {
      type: "gaussian"
      std: 0.044721361
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "d_data"
  bottom: "data"
  top: "pt_loss"
  name: "pt_loss"
  type: EUCLIDEAN_LOSS
}
state {
  phase: TEST
}
I0626 03:06:32.950762 16647 net.cpp:67] Creating Layer data
I0626 03:06:32.950773 16647 net.cpp:358] data -> data
I0626 03:06:32.950784 16647 net.cpp:96] Setting up data
I0626 03:06:32.950793 16647 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/test
I0626 03:06:33.036375 16647 data_layer.cpp:145] output data size: 100,3600,1,1
I0626 03:06:33.036607 16647 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 03:06:33.036643 16647 net.cpp:67] Creating Layer data_data_0_split
I0626 03:06:33.036669 16647 net.cpp:396] data_data_0_split <- data
I0626 03:06:33.036689 16647 net.cpp:358] data_data_0_split -> data_data_0_split_0
I0626 03:06:33.036711 16647 net.cpp:358] data_data_0_split -> data_data_0_split_1
I0626 03:06:33.036732 16647 net.cpp:96] Setting up data_data_0_split
I0626 03:06:33.036749 16647 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 03:06:33.036764 16647 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 03:06:33.036785 16647 net.cpp:67] Creating Layer inner1
I0626 03:06:33.036800 16647 net.cpp:396] inner1 <- data_data_0_split_0
I0626 03:06:33.036820 16647 net.cpp:358] inner1 -> inner1
I0626 03:06:33.036840 16647 net.cpp:96] Setting up inner1
I0626 03:06:33.065647 16647 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:33.065695 16647 net.cpp:67] Creating Layer inner1relu
I0626 03:06:33.065704 16647 net.cpp:396] inner1relu <- inner1
I0626 03:06:33.065716 16647 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:06:33.065726 16647 net.cpp:96] Setting up inner1relu
I0626 03:06:33.065735 16647 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:33.065747 16647 net.cpp:67] Creating Layer inner1drop
I0626 03:06:33.065753 16647 net.cpp:396] inner1drop <- inner1
I0626 03:06:33.065762 16647 net.cpp:347] inner1drop -> inner1 (in-place)
I0626 03:06:33.065771 16647 net.cpp:96] Setting up inner1drop
I0626 03:06:33.065780 16647 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:33.065790 16647 net.cpp:67] Creating Layer inner2
I0626 03:06:33.065798 16647 net.cpp:396] inner2 <- inner1
I0626 03:06:33.065809 16647 net.cpp:358] inner2 -> inner2
I0626 03:06:33.065831 16647 net.cpp:96] Setting up inner2
I0626 03:06:33.068816 16647 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:33.068835 16647 net.cpp:67] Creating Layer inner2relu
I0626 03:06:33.068843 16647 net.cpp:396] inner2relu <- inner2
I0626 03:06:33.068853 16647 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:06:33.068862 16647 net.cpp:96] Setting up inner2relu
I0626 03:06:33.068871 16647 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:33.068879 16647 net.cpp:67] Creating Layer inner2drop
I0626 03:06:33.068887 16647 net.cpp:396] inner2drop <- inner2
I0626 03:06:33.068897 16647 net.cpp:347] inner2drop -> inner2 (in-place)
I0626 03:06:33.068907 16647 net.cpp:96] Setting up inner2drop
I0626 03:06:33.068915 16647 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:33.068925 16647 net.cpp:67] Creating Layer inner3
I0626 03:06:33.068933 16647 net.cpp:396] inner3 <- inner2
I0626 03:06:33.068944 16647 net.cpp:358] inner3 -> inner3
I0626 03:06:33.068954 16647 net.cpp:96] Setting up inner3
I0626 03:06:33.080744 16647 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:33.080776 16647 net.cpp:67] Creating Layer inner3relu
I0626 03:06:33.080786 16647 net.cpp:396] inner3relu <- inner3
I0626 03:06:33.080796 16647 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:06:33.080806 16647 net.cpp:96] Setting up inner3relu
I0626 03:06:33.080816 16647 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:33.080826 16647 net.cpp:67] Creating Layer inner3drop
I0626 03:06:33.080833 16647 net.cpp:396] inner3drop <- inner3
I0626 03:06:33.080844 16647 net.cpp:347] inner3drop -> inner3 (in-place)
I0626 03:06:33.080853 16647 net.cpp:96] Setting up inner3drop
I0626 03:06:33.080862 16647 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:33.080873 16647 net.cpp:67] Creating Layer output
I0626 03:06:33.080896 16647 net.cpp:396] output <- inner3
I0626 03:06:33.080906 16647 net.cpp:358] output -> output
I0626 03:06:33.080919 16647 net.cpp:96] Setting up output
I0626 03:06:33.082141 16647 net.cpp:103] Top shape: 100 50 1 1 (5000)
I0626 03:06:33.082159 16647 net.cpp:67] Creating Layer outputdrop
I0626 03:06:33.082166 16647 net.cpp:396] outputdrop <- output
I0626 03:06:33.082176 16647 net.cpp:347] outputdrop -> output (in-place)
I0626 03:06:33.082185 16647 net.cpp:96] Setting up outputdrop
I0626 03:06:33.082193 16647 net.cpp:103] Top shape: 100 50 1 1 (5000)
I0626 03:06:33.082206 16647 net.cpp:67] Creating Layer d_inner3
I0626 03:06:33.082213 16647 net.cpp:396] d_inner3 <- output
I0626 03:06:33.082223 16647 net.cpp:358] d_inner3 -> d_inner3
I0626 03:06:33.082233 16647 net.cpp:96] Setting up d_inner3
I0626 03:06:33.083431 16647 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:33.083446 16647 net.cpp:67] Creating Layer d_inner3relu
I0626 03:06:33.083454 16647 net.cpp:396] d_inner3relu <- d_inner3
I0626 03:06:33.083472 16647 net.cpp:347] d_inner3relu -> d_inner3 (in-place)
I0626 03:06:33.083482 16647 net.cpp:96] Setting up d_inner3relu
I0626 03:06:33.083489 16647 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:33.083499 16647 net.cpp:67] Creating Layer d_inner3drop
I0626 03:06:33.083506 16647 net.cpp:396] d_inner3drop <- d_inner3
I0626 03:06:33.083515 16647 net.cpp:347] d_inner3drop -> d_inner3 (in-place)
I0626 03:06:33.083524 16647 net.cpp:96] Setting up d_inner3drop
I0626 03:06:33.083533 16647 net.cpp:103] Top shape: 100 2000 1 1 (200000)
I0626 03:06:33.083545 16647 net.cpp:67] Creating Layer d_inner2
I0626 03:06:33.083554 16647 net.cpp:396] d_inner2 <- d_inner3
I0626 03:06:33.083564 16647 net.cpp:358] d_inner2 -> d_inner2
I0626 03:06:33.083575 16647 net.cpp:96] Setting up d_inner2
I0626 03:06:33.095288 16647 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:33.095316 16647 net.cpp:67] Creating Layer d_inner2relu
I0626 03:06:33.095325 16647 net.cpp:396] d_inner2relu <- d_inner2
I0626 03:06:33.095335 16647 net.cpp:347] d_inner2relu -> d_inner2 (in-place)
I0626 03:06:33.095345 16647 net.cpp:96] Setting up d_inner2relu
I0626 03:06:33.095351 16647 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:33.095369 16647 net.cpp:67] Creating Layer d_inner2drop
I0626 03:06:33.095376 16647 net.cpp:396] d_inner2drop <- d_inner2
I0626 03:06:33.095387 16647 net.cpp:347] d_inner2drop -> d_inner2 (in-place)
I0626 03:06:33.095396 16647 net.cpp:96] Setting up d_inner2drop
I0626 03:06:33.095404 16647 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:33.095414 16647 net.cpp:67] Creating Layer d_inner1
I0626 03:06:33.095422 16647 net.cpp:396] d_inner1 <- d_inner2
I0626 03:06:33.095433 16647 net.cpp:358] d_inner1 -> d_inner1
I0626 03:06:33.095443 16647 net.cpp:96] Setting up d_inner1
I0626 03:06:33.098377 16647 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:33.098394 16647 net.cpp:67] Creating Layer d_inner1relu
I0626 03:06:33.098403 16647 net.cpp:396] d_inner1relu <- d_inner1
I0626 03:06:33.098412 16647 net.cpp:347] d_inner1relu -> d_inner1 (in-place)
I0626 03:06:33.098421 16647 net.cpp:96] Setting up d_inner1relu
I0626 03:06:33.098429 16647 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:33.098438 16647 net.cpp:67] Creating Layer d_inner1drop
I0626 03:06:33.098446 16647 net.cpp:396] d_inner1drop <- d_inner1
I0626 03:06:33.098456 16647 net.cpp:347] d_inner1drop -> d_inner1 (in-place)
I0626 03:06:33.098465 16647 net.cpp:96] Setting up d_inner1drop
I0626 03:06:33.098474 16647 net.cpp:103] Top shape: 100 500 1 1 (50000)
I0626 03:06:33.098484 16647 net.cpp:67] Creating Layer d_data
I0626 03:06:33.098491 16647 net.cpp:396] d_data <- d_inner1
I0626 03:06:33.098502 16647 net.cpp:358] d_data -> d_data
I0626 03:06:33.098512 16647 net.cpp:96] Setting up d_data
I0626 03:06:33.119577 16647 net.cpp:103] Top shape: 100 3600 1 1 (360000)
I0626 03:06:33.119616 16647 net.cpp:67] Creating Layer pt_loss
I0626 03:06:33.119626 16647 net.cpp:396] pt_loss <- d_data
I0626 03:06:33.119635 16647 net.cpp:396] pt_loss <- data_data_0_split_1
I0626 03:06:33.119662 16647 net.cpp:358] pt_loss -> pt_loss
I0626 03:06:33.119673 16647 net.cpp:96] Setting up pt_loss
I0626 03:06:33.119683 16647 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:33.119691 16647 net.cpp:109]     with loss weight 1
I0626 03:06:33.119709 16647 net.cpp:170] pt_loss needs backward computation.
I0626 03:06:33.119716 16647 net.cpp:170] d_data needs backward computation.
I0626 03:06:33.119724 16647 net.cpp:170] d_inner1drop needs backward computation.
I0626 03:06:33.119732 16647 net.cpp:170] d_inner1relu needs backward computation.
I0626 03:06:33.119740 16647 net.cpp:170] d_inner1 needs backward computation.
I0626 03:06:33.119748 16647 net.cpp:170] d_inner2drop needs backward computation.
I0626 03:06:33.119755 16647 net.cpp:170] d_inner2relu needs backward computation.
I0626 03:06:33.119763 16647 net.cpp:170] d_inner2 needs backward computation.
I0626 03:06:33.119771 16647 net.cpp:170] d_inner3drop needs backward computation.
I0626 03:06:33.119778 16647 net.cpp:170] d_inner3relu needs backward computation.
I0626 03:06:33.119786 16647 net.cpp:170] d_inner3 needs backward computation.
I0626 03:06:33.119793 16647 net.cpp:170] outputdrop needs backward computation.
I0626 03:06:33.119801 16647 net.cpp:170] output needs backward computation.
I0626 03:06:33.119808 16647 net.cpp:170] inner3drop needs backward computation.
I0626 03:06:33.119817 16647 net.cpp:170] inner3relu needs backward computation.
I0626 03:06:33.119823 16647 net.cpp:170] inner3 needs backward computation.
I0626 03:06:33.119832 16647 net.cpp:170] inner2drop needs backward computation.
I0626 03:06:33.119839 16647 net.cpp:170] inner2relu needs backward computation.
I0626 03:06:33.119846 16647 net.cpp:170] inner2 needs backward computation.
I0626 03:06:33.119854 16647 net.cpp:170] inner1drop needs backward computation.
I0626 03:06:33.119861 16647 net.cpp:170] inner1relu needs backward computation.
I0626 03:06:33.119869 16647 net.cpp:170] inner1 needs backward computation.
I0626 03:06:33.119877 16647 net.cpp:172] data_data_0_split does not need backward computation.
I0626 03:06:33.119885 16647 net.cpp:172] data does not need backward computation.
I0626 03:06:33.119892 16647 net.cpp:208] This network produces output pt_loss
I0626 03:06:33.119915 16647 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:06:33.119926 16647 net.cpp:219] Network initialization done.
I0626 03:06:33.119935 16647 net.cpp:220] Memory required for data: 13000004
I0626 03:06:33.119989 16647 solver.cpp:41] Solver scaffolding done.
I0626 03:06:33.119999 16647 caffe.cpp:115] Finetuning from modules/image30x30_dim50/stack_init_final.caffemodel
I0626 03:06:33.141594 16647 solver.cpp:160] Solving net
I0626 03:06:33.141656 16647 solver.cpp:248] Iteration 0, Testing net (#0)
I0626 03:06:33.266394 16647 solver.cpp:299]     Test net output #0: pt_loss = 0.00848329 (* 1 = 0.00848329 loss)
I0626 03:06:33.278072 16647 solver.cpp:192] Iteration 0, loss = 0.0118181
I0626 03:06:33.278120 16647 solver.cpp:207]     Train net output #0: pt_loss = 0.0118181 (* 1 = 0.0118181 loss)
I0626 03:06:33.278146 16647 solver.cpp:408] Iteration 0, lr = 0.1, mom = 0
I0626 03:06:43.922533 16647 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/save_iter_1000.caffemodel
I0626 03:06:43.997016 16647 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/save_iter_1000.solverstate
I0626 03:06:44.046432 16647 solver.cpp:229] Iteration 1000, loss = 0.0104546
I0626 03:06:44.046476 16647 solver.cpp:234] Optimization Done.
I0626 03:06:44.046485 16647 caffe.cpp:121] Optimization Done.
/usr/lib/python2.7/dist-packages/sklearn/lda.py:4: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19
  "in 0.17 and will be removed in 0.19", DeprecationWarning)
I0626 03:06:44.400171  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:06:44.400202  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:06:44.400310  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:06:44.400378  8898 net.cpp:67] Creating Layer data
I0626 03:06:44.400390  8898 net.cpp:358] data -> data
I0626 03:06:44.400406  8898 net.cpp:96] Setting up data
I0626 03:06:44.400415  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:06:44.547047  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:06:44.548817  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:06:44.548872  8898 net.cpp:67] Creating Layer label
I0626 03:06:44.548895  8898 net.cpp:358] label -> label
I0626 03:06:44.548925  8898 net.cpp:96] Setting up label
I0626 03:06:44.548943  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:06:44.663431  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:06:44.663555  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:44.663591  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:06:44.663611  8898 net.cpp:396] label_label_0_split <- label
I0626 03:06:44.663635  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:06:44.663662  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:06:44.663695  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:06:44.663715  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:44.663731  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:44.663753  8898 net.cpp:67] Creating Layer inner1
I0626 03:06:44.663769  8898 net.cpp:396] inner1 <- data
I0626 03:06:44.663790  8898 net.cpp:358] inner1 -> inner1
I0626 03:06:44.663815  8898 net.cpp:96] Setting up inner1
I0626 03:06:44.691215  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:44.691256  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:06:44.691267  8898 net.cpp:396] inner1relu <- inner1
I0626 03:06:44.691277  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:06:44.691289  8898 net.cpp:96] Setting up inner1relu
I0626 03:06:44.691298  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:44.691308  8898 net.cpp:67] Creating Layer inner2
I0626 03:06:44.691316  8898 net.cpp:396] inner2 <- inner1
I0626 03:06:44.691325  8898 net.cpp:358] inner2 -> inner2
I0626 03:06:44.691336  8898 net.cpp:96] Setting up inner2
I0626 03:06:44.694181  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:44.694200  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:06:44.694207  8898 net.cpp:396] inner2relu <- inner2
I0626 03:06:44.694216  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:06:44.694226  8898 net.cpp:96] Setting up inner2relu
I0626 03:06:44.694233  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:44.694243  8898 net.cpp:67] Creating Layer inner3
I0626 03:06:44.694252  8898 net.cpp:396] inner3 <- inner2
I0626 03:06:44.694260  8898 net.cpp:358] inner3 -> inner3
I0626 03:06:44.694270  8898 net.cpp:96] Setting up inner3
I0626 03:06:44.706351  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:44.706404  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:06:44.706414  8898 net.cpp:396] inner3relu <- inner3
I0626 03:06:44.706426  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:06:44.706437  8898 net.cpp:96] Setting up inner3relu
I0626 03:06:44.706445  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:44.706456  8898 net.cpp:67] Creating Layer output
I0626 03:06:44.706465  8898 net.cpp:396] output <- inner3
I0626 03:06:44.706475  8898 net.cpp:358] output -> output
I0626 03:06:44.706485  8898 net.cpp:96] Setting up output
I0626 03:06:44.707687  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:06:44.707706  8898 net.cpp:67] Creating Layer loss
I0626 03:06:44.707715  8898 net.cpp:396] loss <- output
I0626 03:06:44.707736  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:06:44.707746  8898 net.cpp:358] loss -> loss
I0626 03:06:44.707759  8898 net.cpp:358] loss -> std
I0626 03:06:44.707770  8898 net.cpp:358] loss -> ind
I0626 03:06:44.707782  8898 net.cpp:358] loss -> proba
I0626 03:06:44.707790  8898 net.cpp:96] Setting up loss
I0626 03:06:44.707798  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:06:44.707813  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:44.707821  8898 net.cpp:109]     with loss weight 1
I0626 03:06:44.707837  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:44.707845  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:06:44.707852  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:44.707865  8898 net.cpp:67] Creating Layer silence
I0626 03:06:44.707873  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:06:44.707882  8898 net.cpp:396] silence <- ind
I0626 03:06:44.707890  8898 net.cpp:396] silence <- proba
I0626 03:06:44.707898  8898 net.cpp:96] Setting up silence
I0626 03:06:44.707906  8898 net.cpp:172] silence does not need backward computation.
I0626 03:06:44.707913  8898 net.cpp:170] loss needs backward computation.
I0626 03:06:44.707921  8898 net.cpp:170] output needs backward computation.
I0626 03:06:44.707929  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:06:44.707937  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:06:44.707943  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:06:44.707952  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:06:44.707958  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:06:44.707965  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:06:44.707973  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:06:44.707980  8898 net.cpp:172] label does not need backward computation.
I0626 03:06:44.707988  8898 net.cpp:172] data does not need backward computation.
I0626 03:06:44.707994  8898 net.cpp:208] This network produces output loss
I0626 03:06:44.708003  8898 net.cpp:208] This network produces output std
I0626 03:06:44.708016  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:06:44.708026  8898 net.cpp:219] Network initialization done.
I0626 03:06:44.708034  8898 net.cpp:220] Memory required for data: 9903112
I0626 03:06:46.758009 17783 caffe.cpp:99] Use GPU with device ID 0
I0626 03:06:47.002315 17783 caffe.cpp:107] Starting Optimization
I0626 03:06:47.002400 17783 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:06:47.002425 17783 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:06:47.002617 17783 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:06:47.002632 17783 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:06:47.002727 17783 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:06:47.002806 17783 net.cpp:67] Creating Layer data
I0626 03:06:47.002820 17783 net.cpp:358] data -> data
I0626 03:06:47.002835 17783 net.cpp:96] Setting up data
I0626 03:06:47.002871 17783 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:06:47.106942 17783 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:06:47.107089 17783 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:06:47.107123 17783 net.cpp:67] Creating Layer label
I0626 03:06:47.107148 17783 net.cpp:358] label -> label
I0626 03:06:47.107174 17783 net.cpp:96] Setting up label
I0626 03:06:47.107192 17783 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:06:47.224089 17783 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:06:47.224211 17783 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:47.224251 17783 net.cpp:67] Creating Layer label_label_0_split
I0626 03:06:47.224268 17783 net.cpp:396] label_label_0_split <- label
I0626 03:06:47.224304 17783 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:06:47.224333 17783 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:06:47.224352 17783 net.cpp:96] Setting up label_label_0_split
I0626 03:06:47.224380 17783 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:47.224396 17783 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:47.224417 17783 net.cpp:67] Creating Layer inner1
I0626 03:06:47.224434 17783 net.cpp:396] inner1 <- data
I0626 03:06:47.224453 17783 net.cpp:358] inner1 -> inner1
I0626 03:06:47.224476 17783 net.cpp:96] Setting up inner1
I0626 03:06:47.251415 17783 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:47.251476 17783 net.cpp:67] Creating Layer inner1relu
I0626 03:06:47.251487 17783 net.cpp:396] inner1relu <- inner1
I0626 03:06:47.251498 17783 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:06:47.251509 17783 net.cpp:96] Setting up inner1relu
I0626 03:06:47.251525 17783 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:47.251536 17783 net.cpp:67] Creating Layer inner2
I0626 03:06:47.251544 17783 net.cpp:396] inner2 <- inner1
I0626 03:06:47.251554 17783 net.cpp:358] inner2 -> inner2
I0626 03:06:47.251565 17783 net.cpp:96] Setting up inner2
I0626 03:06:47.254508 17783 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:47.254534 17783 net.cpp:67] Creating Layer inner2relu
I0626 03:06:47.254544 17783 net.cpp:396] inner2relu <- inner2
I0626 03:06:47.254552 17783 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:06:47.254561 17783 net.cpp:96] Setting up inner2relu
I0626 03:06:47.254570 17783 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:47.254580 17783 net.cpp:67] Creating Layer inner3
I0626 03:06:47.254587 17783 net.cpp:396] inner3 <- inner2
I0626 03:06:47.254596 17783 net.cpp:358] inner3 -> inner3
I0626 03:06:47.254606 17783 net.cpp:96] Setting up inner3
I0626 03:06:47.266404 17783 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:47.266438 17783 net.cpp:67] Creating Layer inner3relu
I0626 03:06:47.266448 17783 net.cpp:396] inner3relu <- inner3
I0626 03:06:47.266458 17783 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:06:47.266468 17783 net.cpp:96] Setting up inner3relu
I0626 03:06:47.266484 17783 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:47.266492 17783 net.cpp:67] Creating Layer output
I0626 03:06:47.266500 17783 net.cpp:396] output <- inner3
I0626 03:06:47.266510 17783 net.cpp:358] output -> output
I0626 03:06:47.266520 17783 net.cpp:96] Setting up output
I0626 03:06:47.267772 17783 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:06:47.267791 17783 net.cpp:67] Creating Layer loss
I0626 03:06:47.267799 17783 net.cpp:396] loss <- output
I0626 03:06:47.267808 17783 net.cpp:396] loss <- label_label_0_split_0
I0626 03:06:47.267819 17783 net.cpp:358] loss -> loss
I0626 03:06:47.267832 17783 net.cpp:358] loss -> std
I0626 03:06:47.267843 17783 net.cpp:358] loss -> ind
I0626 03:06:47.267854 17783 net.cpp:358] loss -> proba
I0626 03:06:47.267864 17783 net.cpp:96] Setting up loss
I0626 03:06:47.267876 17783 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:06:47.267891 17783 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:47.267899 17783 net.cpp:109]     with loss weight 1
I0626 03:06:47.267928 17783 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:47.267937 17783 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:06:47.267946 17783 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:47.267958 17783 net.cpp:67] Creating Layer silence
I0626 03:06:47.267967 17783 net.cpp:396] silence <- label_label_0_split_1
I0626 03:06:47.267976 17783 net.cpp:396] silence <- ind
I0626 03:06:47.267985 17783 net.cpp:396] silence <- proba
I0626 03:06:47.267994 17783 net.cpp:96] Setting up silence
I0626 03:06:47.268023 17783 net.cpp:172] silence does not need backward computation.
I0626 03:06:47.268030 17783 net.cpp:170] loss needs backward computation.
I0626 03:06:47.268039 17783 net.cpp:170] output needs backward computation.
I0626 03:06:47.268048 17783 net.cpp:170] inner3relu needs backward computation.
I0626 03:06:47.268054 17783 net.cpp:170] inner3 needs backward computation.
I0626 03:06:47.268062 17783 net.cpp:170] inner2relu needs backward computation.
I0626 03:06:47.268070 17783 net.cpp:170] inner2 needs backward computation.
I0626 03:06:47.268079 17783 net.cpp:170] inner1relu needs backward computation.
I0626 03:06:47.268086 17783 net.cpp:170] inner1 needs backward computation.
I0626 03:06:47.268095 17783 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:06:47.268102 17783 net.cpp:172] label does not need backward computation.
I0626 03:06:47.268110 17783 net.cpp:172] data does not need backward computation.
I0626 03:06:47.268118 17783 net.cpp:208] This network produces output loss
I0626 03:06:47.268126 17783 net.cpp:208] This network produces output std
I0626 03:06:47.268141 17783 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:06:47.268151 17783 net.cpp:219] Network initialization done.
I0626 03:06:47.268163 17783 net.cpp:220] Memory required for data: 9903112
I0626 03:06:47.268204 17783 solver.cpp:41] Solver scaffolding done.
I0626 03:06:47.268213 17783 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:06:47.279002 17783 solver.cpp:160] Solving net
I0626 03:06:47.300865 17783 solver.cpp:192] Iteration 0, loss = 0.0381427
I0626 03:06:47.300931 17783 solver.cpp:207]     Train net output #0: loss = 0.0381427 (* 1 = 0.0381427 loss)
I0626 03:06:47.300945 17783 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:47.300961 17783 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:06:47.396004 17783 solver.cpp:192] Iteration 10, loss = 0.0247912
I0626 03:06:47.396071 17783 solver.cpp:207]     Train net output #0: loss = 0.0247912 (* 1 = 0.0247912 loss)
I0626 03:06:47.396087 17783 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:47.396102 17783 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:06:47.509714 17783 solver.cpp:192] Iteration 20, loss = 0.0382026
I0626 03:06:47.509790 17783 solver.cpp:207]     Train net output #0: loss = 0.0382026 (* 1 = 0.0382026 loss)
I0626 03:06:47.509810 17783 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:47.509829 17783 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:06:47.620524 17783 solver.cpp:192] Iteration 30, loss = 0.0216176
I0626 03:06:47.620604 17783 solver.cpp:207]     Train net output #0: loss = 0.0216176 (* 1 = 0.0216176 loss)
I0626 03:06:47.620626 17783 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:47.620645 17783 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:06:47.738328 17783 solver.cpp:192] Iteration 40, loss = 0.00781815
I0626 03:06:47.738389 17783 solver.cpp:207]     Train net output #0: loss = 0.00781815 (* 1 = 0.00781815 loss)
I0626 03:06:47.738401 17783 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:47.738414 17783 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:06:47.846248 17783 solver.cpp:192] Iteration 50, loss = 0.0101112
I0626 03:06:47.846323 17783 solver.cpp:207]     Train net output #0: loss = 0.0101112 (* 1 = 0.0101112 loss)
I0626 03:06:47.846341 17783 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:47.846359 17783 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:06:47.953950 17783 solver.cpp:192] Iteration 60, loss = 0.00549281
I0626 03:06:47.954026 17783 solver.cpp:207]     Train net output #0: loss = 0.00549281 (* 1 = 0.00549281 loss)
I0626 03:06:47.954046 17783 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:47.954063 17783 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:06:48.054493 17783 solver.cpp:192] Iteration 70, loss = 0.0055915
I0626 03:06:48.054569 17783 solver.cpp:207]     Train net output #0: loss = 0.0055915 (* 1 = 0.0055915 loss)
I0626 03:06:48.054610 17783 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:48.054626 17783 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:06:48.163303 17783 solver.cpp:192] Iteration 80, loss = 0.00630292
I0626 03:06:48.163358 17783 solver.cpp:207]     Train net output #0: loss = 0.00630292 (* 1 = 0.00630292 loss)
I0626 03:06:48.163369 17783 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:48.163381 17783 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:06:48.269691 17783 solver.cpp:192] Iteration 90, loss = 0.00661903
I0626 03:06:48.269750 17783 solver.cpp:207]     Train net output #0: loss = 0.00661903 (* 1 = 0.00661903 loss)
I0626 03:06:48.269762 17783 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:48.269773 17783 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:06:48.382833 17783 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:06:48.421373 17783 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:06:48.447053 17783 solver.cpp:229] Iteration 100, loss = 0.00669295
I0626 03:06:48.447100 17783 solver.cpp:234] Optimization Done.
I0626 03:06:48.447109 17783 caffe.cpp:121] Optimization Done.
I0626 03:06:48.510195  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:06:48.510226  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:06:48.510337  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:06:48.510422  8898 net.cpp:67] Creating Layer data
I0626 03:06:48.510435  8898 net.cpp:358] data -> data
I0626 03:06:48.510450  8898 net.cpp:96] Setting up data
I0626 03:06:48.510460  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:06:48.700031  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:06:48.701792  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:06:48.701843  8898 net.cpp:67] Creating Layer label
I0626 03:06:48.701865  8898 net.cpp:358] label -> label
I0626 03:06:48.701898  8898 net.cpp:96] Setting up label
I0626 03:06:48.701917  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:06:48.784658  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:06:48.784766  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:48.784801  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:06:48.784818  8898 net.cpp:396] label_label_0_split <- label
I0626 03:06:48.784842  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:06:48.784869  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:06:48.784895  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:06:48.784915  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:48.784931  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:48.784952  8898 net.cpp:67] Creating Layer inner1
I0626 03:06:48.784968  8898 net.cpp:396] inner1 <- data
I0626 03:06:48.784989  8898 net.cpp:358] inner1 -> inner1
I0626 03:06:48.785029  8898 net.cpp:96] Setting up inner1
I0626 03:06:48.811614  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:48.811663  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:06:48.811674  8898 net.cpp:396] inner1relu <- inner1
I0626 03:06:48.811686  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:06:48.811697  8898 net.cpp:96] Setting up inner1relu
I0626 03:06:48.811707  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:48.811717  8898 net.cpp:67] Creating Layer inner2
I0626 03:06:48.811724  8898 net.cpp:396] inner2 <- inner1
I0626 03:06:48.811734  8898 net.cpp:358] inner2 -> inner2
I0626 03:06:48.811745  8898 net.cpp:96] Setting up inner2
I0626 03:06:48.814783  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:48.814801  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:06:48.814810  8898 net.cpp:396] inner2relu <- inner2
I0626 03:06:48.814821  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:06:48.814829  8898 net.cpp:96] Setting up inner2relu
I0626 03:06:48.814837  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:48.814847  8898 net.cpp:67] Creating Layer inner3
I0626 03:06:48.814855  8898 net.cpp:396] inner3 <- inner2
I0626 03:06:48.814864  8898 net.cpp:358] inner3 -> inner3
I0626 03:06:48.814874  8898 net.cpp:96] Setting up inner3
I0626 03:06:48.826612  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:48.826663  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:06:48.826671  8898 net.cpp:396] inner3relu <- inner3
I0626 03:06:48.826683  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:06:48.826694  8898 net.cpp:96] Setting up inner3relu
I0626 03:06:48.826701  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:48.826712  8898 net.cpp:67] Creating Layer output
I0626 03:06:48.826719  8898 net.cpp:396] output <- inner3
I0626 03:06:48.826730  8898 net.cpp:358] output -> output
I0626 03:06:48.826740  8898 net.cpp:96] Setting up output
I0626 03:06:48.827910  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:06:48.827929  8898 net.cpp:67] Creating Layer loss
I0626 03:06:48.827937  8898 net.cpp:396] loss <- output
I0626 03:06:48.827947  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:06:48.827960  8898 net.cpp:358] loss -> loss
I0626 03:06:48.827972  8898 net.cpp:358] loss -> std
I0626 03:06:48.827983  8898 net.cpp:358] loss -> ind
I0626 03:06:48.827996  8898 net.cpp:358] loss -> proba
I0626 03:06:48.828006  8898 net.cpp:96] Setting up loss
I0626 03:06:48.828013  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:06:48.828029  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:48.828037  8898 net.cpp:109]     with loss weight 1
I0626 03:06:48.828053  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:48.828060  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:06:48.828068  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:48.828080  8898 net.cpp:67] Creating Layer silence
I0626 03:06:48.828088  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:06:48.828106  8898 net.cpp:396] silence <- ind
I0626 03:06:48.828115  8898 net.cpp:396] silence <- proba
I0626 03:06:48.828124  8898 net.cpp:96] Setting up silence
I0626 03:06:48.828131  8898 net.cpp:172] silence does not need backward computation.
I0626 03:06:48.828138  8898 net.cpp:170] loss needs backward computation.
I0626 03:06:48.828146  8898 net.cpp:170] output needs backward computation.
I0626 03:06:48.828155  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:06:48.828163  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:06:48.828178  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:06:48.828186  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:06:48.828192  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:06:48.828199  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:06:48.828207  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:06:48.828217  8898 net.cpp:172] label does not need backward computation.
I0626 03:06:48.828234  8898 net.cpp:172] data does not need backward computation.
I0626 03:06:48.828241  8898 net.cpp:208] This network produces output loss
I0626 03:06:48.828249  8898 net.cpp:208] This network produces output std
I0626 03:06:48.828267  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:06:48.828277  8898 net.cpp:219] Network initialization done.
I0626 03:06:48.828285  8898 net.cpp:220] Memory required for data: 9903112
(2307, 3600)
Cluster Centers:  [[-0.29276978  0.36116645  1.36797777  1.00212963  0.65368893  0.1506435
  -0.16773861  0.05424868 -0.85875336  0.05396479  0.53924296 -0.80810245
  -0.06291847 -0.25260168  0.71288375  1.1507308   0.32690922  0.68083251
   0.81383485 -1.25540115 -0.14794669 -0.36350759 -0.34703712  1.07164555
  -1.01557256  2.17207797  0.72656719 -0.34573944 -0.34580113 -1.40451642
  -0.42907095 -2.10073645 -0.34290998  1.42882306  1.16609268 -0.6409052
  -0.93455666  1.44052436  0.07255363  0.67939095 -0.06593637  1.03956457
  -1.29096054 -1.38267385 -0.3654939   0.58507258 -0.5409591   1.33045857
   1.00408634  0.15437445]
 [ 0.24871744 -0.06438863  0.94537783  0.2413819   0.16097121 -0.18164237
  -0.35170593  0.294422   -0.43747225 -0.15784643 -0.09375789 -0.16253493
   0.1601597  -0.02341769  0.37686554  0.94228921  0.2506425   0.4825645
   0.69058499 -0.48022324  0.0457314   0.07267141 -0.30137618  0.3507082
  -0.56422177  1.2398867   0.21315876 -0.65067614  0.25790452 -0.64329297
  -0.67286687 -1.13664753  0.17405848  0.75566242  0.79862017 -0.37261478
  -0.28872522  0.88707508 -0.12373375 -0.10773341  0.03686674  0.16981796
  -1.14253288 -0.71285068  0.49261015  0.83346074 -0.39570067  0.49801627
   0.54074238  0.12003729]
 [ 0.44749441 -0.35422623  1.05249198  0.81385561  0.40410015 -0.22304909
  -1.02641417  0.48339299 -0.78456941 -0.15671595 -0.51885805 -0.16381564
   0.61064706 -0.37396542  1.11516138  0.84665345  0.31534329  0.33345546
   0.41907023 -0.78652205 -0.34831909  0.16184643 -0.24588026  0.35872404
  -0.64967531  2.00314189  0.24175525 -0.22719063  0.05071974 -0.80492212
  -0.34909762 -1.0620681  -0.07838299  1.3297217   0.98093497 -0.26830099
  -0.24390809  0.80721283 -0.02979958  0.10803503 -0.06629837  0.77971501
  -2.11372041 -1.00766986  0.30336867  0.99405452 -0.49466159  0.42210391
   0.21446456 -0.37037062]
 [ 0.28225257 -0.35084727  0.90211554  0.33452291  0.46042586  0.24080193
  -1.33846212  0.31968858 -0.8430694  -0.03592946 -0.18846807 -0.44853331
   0.61872675 -0.57094877  0.70655093  1.16842063  0.66948793  0.84900588
   0.40067047 -1.17612013 -0.20802997  0.06236041 -0.4072503   1.13450769
  -0.97230791  2.17898732  1.0697559  -1.43459875  0.33050196 -1.49005206
  -0.69607299 -1.35297124 -0.23849932  1.25768544  1.6186789  -0.12844015
  -0.35835575  0.71720979  0.43971424 -0.01542648 -0.30489685  0.47549772
  -1.75651526 -0.83078722 -0.07174376  1.14053954 -0.34922205  0.84411759
   0.91743353 -0.19822999]
 [-0.19486996 -0.58562982  1.25581694  0.74432499  0.18808418  0.04293922
  -0.78320625  0.32937712 -0.70313739 -0.23140263 -0.01619567 -0.45458582
  -0.20345404 -0.32534355  0.67517233  1.06536032  0.3217747   0.45114322
   0.8044988  -0.66591209 -0.32207337 -0.54714471 -0.61494519  0.99352069
  -0.55112521  1.6887843   0.4220967  -0.98931781  0.20511016 -0.94626969
  -0.54688209 -1.07286862 -0.37252137  1.18395171  1.13609504 -0.3153482
  -0.67607129  1.07700546  0.40848076 -0.13760755 -0.05749785  0.42765356
  -1.79357138 -0.62723708  0.09487437  1.4078253  -0.78179696  0.91749625
   0.64809327  0.26236342]]
[2 4 4 ..., 4 3 4]
0.92977893368
[[ 0.06963312  0.27796984  0.37947184  0.11606293  0.15686226]
 [ 0.14016345  0.16032496  0.15190257  0.19033652  0.35727251]
 [ 0.13072375  0.1520142   0.16111597  0.18067099  0.37547509]
 [ 0.07465279  0.2337969   0.40332389  0.13624609  0.15198032]
 [ 0.13720145  0.14502264  0.15526432  0.17856786  0.38394374]
 [ 0.07334494  0.19170699  0.45684683  0.13133954  0.1467617 ]
 [ 0.10704167  0.12698742  0.14933385  0.17055896  0.4460781 ]
 [ 0.0753136   0.19031944  0.45218388  0.14087096  0.14131212]
 [ 0.06774034  0.18994034  0.48177618  0.13206185  0.12848129]
 [ 0.13261828  0.12140207  0.14095643  0.17821853  0.42680469]]
[2 4 4 ..., 4 3 4]
0.0446467273515
[[ 0.04375802  0.3170608   0.44362694  0.07692792  0.11862632]
 [ 0.09620796  0.08497199  0.09542742  0.13670634  0.58668629]
 [ 0.08657363  0.08263329  0.10067001  0.11408879  0.61603428]
 [ 0.04698325  0.24225916  0.51789798  0.08420756  0.10865204]
 [ 0.10097734  0.078678I0626 03:06:50.272403 18014 caffe.cpp:99] Use GPU with device ID 0
I0626 03:06:50.515585 18014 caffe.cpp:107] Starting Optimization
I0626 03:06:50.515673 18014 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:06:50.515697 18014 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:06:50.515889 18014 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:06:50.515903 18014 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:06:50.516001 18014 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000223"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000223"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:06:50.516077 18014 net.cpp:67] Creating Layer data
I0626 03:06:50.516098 18014 net.cpp:358] data -> data
I0626 03:06:50.516125 18014 net.cpp:96] Setting up data
I0626 03:06:50.516161 18014 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:06:50.603307 18014 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:06:50.603526 18014 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:06:50.603567 18014 net.cpp:67] Creating Layer label
I0626 03:06:50.603590 18014 net.cpp:358] label -> label
I0626 03:06:50.603622 18014 net.cpp:96] Setting up label
I0626 03:06:50.603642 18014 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:06:50.720396 18014 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:06:50.720526 18014 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:50.720558 18014 net.cpp:67] Creating Layer label_label_0_split
I0626 03:06:50.720576 18014 net.cpp:396] label_label_0_split <- label
I0626 03:06:50.720610 18014 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:06:50.720638 18014 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:06:50.720659 18014 net.cpp:96] Setting up label_label_0_split
I0626 03:06:50.720685 18014 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:50.720700 18014 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:50.720721 18014 net.cpp:67] Creating Layer inner1
I0626 03:06:50.720736 18014 net.cpp:396] inner1 <- data
I0626 03:06:50.720757 18014 net.cpp:358] inner1 -> inner1
I0626 03:06:50.720778 18014 net.cpp:96] Setting up inner1
I0626 03:06:50.748461 18014 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:50.748519 18014 net.cpp:67] Creating Layer inner1relu
I0626 03:06:50.748530 18014 net.cpp:396] inner1relu <- inner1
I0626 03:06:50.748543 18014 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:06:50.748554 18014 net.cpp:96] Setting up inner1relu
I0626 03:06:50.748569 18014 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:50.748580 18014 net.cpp:67] Creating Layer inner2
I0626 03:06:50.748589 18014 net.cpp:396] inner2 <- inner1
I0626 03:06:50.748600 18014 net.cpp:358] inner2 -> inner2
I0626 03:06:50.748610 18014 net.cpp:96] Setting up inner2
I0626 03:06:50.751651 18014 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:50.751668 18014 net.cpp:67] Creating Layer inner2relu
I0626 03:06:50.751677 18014 net.cpp:396] inner2relu <- inner2
I0626 03:06:50.751688 18014 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:06:50.751696 18014 net.cpp:96] Setting up inner2relu
I0626 03:06:50.751705 18014 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:50.751715 18014 net.cpp:67] Creating Layer inner3
I0626 03:06:50.751724 18014 net.cpp:396] inner3 <- inner2
I0626 03:06:50.751734 18014 net.cpp:358] inner3 -> inner3
I0626 03:06:50.751744 18014 net.cpp:96] Setting up inner3
I0626 03:06:50.763720 18014 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:50.763753 18014 net.cpp:67] Creating Layer inner3relu
I0626 03:06:50.763762 18014 net.cpp:396] inner3relu <- inner3
I0626 03:06:50.763773 18014 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:06:50.763783 18014 net.cpp:96] Setting up inner3relu
I0626 03:06:50.763792 18014 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:50.763803 18014 net.cpp:67] Creating Layer output
I0626 03:06:50.763810 18014 net.cpp:396] output <- inner3
I0626 03:06:50.763819 18014 net.cpp:358] output -> output
I0626 03:06:50.763830 18014 net.cpp:96] Setting up output
I0626 03:06:50.765019 18014 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:06:50.765036 18014 net.cpp:67] Creating Layer loss
I0626 03:06:50.765045 18014 net.cpp:396] loss <- output
I0626 03:06:50.765054 18014 net.cpp:396] loss <- label_label_0_split_0
I0626 03:06:50.765065 18014 net.cpp:358] loss -> loss
I0626 03:06:50.765077 18014 net.cpp:358] loss -> std
I0626 03:06:50.765089 18014 net.cpp:358] loss -> ind
I0626 03:06:50.765100 18014 net.cpp:358] loss -> proba
I0626 03:06:50.765110 18014 net.cpp:96] Setting up loss
I0626 03:06:50.765121 18014 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:06:50.765136 18014 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:50.765156 18014 net.cpp:109]     with loss weight 1
I0626 03:06:50.765184 18014 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:50.765192 18014 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:06:50.765200 18014 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:50.765214 18014 net.cpp:67] Creating Layer silence
I0626 03:06:50.765223 18014 net.cpp:396] silence <- label_label_0_split_1
I0626 03:06:50.765231 18014 net.cpp:396] silence <- ind
I0626 03:06:50.765240 18014 net.cpp:396] silence <- proba
I0626 03:06:50.765249 18014 net.cpp:96] Setting up silence
I0626 03:06:50.765269 18014 net.cpp:172] silence does not need backward computation.
I0626 03:06:50.765277 18014 net.cpp:170] loss needs backward computation.
I0626 03:06:50.765285 18014 net.cpp:170] output needs backward computation.
I0626 03:06:50.765295 18014 net.cpp:170] inner3relu needs backward computation.
I0626 03:06:50.765301 18014 net.cpp:170] inner3 needs backward computation.
I0626 03:06:50.765310 18014 net.cpp:170] inner2relu needs backward computation.
I0626 03:06:50.765317 18014 net.cpp:170] inner2 needs backward computation.
I0626 03:06:50.765324 18014 net.cpp:170] inner1relu needs backward computation.
I0626 03:06:50.765332 18014 net.cpp:170] inner1 needs backward computation.
I0626 03:06:50.765341 18014 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:06:50.765348 18014 net.cpp:172] label does not need backward computation.
I0626 03:06:50.765355 18014 net.cpp:172] data does not need backward computation.
I0626 03:06:50.765363 18014 net.cpp:208] This network produces output loss
I0626 03:06:50.765372 18014 net.cpp:208] This network produces output std
I0626 03:06:50.765385 18014 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:06:50.765395 18014 net.cpp:219] Network initialization done.
I0626 03:06:50.765408 18014 net.cpp:220] Memory required for data: 9903112
I0626 03:06:50.765447 18014 solver.cpp:41] Solver scaffolding done.
I0626 03:06:50.765456 18014 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:06:50.776455 18014 solver.cpp:160] Solving net
I0626 03:06:50.797926 18014 solver.cpp:192] Iteration 0, loss = 0.0800229
I0626 03:06:50.797976 18014 solver.cpp:207]     Train net output #0: loss = 0.0800229 (* 1 = 0.0800229 loss)
I0626 03:06:50.797988 18014 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:50.798012 18014 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:06:50.901906 18014 solver.cpp:192] Iteration 10, loss = 0.102711
I0626 03:06:50.901964 18014 solver.cpp:207]     Train net output #0: loss = 0.102711 (* 1 = 0.102711 loss)
I0626 03:06:50.901978 18014 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:50.901989 18014 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:06:51.006988 18014 solver.cpp:192] Iteration 20, loss = 0.0700993
I0626 03:06:51.007066 18014 solver.cpp:207]     Train net output #0: loss = 0.0700993 (* 1 = 0.0700993 loss)
I0626 03:06:51.007086 18014 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:51.007105 18014 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:06:51.121142 18014 solver.cpp:192] Iteration 30, loss = 0.0700021
I0626 03:06:51.121222 18014 solver.cpp:207]     Train net output #0: loss = 0.0700021 (* 1 = 0.0700021 loss)
I0626 03:06:51.121243 18014 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:51.121264 18014 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:06:51.232985 18014 solver.cpp:192] Iteration 40, loss = 0.0711815
I0626 03:06:51.233062 18014 solver.cpp:207]     Train net output #0: loss = 0.0711815 (* 1 = 0.0711815 loss)
I0626 03:06:51.233080 18014 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:51.233099 18014 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:06:51.345441 18014 solver.cpp:192] Iteration 50, loss = 0.0501085
I0626 03:06:51.345515 18014 solver.cpp:207]     Train net output #0: loss = 0.0501085 (* 1 = 0.0501085 loss)
I0626 03:06:51.345532 18014 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:51.345564 18014 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:06:51.460762 18014 solver.cpp:192] Iteration 60, loss = 0.0369945
I0626 03:06:51.460834 18014 solver.cpp:207]     Train net output #0: loss = 0.0369945 (* 1 = 0.0369945 loss)
I0626 03:06:51.460853 18014 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:51.460871 18014 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:06:51.572999 18014 solver.cpp:192] Iteration 70, loss = 0.0430197
I0626 03:06:51.573072 18014 solver.cpp:207]     Train net output #0: loss = 0.0430197 (* 1 = 0.0430197 loss)
I0626 03:06:51.573109 18014 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:51.573125 18014 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:06:51.686944 18014 solver.cpp:192] Iteration 80, loss = 0.0282573
I0626 03:06:51.686992 18014 solver.cpp:207]     Train net output #0: loss = 0.0282573 (* 1 = 0.0282573 loss)
I0626 03:06:51.687005 18014 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:51.687016 18014 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:06:51.796845 18014 solver.cpp:192] Iteration 90, loss = 0.033749
I0626 03:06:51.796893 18014 solver.cpp:207]     Train net output #0: loss = 0.033749 (* 1 = 0.033749 loss)
I0626 03:06:51.796905 18014 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:51.796917 18014 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:06:51.905413 18014 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:06:51.947603 18014 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:06:51.975847 18014 solver.cpp:229] Iteration 100, loss = 0.0543588
I0626 03:06:51.975893 18014 solver.cpp:234] Optimization Done.
I0626 03:06:51.975903 18014 caffe.cpp:121] Optimization Done.
I0626 03:06:52.044071  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:06:52.044102  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:06:52.044219  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:06:52.044306  8898 net.cpp:67] Creating Layer data
I0626 03:06:52.044319  8898 net.cpp:358] data -> data
I0626 03:06:52.044335  8898 net.cpp:96] Setting up data
I0626 03:06:52.044345  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:06:52.254617  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:06:52.256392  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:06:52.256446  8898 net.cpp:67] Creating Layer label
I0626 03:06:52.256469  8898 net.cpp:358] label -> label
I0626 03:06:52.256500  8898 net.cpp:96] Setting up label
I0626 03:06:52.256520  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:06:52.355036  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:06:52.355151  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:52.355190  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:06:52.355208  8898 net.cpp:396] label_label_0_split <- label
I0626 03:06:52.355232  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:06:52.355262  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:06:52.355288  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:06:52.355306  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:52.355322  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:52.355345  8898 net.cpp:67] Creating Layer inner1
I0626 03:06:52.355361  8898 net.cpp:396] inner1 <- data
I0626 03:06:52.355381  8898 net.cpp:358] inner1 -> inner1
I0626 03:06:52.355406  8898 net.cpp:96] Setting up inner1
I0626 03:06:52.384501  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:52.384552  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:06:52.384562  8898 net.cpp:396] inner1relu <- inner1
I0626 03:06:52.384572  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:06:52.384582  8898 net.cpp:96] Setting up inner1relu
I0626 03:06:52.384591  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:52.384601  8898 net.cpp:67] Creating Layer inner2
I0626 03:06:52.384609  8898 net.cpp:396] inner2 <- inner1
I0626 03:06:52.384618  8898 net.cpp:358] inner2 -> inner2
I0626 03:06:52.384629  8898 net.cpp:96] Setting up inner2
I0626 03:06:52.387667  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:52.387684  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:06:52.387692  8898 net.cpp:396] inner2relu <- inner2
I0626 03:06:52.387702  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:06:52.387712  8898 net.cpp:96] Setting up inner2relu
I0626 03:06:52.387719  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:52.387729  8898 net.cpp:67] Creating Layer inner3
I0626 03:06:52.387737  8898 net.cpp:396] inner3 <- inner2
I0626 03:06:52.387747  8898 net.cpp:358] inner3 -> inner3
I0626 03:06:52.387756  8898 net.cpp:96] Setting up inner3
I0626 03:06:52.399487  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:52.399518  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:06:52.399528  8898 net.cpp:396] inner3relu <- inner3
I0626 03:06:52.399540  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:06:52.399551  8898 net.cpp:96] Setting up inner3relu
I0626 03:06:52.399559  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:52.399569  8898 net.cpp:67] Creating Layer output
I0626 03:06:52.399577  8898 net.cpp:396] output <- inner3
I0626 03:06:52.399587  8898 net.cpp:358] output -> output
I0626 03:06:52.399605  8898 net.cpp:96] Setting up output
I0626 03:06:52.400813  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:06:52.400832  8898 net.cpp:67] Creating Layer loss
I0626 03:06:52.400840  8898 net.cpp:396] loss <- output
I0626 03:06:52.400848  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:06:52.400859  8898 net.cpp:358] loss -> loss
I0626 03:06:52.400873  8898 net.cpp:358] loss -> std
I0626 03:06:52.400884  8898 net.cpp:358] loss -> ind
I0626 03:06:52.400895  8898 net.cpp:358] loss -> proba
I0626 03:06:52.400904  8898 net.cpp:96] Setting up loss
I0626 03:06:52.400913  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:06:52.400926  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:52.400934  8898 net.cpp:109]     with loss weight 1
I0626 03:06:52.400950  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:52.400957  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:06:52.400965  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:52.400979  8898 net.cpp:67] Creating Layer silence
I0626 03:06:52.400988  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:06:52.400996  8898 net.cpp:396] silence <- ind
I0626 03:06:52.401005  8898 net.cpp:396] silence <- proba
I0626 03:06:52.401012  8898 net.cpp:96] Setting up silence
I0626 03:06:52.401021  8898 net.cpp:172] silence does not need backward computation.
I0626 03:06:52.401027  8898 net.cpp:170] loss needs backward computation.
I0626 03:06:52.401034  8898 net.cpp:170] output needs backward computation.
I0626 03:06:52.401042  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:06:52.401049  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:06:52.401057  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:06:52.401064  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:06:52.401072  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:06:52.401079  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:06:52.401087  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:06:52.401094  8898 net.cpp:172] label does not need backward computation.
I0626 03:06:52.401101  8898 net.cpp:172] data does not need backward computation.
I0626 03:06:52.401108  8898 net.cpp:208] This network produces output loss
I0626 03:06:52.401115  8898 net.cpp:208] This network produces output std
I0626 03:06:52.401129  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:06:52.401139  8898 net.cpp:219] Network initialization done.
I0626 03:06:52.401146  8898 net.cpp:220] Memory required for data: 9903112
I0626 03:06:53.876514 18245 caffe.cpp:99] Use GPU with device ID 0
I0626 03:06:54.119927 18245 caffe.cpp:107] Starting Optimization
I0626 03:06:54.120015 18245 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:06:54.120039 18245 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:06:54.120236 18245 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:06:54.120249 18245 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:06:54.120343 18245 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000446"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000446"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:06:54.120429 18245 net.cpp:67] Creating Layer data
I0626 03:06:54.120442 18245 net.cpp:358] data -> data
I0626 03:06:54.120460 18245 net.cpp:96] Setting up data
I0626 03:06:54.120491 18245 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:06:54.223636 18245 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:06:54.223814 18245 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:06:54.223853 18245 net.cpp:67] Creating Layer label
I0626 03:06:54.223875 18245 net.cpp:358] label -> label
I0626 03:06:54.223906 18245 net.cpp:96] Setting up label
I0626 03:06:54.223925 18245 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:06:54.349016 18245 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:06:54.349148 18245 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:54.349181 18245 net.cpp:67] Creating Layer label_label_0_split
I0626 03:06:54.349200 18245 net.cpp:396] label_label_0_split <- label
I0626 03:06:54.349238 18245 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:06:54.349267 18245 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:06:54.349289 18245 net.cpp:96] Setting up label_label_0_split
I0626 03:06:54.349318 18245 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:54.349334 18245 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:54.349356 18245 net.cpp:67] Creating Layer inner1
I0626 03:06:54.349373 18245 net.cpp:396] inner1 <- data
I0626 03:06:54.349395 18245 net.cpp:358] inner1 -> inner1
I0626 03:06:54.349419 18245 net.cpp:96] Setting up inner1
I0626 03:06:54.378288 18245 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:54.378335 18245 net.cpp:67] Creating Layer inner1relu
I0626 03:06:54.378345 18245 net.cpp:396] inner1relu <- inner1
I0626 03:06:54.378357 18245 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:06:54.378370 18245 net.cpp:96] Setting up inner1relu
I0626 03:06:54.378384 18245 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:54.378401 18245 net.cpp:67] Creating Layer inner2
I0626 03:06:54.378409 18245 net.cpp:396] inner2 <- inner1
I0626 03:06:54.378418 18245 net.cpp:358] inner2 -> inner2
I0626 03:06:54.378429 18245 net.cpp:96] Setting up inner2
I0626 03:06:54.381387 18245 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:54.381404 18245 net.cpp:67] Creating Layer inner2relu
I0626 03:06:54.381413 18245 net.cpp:396] inner2relu <- inner2
I0626 03:06:54.381422 18245 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:06:54.381433 18245 net.cpp:96] Setting up inner2relu
I0626 03:06:54.381440 18245 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:54.381449 18245 net.cpp:67] Creating Layer inner3
I0626 03:06:54.381458 18245 net.cpp:396] inner3 <- inner2
I0626 03:06:54.381467 18245 net.cpp:358] inner3 -> inner3
I0626 03:06:54.381477 18245 net.cpp:96] Setting up inner3
I0626 03:06:54.393214 18245 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:54.393247 18245 net.cpp:67] Creating Layer inner3relu
I0626 03:06:54.393256 18245 net.cpp:396] inner3relu <- inner3
I0626 03:06:54.393266 18245 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:06:54.393276 18245 net.cpp:96] Setting up inner3relu
I0626 03:06:54.393285 18245 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:54.393296 18245 net.cpp:67] Creating Layer output
I0626 03:06:54.393303 18245 net.cpp:396] output <- inner3
I0626 03:06:54.393312 18245 net.cpp:358] output -> output
I0626 03:06:54.393323 18245 net.cpp:96] Setting up output
I0626 03:06:54.394511 18245 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:06:54.394528 18245 net.cpp:67] Creating Layer loss
I0626 03:06:54.394536 18245 net.cpp:396] loss <- output
I0626 03:06:54.394546 18245 net.cpp:396] loss <- label_label_0_split_0
I0626 03:06:54.394556 18245 net.cpp:358] loss -> loss
I0626 03:06:54.394567 18245 net.cpp:358] loss -> std
I0626 03:06:54.394578 18245 net.cpp:358] loss -> ind
I0626 03:06:54.394589 18245 net.cpp:358] loss -> proba
I0626 03:06:54.394599 18245 net.cpp:96] Setting up loss
I0626 03:06:54.394611 18245 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:06:54.394626 18245 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:54.394634 18245 net.cpp:109]     with loss weight 1
I0626 03:06:54.394666 18245 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:54.394675 18245 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:06:54.394682 18245 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:54.394695 18245 net.cpp:67] Creating Layer silence
I0626 03:06:54.394704 18245 net.cpp:396] silence <- label_label_0_split_1
I0626 03:06:54.394713 18245 net.cpp:396] silence <- ind
I0626 03:06:54.394721 18245 net.cpp:396] silence <- proba
I0626 03:06:54.394731 18245 net.cpp:96] Setting up silence
I0626 03:06:54.394752 18245 net.cpp:172] silence does not need backward computation.
I0626 03:06:54.394760 18245 net.cpp:170] loss needs backward computation.
I0626 03:06:54.394768 18245 net.cpp:170] output needs backward computation.
I0626 03:06:54.394776 18245 net.cpp:170] inner3relu needs backward computation.
I0626 03:06:54.394784 18245 net.cpp:170] inner3 needs backward computation.
I0626 03:06:54.394791 18245 net.cpp:170] inner2relu needs backward computation.
I0626 03:06:54.394799 18245 net.cpp:170] inner2 needs backward computation.
I0626 03:06:54.394806 18245 net.cpp:170] inner1relu needs backward computation.
I0626 03:06:54.394814 18245 net.cpp:170] inner1 needs backward computation.
I0626 03:06:54.394821 18245 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:06:54.394829 18245 net.cpp:172] label does not need backward computation.
I0626 03:06:54.394837 18245 net.cpp:172] data does not need backward computation.
I0626 03:06:54.394845 18245 net.cpp:208] This network produces output loss
I0626 03:06:54.394852 18245 net.cpp:208] This network produces output std
I0626 03:06:54.394866 18245 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:06:54.394876 18245 net.cpp:219] Network initialization done.
I0626 03:06:54.394888 18245 net.cpp:220] Memory required for data: 9903112
I0626 03:06:54.394933 18245 solver.cpp:41] Solver scaffolding done.
I0626 03:06:54.394942 18245 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:06:54.405925 18245 solver.cpp:160] Solving net
I0626 03:06:54.428227 18245 solver.cpp:192] Iteration 0, loss = 0.154194
I0626 03:06:54.428273 18245 solver.cpp:207]     Train net output #0: loss = 0.154194 (* 1 = 0.154194 loss)
I0626 03:06:54.428285 18245 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:54.428308 18245 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:06:54.511013 18245 solver.cpp:192] Iteration 10, loss = 0.168645
I0626 03:06:54.511080 18245 solver.cpp:207]     Train net output #0: loss = 0.168645 (* 1 = 0.168645 loss)
I0626 03:06:54.511095 18245 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:54.511108 18245 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:06:54.618700 18245 solver.cpp:192] Iteration 20, loss = 0.142766
I0626 03:06:54.618770 18245 solver.cpp:207]     Train net output #0: loss = 0.142766 (* 1 = 0.142766 loss)
I0626 03:06:54.618788 18245 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:54.618803 18245 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:06:54.727602 18245 solver.cpp:192] Iteration 30, loss = 0.13988
I0626 03:06:54.727669 18245 solver.cpp:207]     Train net output #0: loss = 0.13988 (* 1 = 0.13988 loss)
I0626 03:06:54.727686 18245 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:54.727701 18245 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:06:54.833986 18245 solver.cpp:192] Iteration 40, loss = 0.200299
I0626 03:06:54.834044 18245 solver.cpp:207]     Train net output #0: loss = 0.200299 (* 1 = 0.200299 loss)
I0626 03:06:54.834061 18245 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:54.834077 18245 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:06:54.940330 18245 solver.cpp:192] Iteration 50, loss = 0.147407
I0626 03:06:54.940388 18245 solver.cpp:207]     Train net output #0: loss = 0.147407 (* 1 = 0.147407 loss)
I0626 03:06:54.940400 18245 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:54.940412 18245 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:06:55.045990 18245 solver.cpp:192] Iteration 60, loss = 0.11243
I0626 03:06:55.046067 18245 solver.cpp:207]     Train net output #0: loss = 0.11243 (* 1 = 0.11243 loss)
I0626 03:06:55.046087 18245 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:55.046106 18245 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:06:55.159052 18245 solver.cpp:192] Iteration 70, loss = 0.113399
I0626 03:06:55.159129 18245 solver.cpp:207]     Train net output #0: loss = 0.113399 (* 1 = 0.113399 loss)
I0626 03:06:55.159176 18245 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:55.159196 18245 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:06:55.273414 18245 solver.cpp:192] Iteration 80, loss = 0.171933
I0626 03:06:55.273474 18245 solver.cpp:207]     Train net output #0: loss = 0.171933 (* 1 = 0.171933 loss)
I0626 03:06:55.273485 18245 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:55.273497 18245 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:06:55.379443 18245 solver.cpp:192] Iteration 90, loss = 0.206704
I0626 03:06:55.379541 18245 solver.cpp:207]     Train net output #0: loss = 0.206704 (* 1 = 0.206704 loss)
I0626 03:06:55.379559 18245 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:55.379575 18245 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:06:55.492139 18245 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:06:55.533648 18245 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:06:55.561859 18245 solver.cpp:229] Iteration 100, loss = 0.124399
I0626 03:06:55.561908 18245 solver.cpp:234] Optimization Done.
I0626 03:06:55.561918 18245 caffe.cpp:121] Optimization Done.
I0626 03:06:55.633903  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:06:55.633940  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:06:55.634057  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:06:55.634132  8898 net.cpp:67] Creating Layer data
I0626 03:06:55.634145  8898 net.cpp:358] data -> data
I0626 03:06:55.634160  8898 net.cpp:96] Setting up data
I0626 03:06:55.634169  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:06:55.874904  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:06:55.876693  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:06:55.876752  8898 net.cpp:67] Creating Layer label
I0626 03:06:55.876776  8898 net.cpp:358] label -> label
I0626 03:06:55.876808  8898 net.cpp:96] Setting up label
I0626 03:06:55.876827  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:06:55.958657  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:06:55.958767  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:55.958806  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:06:55.958824  8898 net.cpp:396] label_label_0_split <- label
I0626 03:06:55.958851  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:06:55.958881  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:06:55.958904  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:06:55.958922  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:55.958938  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:55.958973  8898 net.cpp:67] Creating Layer inner1
I0626 03:06:55.958992  8898 net.cpp:396] inner1 <- data
I0626 03:06:55.959017  8898 net.cpp:358] inner1 -> inner1
I0626 03:06:55.959041  8898 net.cpp:96] Setting up inner1
I0626 03:06:55.986531  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:55.986584  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:06:55.986594  8898 net.cpp:396] inner1relu <- inner1
I0626 03:06:55.986605  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:06:55.986616  8898 net.cpp:96] Setting up inner1relu
I0626 03:06:55.986625  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:55.986635  8898 net.cpp:67] Creating Layer inner2
I0626 03:06:55.986644  8898 net.cpp:396] inner2 <- inner1
I0626 03:06:55.986654  8898 net.cpp:358] inner2 -> inner2
I0626 03:06:55.986663  8898 net.cpp:96] Setting up inner2
I0626 03:06:55.989603  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:55.989620  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:06:55.989629  8898 net.cpp:396] inner2relu <- inner2
I0626 03:06:55.989637  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:06:55.989646  8898 net.cpp:96] Setting up inner2relu
I0626 03:06:55.989655  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:55.989663  8898 net.cpp:67] Creating Layer inner3
I0626 03:06:55.989671  8898 net.cpp:396] inner3 <- inner2
I0626 03:06:55.989681  8898 net.cpp:358] inner3 -> inner3
I0626 03:06:55.989691  8898 net.cpp:96] Setting up inner3
I0626 03:06:56.001744  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:56.001783  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:06:56.001794  8898 net.cpp:396] inner3relu <- inner3
I0626 03:06:56.001806  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:06:56.001816  8898 net.cpp:96] Setting up inner3relu
I0626 03:06:56.001823  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:56.001834  8898 net.cpp:67] Creating Layer output
I0626 03:06:56.001842  8898 net.cpp:396] output <- inner3
I0626 03:06:56.001852  8898 net.cpp:358] output -> output
I0626 03:06:56.001863  8898 net.cpp:96] Setting up output
I0626 03:06:56.003036  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:06:56.003053  8898 net.cpp:67] Creating Layer loss
I0626 03:06:56.003062  8898 net.cpp:396] loss <- output
I0626 03:06:56.003072  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:06:56.003082  8898 net.cpp:358] loss -> loss
I0626 03:06:56.003095  8898 net.cpp:358] loss -> std
I0626 03:06:56.003108  8898 net.cpp:358] loss -> ind
I0626 03:06:56.003118  8898 net.cpp:358] loss -> proba
I0626 03:06:56.003129  8898 net.cpp:96] Setting up loss
I0626 03:06:56.003136  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:06:56.003152  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:56.003160  8898 net.cpp:109]     with loss weight 1
I0626 03:06:56.003176  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:56.003185  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:06:56.003191  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:56.003204  8898 net.cpp:67] Creating Layer silence
I0626 03:06:56.003219  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:06:56.003228  8898 net.cpp:396] silence <- ind
I0626 03:06:56.003237  8898 net.cpp:396] silence <- proba
I0626 03:06:56.003252  8898 net.cpp:96] Setting up silence
I0626 03:06:56.003260  8898 net.cpp:172] silence does not need backward computation.
I0626 03:06:56.003268  8898 net.cpp:170] loss needs backward computation.
I0626 03:06:56.003275  8898 net.cpp:170] output needs backward computation.
I0626 03:06:56.003283  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:06:56.003289  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:06:56.003298  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:06:56.003304  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:06:56.003311  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:06:56.003319  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:06:56.003334  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:06:56.003341  8898 net.cpp:172] label does not need backward computation.
I0626 03:06:56.003350  8898 net.cpp:172] data does not need backward computation.
I0626 03:06:56.003355  8898 net.cpp:208] This network produces output loss
I0626 03:06:56.003363  8898 net.cpp:208] This network produces output std
I0626 03:06:56.003377  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:06:56.003388  8898 net.cpp:219] Network initialization done.
I0626 03:06:56.003396  8898 net.cpp:220] Memory required for data: 9903112
I0626 03:06:57.472290 18476 caffe.cpp:99] Use GPU with device ID 0
I0626 03:06:57.715221 18476 caffe.cpp:107] Starting Optimization
I0626 03:06:57.715315 18476 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:06:57.715339 18476 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:06:57.715562 18476 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:06:57.715576 18476 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:06:57.715667 18476 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000669"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000669"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:06:57.715749 18476 net.cpp:67] Creating Layer data
I0626 03:06:57.715761 18476 net.cpp:358] data -> data
I0626 03:06:57.715778 18476 net.cpp:96] Setting up data
I0626 03:06:57.715813 18476 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:06:57.795395 18476 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:06:57.795660 18476 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:06:57.795704 18476 net.cpp:67] Creating Layer label
I0626 03:06:57.795727 18476 net.cpp:358] label -> label
I0626 03:06:57.795758 18476 net.cpp:96] Setting up label
I0626 03:06:57.795778 18476 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:06:57.929092 18476 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:06:57.929236 18476 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:57.929275 18476 net.cpp:67] Creating Layer label_label_0_split
I0626 03:06:57.929293 18476 net.cpp:396] label_label_0_split <- label
I0626 03:06:57.929335 18476 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:06:57.929365 18476 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:06:57.929388 18476 net.cpp:96] Setting up label_label_0_split
I0626 03:06:57.929417 18476 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:57.929436 18476 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:57.929460 18476 net.cpp:67] Creating Layer inner1
I0626 03:06:57.929477 18476 net.cpp:396] inner1 <- data
I0626 03:06:57.929499 18476 net.cpp:358] inner1 -> inner1
I0626 03:06:57.929524 18476 net.cpp:96] Setting up inner1
I0626 03:06:57.957934 18476 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:57.957984 18476 net.cpp:67] Creating Layer inner1relu
I0626 03:06:57.957995 18476 net.cpp:396] inner1relu <- inner1
I0626 03:06:57.958007 18476 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:06:57.958019 18476 net.cpp:96] Setting up inner1relu
I0626 03:06:57.958034 18476 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:57.958045 18476 net.cpp:67] Creating Layer inner2
I0626 03:06:57.958052 18476 net.cpp:396] inner2 <- inner1
I0626 03:06:57.958062 18476 net.cpp:358] inner2 -> inner2
I0626 03:06:57.958075 18476 net.cpp:96] Setting up inner2
I0626 03:06:57.961109 18476 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:57.961128 18476 net.cpp:67] Creating Layer inner2relu
I0626 03:06:57.961138 18476 net.cpp:396] inner2relu <- inner2
I0626 03:06:57.961146 18476 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:06:57.961156 18476 net.cpp:96] Setting up inner2relu
I0626 03:06:57.961165 18476 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:57.961175 18476 net.cpp:67] Creating Layer inner3
I0626 03:06:57.961184 18476 net.cpp:396] inner3 <- inner2
I0626 03:06:57.961194 18476 net.cpp:358] inner3 -> inner3
I0626 03:06:57.961203 18476 net.cpp:96] Setting up inner3
I0626 03:06:57.973127 18476 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:57.973168 18476 net.cpp:67] Creating Layer inner3relu
I0626 03:06:57.973186 18476 net.cpp:396] inner3relu <- inner3
I0626 03:06:57.973196 18476 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:06:57.973208 18476 net.cpp:96] Setting up inner3relu
I0626 03:06:57.973217 18476 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:57.973227 18476 net.cpp:67] Creating Layer output
I0626 03:06:57.973235 18476 net.cpp:396] output <- inner3
I0626 03:06:57.973244 18476 net.cpp:358] output -> output
I0626 03:06:57.973255 18476 net.cpp:96] Setting up output
I0626 03:06:57.974440 18476 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:06:57.974457 18476 net.cpp:67] Creating Layer loss
I0626 03:06:57.974465 18476 net.cpp:396] loss <- output
I0626 03:06:57.974475 18476 net.cpp:396] loss <- label_label_0_split_0
I0626 03:06:57.974485 18476 net.cpp:358] loss -> loss
I0626 03:06:57.974498 18476 net.cpp:358] loss -> std
I0626 03:06:57.974509 18476 net.cpp:358] loss -> ind
I0626 03:06:57.974520 18476 net.cpp:358] loss -> proba
I0626 03:06:57.974540 18476 net.cpp:96] Setting up loss
I0626 03:06:57.974555 18476 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:06:57.974568 18476 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:57.974577 18476 net.cpp:109]     with loss weight 1
I0626 03:06:57.974606 18476 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:57.974614 18476 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:06:57.974622 18476 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:57.974635 18476 net.cpp:67] Creating Layer silence
I0626 03:06:57.974643 18476 net.cpp:396] silence <- label_label_0_split_1
I0626 03:06:57.974653 18476 net.cpp:396] silence <- ind
I0626 03:06:57.974661 18476 net.cpp:396] silence <- proba
I0626 03:06:57.974670 18476 net.cpp:96] Setting up silence
I0626 03:06:57.974689 18476 net.cpp:172] silence does not need backward computation.
I0626 03:06:57.974699 18476 net.cpp:170] loss needs backward computation.
I0626 03:06:57.974706 18476 net.cpp:170] output needs backward computation.
I0626 03:06:57.974714 18476 net.cpp:170] inner3relu needs backward computation.
I0626 03:06:57.974722 18476 net.cpp:170] inner3 needs backward computation.
I0626 03:06:57.974730 18476 net.cpp:170] inner2relu needs backward computation.
I0626 03:06:57.974738 18476 net.cpp:170] inner2 needs backward computation.
I0626 03:06:57.974746 18476 net.cpp:170] inner1relu needs backward computation.
I0626 03:06:57.974755 18476 net.cpp:170] inner1 needs backward computation.
I0626 03:06:57.974762 18476 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:06:57.974771 18476 net.cpp:172] label does not need backward computation.
I0626 03:06:57.974778 18476 net.cpp:172] data does not need backward computation.
I0626 03:06:57.974786 18476 net.cpp:208] This network produces output loss
I0626 03:06:57.974794 18476 net.cpp:208] This network produces output std
I0626 03:06:57.974808 18476 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:06:57.974818 18476 net.cpp:219] Network initialization done.
I0626 03:06:57.974831 18476 net.cpp:220] Memory required for data: 9903112
I0626 03:06:57.974870 18476 solver.cpp:41] Solver scaffolding done.
I0626 03:06:57.974879 18476 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:06:57.985749 18476 solver.cpp:160] Solving net
I0626 03:06:58.007462 18476 solver.cpp:192] Iteration 0, loss = 0.191974
I0626 03:06:58.007525 18476 solver.cpp:207]     Train net output #0: loss = 0.191974 (* 1 = 0.191974 loss)
I0626 03:06:58.007539 18476 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:58.007555 18476 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:06:58.109680 18476 solver.cpp:192] Iteration 10, loss = 0.194334
I0626 03:06:58.109753 18476 solver.cpp:207]     Train net output #0: loss = 0.194334 (* 1 = 0.194334 loss)
I0626 03:06:58.109772 18476 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:58.109789 18476 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:06:58.212455 18476 solver.cpp:192] Iteration 20, loss = 0.17175
I0626 03:06:58.212515 18476 solver.cpp:207]     Train net output #0: loss = 0.17175 (* 1 = 0.17175 loss)
I0626 03:06:58.212527 18476 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:58.212538 18476 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:06:58.311811 18476 solver.cpp:192] Iteration 30, loss = 0.298704
I0626 03:06:58.311875 18476 solver.cpp:207]     Train net output #0: loss = 0.298704 (* 1 = 0.298704 loss)
I0626 03:06:58.311892 18476 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:58.311908 18476 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:06:58.422956 18476 solver.cpp:192] Iteration 40, loss = 0.258916
I0626 03:06:58.423048 18476 solver.cpp:207]     Train net output #0: loss = 0.258916 (* 1 = 0.258916 loss)
I0626 03:06:58.423072 18476 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:58.423095 18476 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:06:58.542264 18476 solver.cpp:192] Iteration 50, loss = 0.224074
I0626 03:06:58.542351 18476 solver.cpp:207]     Train net output #0: loss = 0.224074 (* 1 = 0.224074 loss)
I0626 03:06:58.542368 18476 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:58.542385 18476 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:06:58.646104 18476 solver.cpp:192] Iteration 60, loss = 0.143318
I0626 03:06:58.646164 18476 solver.cpp:207]     Train net output #0: loss = 0.143318 (* 1 = 0.143318 loss)
I0626 03:06:58.646189 18476 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:58.646201 18476 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:06:58.752301 18476 solver.cpp:192] Iteration 70, loss = 0.15561
I0626 03:06:58.752349 18476 solver.cpp:207]     Train net output #0: loss = 0.15561 (* 1 = 0.15561 loss)
I0626 03:06:58.752374 18476 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:58.752387 18476 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:06:58.865144 18476 solver.cpp:192] Iteration 80, loss = 0.14872
I0626 03:06:58.865191 18476 solver.cpp:207]     Train net output #0: loss = 0.14872 (* 1 = 0.14872 loss)
I0626 03:06:58.865203 18476 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:58.865216 18476 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:06:58.970580 18476 solver.cpp:192] Iteration 90, loss = 0.140581
I0626 03:06:58.970652 18476 solver.cpp:207]     Train net output #0: loss = 0.140581 (* 1 = 0.140581 loss)
I0626 03:06:58.970670 18476 solver.cpp:207]     Train net output #1: std = 0
I0626 03:06:58.970686 18476 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:06:59.086428 18476 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:06:59.128820 18476 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:06:59.156520 18476 solver.cpp:229] Iteration 100, loss = 0.121979
I0626 03:06:59.156569 18476 solver.cpp:234] Optimization Done.
I0626 03:06:59.156576 18476 caffe.cpp:121] Optimization Done.
I0626 03:06:59.222919  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:06:59.222951  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:06:59.223068  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:06:59.223148  8898 net.cpp:67] Creating Layer data
I0626 03:06:59.223163  8898 net.cpp:358] data -> data
I0626 03:06:59.223179  8898 net.cpp:96] Setting up data
I0626 03:06:59.223191  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:06:59.480010  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:06:59.481776  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:06:59.481828  8898 net.cpp:67] Creating Layer label
I0626 03:06:59.481856  8898 net.cpp:358] label -> label
I0626 03:06:59.481889  8898 net.cpp:96] Setting up label
I0626 03:06:59.481909  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:06:59.580389  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:06:59.580494  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:59.580533  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:06:59.580551  8898 net.cpp:396] label_label_0_split <- label
I0626 03:06:59.580575  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:06:59.580602  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:06:59.580623  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:06:59.580642  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:59.580658  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:59.580679  8898 net.cpp:67] Creating Layer inner1
I0626 03:06:59.580696  8898 net.cpp:396] inner1 <- data
I0626 03:06:59.580716  8898 net.cpp:358] inner1 -> inner1
I0626 03:06:59.580744  8898 net.cpp:96] Setting up inner1
I0626 03:06:59.609434  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:59.609483  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:06:59.609493  8898 net.cpp:396] inner1relu <- inner1
I0626 03:06:59.609504  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:06:59.609522  8898 net.cpp:96] Setting up inner1relu
I0626 03:06:59.609530  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:59.609541  8898 net.cpp:67] Creating Layer inner2
I0626 03:06:59.609549  8898 net.cpp:396] inner2 <- inner1
I0626 03:06:59.609557  8898 net.cpp:358] inner2 -> inner2
I0626 03:06:59.609568  8898 net.cpp:96] Setting up inner2
I0626 03:06:59.612478  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:59.612494  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:06:59.612502  8898 net.cpp:396] inner2relu <- inner2
I0626 03:06:59.612511  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:06:59.612521  8898 net.cpp:96] Setting up inner2relu
I0626 03:06:59.612529  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:06:59.612538  8898 net.cpp:67] Creating Layer inner3
I0626 03:06:59.612546  8898 net.cpp:396] inner3 <- inner2
I0626 03:06:59.612556  8898 net.cpp:358] inner3 -> inner3
I0626 03:06:59.612567  8898 net.cpp:96] Setting up inner3
I0626 03:06:59.624114  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:59.624152  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:06:59.624162  8898 net.cpp:396] inner3relu <- inner3
I0626 03:06:59.624174  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:06:59.624184  8898 net.cpp:96] Setting up inner3relu
I0626 03:06:59.624192  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:06:59.624214  8898 net.cpp:67] Creating Layer output
I0626 03:06:59.624222  8898 net.cpp:396] output <- inner3
I0626 03:06:59.624233  8898 net.cpp:358] output -> output
I0626 03:06:59.624243  8898 net.cpp:96] Setting up output
I0626 03:06:59.625402  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:06:59.625419  8898 net.cpp:67] Creating Layer loss
I0626 03:06:59.625427  8898 net.cpp:396] loss <- output
I0626 03:06:59.625437  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:06:59.625447  8898 net.cpp:358] loss -> loss
I0626 03:06:59.625459  8898 net.cpp:358] loss -> std
I0626 03:06:59.625469  8898 net.cpp:358] loss -> ind
I0626 03:06:59.625480  8898 net.cpp:358] loss -> proba
I0626 03:06:59.625490  8898 net.cpp:96] Setting up loss
I0626 03:06:59.625497  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:06:59.625514  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:59.625520  8898 net.cpp:109]     with loss weight 1
I0626 03:06:59.625536  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:06:59.625543  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:06:59.625551  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:06:59.625563  8898 net.cpp:67] Creating Layer silence
I0626 03:06:59.625571  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:06:59.625581  8898 net.cpp:396] silence <- ind
I0626 03:06:59.625588  8898 net.cpp:396] silence <- proba
I0626 03:06:59.625597  8898 net.cpp:96] Setting up silence
I0626 03:06:59.625604  8898 net.cpp:172] silence does not need backward computation.
I0626 03:06:59.625612  8898 net.cpp:170] loss needs backward computation.
I0626 03:06:59.625619  8898 net.cpp:170] output needs backward computation.
I0626 03:06:59.625627  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:06:59.625635  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:06:59.625643  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:06:59.625650  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:06:59.625658  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:06:59.625664  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:06:59.625672  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:06:59.625679  8898 net.cpp:172] label does not need backward computation.
I0626 03:06:59.625687  8898 net.cpp:172] data does not need backward computation.
I0626 03:06:59.625694  8898 net.cpp:208] This network produces output loss
I0626 03:06:59.625701  8898 net.cpp:208] This network produces output std
I0626 03:06:59.625716  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:06:59.625726  8898 net.cpp:219] Network initialization done.
I0626 03:06:59.625733  8898 net.cpp:220] Memory required for data: 9903112
I0626 03:07:01.102699 18707 caffe.cpp:99] Use GPU with device ID 0
I0626 03:07:01.346681 18707 caffe.cpp:107] Starting Optimization
I0626 03:07:01.346772 18707 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:07:01.346796 18707 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:07:01.346997 18707 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:01.347012 18707 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:01.347111 18707 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000892"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000892"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:07:01.347199 18707 net.cpp:67] Creating Layer data
I0626 03:07:01.347213 18707 net.cpp:358] data -> data
I0626 03:07:01.347230 18707 net.cpp:96] Setting up data
I0626 03:07:01.347265 18707 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:01.440804 18707 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:01.440997 18707 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:01.441037 18707 net.cpp:67] Creating Layer label
I0626 03:07:01.441061 18707 net.cpp:358] label -> label
I0626 03:07:01.441092 18707 net.cpp:96] Setting up label
I0626 03:07:01.441113 18707 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:01.566238 18707 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:01.566380 18707 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:01.566416 18707 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:01.566437 18707 net.cpp:396] label_label_0_split <- label
I0626 03:07:01.566475 18707 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:01.566506 18707 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:01.566529 18707 net.cpp:96] Setting up label_label_0_split
I0626 03:07:01.566557 18707 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:01.566576 18707 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:01.566601 18707 net.cpp:67] Creating Layer inner1
I0626 03:07:01.566618 18707 net.cpp:396] inner1 <- data
I0626 03:07:01.566640 18707 net.cpp:358] inner1 -> inner1
I0626 03:07:01.566665 18707 net.cpp:96] Setting up inner1
I0626 03:07:01.594846 18707 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:01.594894 18707 net.cpp:67] Creating Layer inner1relu
I0626 03:07:01.594905 18707 net.cpp:396] inner1relu <- inner1
I0626 03:07:01.594918 18707 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:01.594936 18707 net.cpp:96] Setting up inner1relu
I0626 03:07:01.594950 18707 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:01.594962 18707 net.cpp:67] Creating Layer inner2
I0626 03:07:01.594970 18707 net.cpp:396] inner2 <- inner1
I0626 03:07:01.594981 18707 net.cpp:358] inner2 -> inner2
I0626 03:07:01.594992 18707 net.cpp:96] Setting up inner2
I0626 03:07:01.597995 18707 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:01.598012 18707 net.cpp:67] Creating Layer inner2relu
I0626 03:07:01.598021 18707 net.cpp:396] inner2relu <- inner2
I0626 03:07:01.598031 18707 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:01.598040 18707 net.cpp:96] Setting up inner2relu
I0626 03:07:01.598048 18707 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:01.598058 18707 net.cpp:67] Creating Layer inner3
I0626 03:07:01.598067 18707 net.cpp:396] inner3 <- inner2
I0626 03:07:01.598076 18707 net.cpp:358] inner3 -> inner3
I0626 03:07:01.598086 18707 net.cpp:96] Setting up inner3
I0626 03:07:01.610045 18707 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:01.610075 18707 net.cpp:67] Creating Layer inner3relu
I0626 03:07:01.610085 18707 net.cpp:396] inner3relu <- inner3
I0626 03:07:01.610095 18707 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:01.610106 18707 net.cpp:96] Setting up inner3relu
I0626 03:07:01.610114 18707 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:01.610124 18707 net.cpp:67] Creating Layer output
I0626 03:07:01.610132 18707 net.cpp:396] output <- inner3
I0626 03:07:01.610142 18707 net.cpp:358] output -> output
I0626 03:07:01.610153 18707 net.cpp:96] Setting up output
I0626 03:07:01.611340 18707 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:01.611357 18707 net.cpp:67] Creating Layer loss
I0626 03:07:01.611366 18707 net.cpp:396] loss <- output
I0626 03:07:01.611376 18707 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:01.611387 18707 net.cpp:358] loss -> loss
I0626 03:07:01.611399 18707 net.cpp:358] loss -> std
I0626 03:07:01.611410 18707 net.cpp:358] loss -> ind
I0626 03:07:01.611421 18707 net.cpp:358] loss -> proba
I0626 03:07:01.611431 18707 net.cpp:96] Setting up loss
I0626 03:07:01.611443 18707 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:01.611457 18707 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:01.611472 18707 net.cpp:109]     with loss weight 1
I0626 03:07:01.611506 18707 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:01.611515 18707 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:01.611522 18707 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:01.611536 18707 net.cpp:67] Creating Layer silence
I0626 03:07:01.611544 18707 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:01.611553 18707 net.cpp:396] silence <- ind
I0626 03:07:01.611562 18707 net.cpp:396] silence <- proba
I0626 03:07:01.611572 18707 net.cpp:96] Setting up silence
I0626 03:07:01.611593 18707 net.cpp:172] silence does not need backward computation.
I0626 03:07:01.611601 18707 net.cpp:170] loss needs backward computation.
I0626 03:07:01.611610 18707 net.cpp:170] output needs backward computation.
I0626 03:07:01.611618 18707 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:01.611626 18707 net.cpp:170] inner3 needs backward computation.
I0626 03:07:01.611634 18707 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:01.611641 18707 net.cpp:170] inner2 needs backward computation.
I0626 03:07:01.611649 18707 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:01.611657 18707 net.cpp:170] inner1 needs backward computation.
I0626 03:07:01.611665 18707 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:01.611673 18707 net.cpp:172] label does not need backward computation.
I0626 03:07:01.611681 18707 net.cpp:172] data does not need backward computation.
I0626 03:07:01.611690 18707 net.cpp:208] This network produces output loss
I0626 03:07:01.611697 18707 net.cpp:208] This network produces output std
I0626 03:07:01.611711 18707 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:01.611727 18707 net.cpp:219] Network initialization done.
I0626 03:07:01.611740 18707 net.cpp:220] Memory required for data: 9903112
I0626 03:07:01.611780 18707 solver.cpp:41] Solver scaffolding done.
I0626 03:07:01.611790 18707 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:07:01.622539 18707 solver.cpp:160] Solving net
I0626 03:07:01.644338 18707 solver.cpp:192] Iteration 0, loss = 0.174441
I0626 03:07:01.644389 18707 solver.cpp:207]     Train net output #0: loss = 0.174441 (* 1 = 0.174441 loss)
I0626 03:07:01.644402 18707 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:01.644424 18707 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:07:01.724951 18707 solver.cpp:192] Iteration 10, loss = 0.154007
I0626 03:07:01.725010 18707 solver.cpp:207]     Train net output #0: loss = 0.154007 (* 1 = 0.154007 loss)
I0626 03:07:01.725023 18707 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:01.725034 18707 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:07:01.821507 18707 solver.cpp:192] Iteration 20, loss = 0.209746
I0626 03:07:01.821568 18707 solver.cpp:207]     Train net output #0: loss = 0.209746 (* 1 = 0.209746 loss)
I0626 03:07:01.821581 18707 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:01.821593 18707 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:07:01.934876 18707 solver.cpp:192] Iteration 30, loss = 0.18896
I0626 03:07:01.934942 18707 solver.cpp:207]     Train net output #0: loss = 0.18896 (* 1 = 0.18896 loss)
I0626 03:07:01.934957 18707 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:01.934969 18707 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:07:02.036736 18707 solver.cpp:192] Iteration 40, loss = 0.180599
I0626 03:07:02.036806 18707 solver.cpp:207]     Train net output #0: loss = 0.180599 (* 1 = 0.180599 loss)
I0626 03:07:02.036823 18707 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:02.036839 18707 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:07:02.143790 18707 solver.cpp:192] Iteration 50, loss = 0.157445
I0626 03:07:02.143851 18707 solver.cpp:207]     Train net output #0: loss = 0.157445 (* 1 = 0.157445 loss)
I0626 03:07:02.143863 18707 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:02.143875 18707 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:07:02.252023 18707 solver.cpp:192] Iteration 60, loss = 0.183091
I0626 03:07:02.252084 18707 solver.cpp:207]     Train net output #0: loss = 0.183091 (* 1 = 0.183091 loss)
I0626 03:07:02.252095 18707 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:02.252106 18707 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:07:02.361006 18707 solver.cpp:192] Iteration 70, loss = 0.155362
I0626 03:07:02.361070 18707 solver.cpp:207]     Train net output #0: loss = 0.155362 (* 1 = 0.155362 loss)
I0626 03:07:02.361104 18707 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:02.361119 18707 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:07:02.474184 18707 solver.cpp:192] Iteration 80, loss = 0.171158
I0626 03:07:02.474256 18707 solver.cpp:207]     Train net output #0: loss = 0.171158 (* 1 = 0.171158 loss)
I0626 03:07:02.474273 18707 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:02.474289 18707 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:07:02.586197 18707 solver.cpp:192] Iteration 90, loss = 0.182037
I0626 03:07:02.586258 18707 solver.cpp:207]     Train net output #0: loss = 0.182037 (* 1 = 0.182037 loss)
I0626 03:07:02.586275 18707 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:02.586292 18707 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:07:02.698662 18707 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:07:02.739917 18707 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:07:02.768112 18707 solver.cpp:229] Iteration 100, loss = 0.131032
I0626 03:07:02.768177 18707 solver.cpp:234] Optimization Done.
I0626 03:07:02.768185 18707 caffe.cpp:121] Optimization Done.
I0626 03:07:02.835880  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:02.835911  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:02.836026  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:07:02.836102  8898 net.cpp:67] Creating Layer data
I0626 03:07:02.836114  8898 net.cpp:358] data -> data
I0626 03:07:02.836128  8898 net.cpp:96] Setting up data
I0626 03:07:02.836138  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:03.050446  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:03.052217  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:03.052270  8898 net.cpp:67] Creating Layer label
I0626 03:07:03.052294  8898 net.cpp:358] label -> label
I0626 03:07:03.052322  8898 net.cpp:96] Setting up label
I0626 03:07:03.052341  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:03.134021  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:03.134125  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:03.134160  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:03.134177  8898 net.cpp:396] label_label_0_split <- label
I0626 03:07:03.134202  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:03.134234  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:03.134269  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:07:03.134291  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:03.134308  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:03.134330  8898 net.cpp:67] Creating Layer inner1
I0626 03:07:03.134346  8898 net.cpp:396] inner1 <- data
I0626 03:07:03.134367  8898 net.cpp:358] inner1 -> inner1
I0626 03:07:03.134392  8898 net.cpp:96] Setting up inner1
I0626 03:07:03.160141  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:03.160192  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:07:03.160202  8898 net.cpp:396] inner1relu <- inner1
I0626 03:07:03.160213  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:03.160225  8898 net.cpp:96] Setting up inner1relu
I0626 03:07:03.160233  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:03.160244  8898 net.cpp:67] Creating Layer inner2
I0626 03:07:03.160251  8898 net.cpp:396] inner2 <- inner1
I0626 03:07:03.160261  8898 net.cpp:358] inner2 -> inner2
I0626 03:07:03.160271  8898 net.cpp:96] Setting up inner2
I0626 03:07:03.163203  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:03.163219  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:07:03.163228  8898 net.cpp:396] inner2relu <- inner2
I0626 03:07:03.163238  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:03.163245  8898 net.cpp:96] Setting up inner2relu
I0626 03:07:03.163254  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:03.163264  8898 net.cpp:67] Creating Layer inner3
I0626 03:07:03.163270  8898 net.cpp:396] inner3 <- inner2
I0626 03:07:03.163280  8898 net.cpp:358] inner3 -> inner3
I0626 03:07:03.163290  8898 net.cpp:96] Setting up inner3
I0626 03:07:03.175223  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:03.175266  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:07:03.175276  8898 net.cpp:396] inner3relu <- inner3
I0626 03:07:03.175287  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:03.175297  8898 net.cpp:96] Setting up inner3relu
I0626 03:07:03.175307  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:03.175317  8898 net.cpp:67] Creating Layer output
I0626 03:07:03.175324  8898 net.cpp:396] output <- inner3
I0626 03:07:03.175334  8898 net.cpp:358] output -> output
I0626 03:07:03.175348  8898 net.cpp:96] Setting up output
I0626 03:07:03.176549  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:03.176569  8898 net.cpp:67] Creating Layer loss
I0626 03:07:03.176578  8898 net.cpp:396] loss <- output
I0626 03:07:03.176586  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:03.176597  8898 net.cpp:358] loss -> loss
I0626 03:07:03.176612  8898 net.cpp:358] loss -> std
I0626 03:07:03.176623  8898 net.cpp:358] loss -> ind
I0626 03:07:03.176635  8898 net.cpp:358] loss -> proba
I0626 03:07:03.176645  8898 net.cpp:96] Setting up loss
I0626 03:07:03.176652  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:03.176667  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:03.176676  8898 net.cpp:109]     with loss weight 1
I0626 03:07:03.176692  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:03.176698  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:03.176707  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:03.176719  8898 net.cpp:67] Creating Layer silence
I0626 03:07:03.176728  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:03.176736  8898 net.cpp:396] silence <- ind
I0626 03:07:03.176745  8898 net.cpp:396] silence <- proba
I0626 03:07:03.176753  8898 net.cpp:96] Setting up silence
I0626 03:07:03.176761  8898 net.cpp:172] silence does not need backward computation.
I0626 03:07:03.176769  8898 net.cpp:170] loss needs backward computation.
I0626 03:07:03.176776  8898 net.cpp:170] output needs backward computation.
I0626 03:07:03.176784  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:03.176791  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:07:03.176800  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:03.176817  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:07:03.176826  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:03.176833  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:07:03.176841  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:03.176849  8898 net.cpp:172] label does not need backward computation.
I0626 03:07:03.176856  8898 net.cpp:172] data does not need backward computation.
I0626 03:07:03.176863  8898 net.cpp:208] This network produces output loss
I0626 03:07:03.176872  8898 net.cpp:208] This network produces output std
I0626 03:07:03.176887  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:03.176898  8898 net.cpp:219] Network initialization done.
I0626 03:07:03.176904  8898 net.cpp:220] Memory required for data: 9903112
I0626 03:07:04.709121 18938 caffe.cpp:99] Use GPU with device ID 0
I0626 03:07:04.951606 18938 caffe.cpp:107] Starting Optimization
I0626 03:07:04.951692 18938 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:07:04.951717 18938 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:07:04.951910 18938 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:04.951925 18938 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:04.952018 18938 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00001115"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00001115"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:07:04.952101 18938 net.cpp:67] Creating Layer data
I0626 03:07:04.952116 18938 net.cpp:358] data -> data
I0626 03:07:04.952132 18938 net.cpp:96] Setting up data
I0626 03:07:04.952167 18938 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:05.036927 18938 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:05.037161 18938 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:05.037201 18938 net.cpp:67] Creating Layer label
I0626 03:07:05.037223 18938 net.cpp:358] label -> label
I0626 03:07:05.037256 18938 net.cpp:96] Setting up label
I0626 03:07:05.037276 18938 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:05.145645 18938 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:05.145778 18938 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:05.145813 18938 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:05.145833 18938 net.cpp:396] label_label_0_split <- label
I0626 03:07:05.145895 18938 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:05.145923 18938 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:05.145946 18938 net.cpp:96] Setting up label_label_0_split
I0626 03:07:05.145973 18938 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:05.145992 18938 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:05.146014 18938 net.cpp:67] Creating Layer inner1
I0626 03:07:05.146031 18938 net.cpp:396] inner1 <- data
I0626 03:07:05.146054 18938 net.cpp:358] inner1 -> inner1
I0626 03:07:05.146078 18938 net.cpp:96] Setting up inner1
I0626 03:07:05.174427 18938 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:05.174485 18938 net.cpp:67] Creating Layer inner1relu
I0626 03:07:05.174496 18938 net.cpp:396] inner1relu <- inner1
I0626 03:07:05.174509 18938 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:05.174520 18938 net.cpp:96] Setting up inner1relu
I0626 03:07:05.174535 18938 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:05.174547 18938 net.cpp:67] Creating Layer inner2
I0626 03:07:05.174556 18938 net.cpp:396] inner2 <- inner1
I0626 03:07:05.174566 18938 net.cpp:358] inner2 -> inner2
I0626 03:07:05.174576 18938 net.cpp:96] Setting up inner2
I0626 03:07:05.177604 18938 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:05.177621 18938 net.cpp:67] Creating Layer inner2relu
I0626 03:07:05.177630 18938 net.cpp:396] inner2relu <- inner2
I0626 03:07:05.177639 18938 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:05.177649 18938 net.cpp:96] Setting up inner2relu
I0626 03:07:05.177656 18938 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:05.177666 18938 net.cpp:67] Creating Layer inner3
I0626 03:07:05.177673 18938 net.cpp:396] inner3 <- inner2
I0626 03:07:05.177683 18938 net.cpp:358] inner3 -> inner3
I0626 03:07:05.177693 18938 net.cpp:96] Setting up inner3
I0626 03:07:05.189662 18938 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:05.189697 18938 net.cpp:67] Creating Layer inner3relu
I0626 03:07:05.189707 18938 net.cpp:396] inner3relu <- inner3
I0626 03:07:05.189718 18938 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:05.189728 18938 net.cpp:96] Setting up inner3relu
I0626 03:07:05.189738 18938 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:05.189748 18938 net.cpp:67] Creating Layer output
I0626 03:07:05.189756 18938 net.cpp:396] output <- inner3
I0626 03:07:05.189766 18938 net.cpp:358] output -> output
I0626 03:07:05.189776 18938 net.cpp:96] Setting up output
I0626 03:07:05.191001 18938 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:05.191018 18938 net.cpp:67] Creating Layer loss
I0626 03:07:05.191027 18938 net.cpp:396] loss <- output
I0626 03:07:05.191037 18938 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:05.191056 18938 net.cpp:358] loss -> loss
I0626 03:07:05.191069 18938 net.cpp:358] loss -> std
I0626 03:07:05.191081 18938 net.cpp:358] loss -> ind
I0626 03:07:05.191092 18938 net.cpp:358] loss -> proba
I0626 03:07:05.191102 18938 net.cpp:96] Setting up loss
I0626 03:07:05.191114 18938 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:05.191128 18938 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:05.191136 18938 net.cpp:109]     with loss weight 1
I0626 03:07:05.191165 18938 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:05.191174 18938 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:05.191182 18938 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:05.191195 18938 net.cpp:67] Creating Layer silence
I0626 03:07:05.191205 18938 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:05.191215 18938 net.cpp:396] silence <- ind
I0626 03:07:05.191223 18938 net.cpp:396] silence <- proba
I0626 03:07:05.191231 18938 net.cpp:96] Setting up silence
I0626 03:07:05.191252 18938 net.cpp:172] silence does not need backward computation.
I0626 03:07:05.191260 18938 net.cpp:170] loss needs backward computation.
I0626 03:07:05.191268 18938 net.cpp:170] output needs backward computation.
I0626 03:07:05.191277 18938 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:05.191284 18938 net.cpp:170] inner3 needs backward computation.
I0626 03:07:05.191293 18938 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:05.191300 18938 net.cpp:170] inner2 needs backward computation.
I0626 03:07:05.191308 18938 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:05.191316 18938 net.cpp:170] inner1 needs backward computation.
I0626 03:07:05.191323 18938 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:05.191332 18938 net.cpp:172] label does not need backward computation.
I0626 03:07:05.191339 18938 net.cpp:172] data does not need backward computation.
I0626 03:07:05.191347 18938 net.cpp:208] This network produces output loss
I0626 03:07:05.191355 18938 net.cpp:208] This network produces output std
I0626 03:07:05.191370 18938 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:05.191380 18938 net.cpp:219] Network initialization done.
I0626 03:07:05.191393 18938 net.cpp:220] Memory required for data: 9903112
I0626 03:07:05.191435 18938 solver.cpp:41] Solver scaffolding done.
I0626 03:07:05.191444 18938 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:07:05.202435 18938 solver.cpp:160] Solving net
I0626 03:07:05.224704 18938 solver.cpp:192] Iteration 0, loss = 0.16583
I0626 03:07:05.224756 18938 solver.cpp:207]     Train net output #0: loss = 0.16583 (* 1 = 0.16583 loss)
I0626 03:07:05.224768 18938 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:05.224784 18938 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:07:05.324167 18938 solver.cpp:192] Iteration 10, loss = 0.156089
I0626 03:07:05.324229 18938 solver.cpp:207]     Train net output #0: loss = 0.156089 (* 1 = 0.156089 loss)
I0626 03:07:05.324246 18938 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:05.324262 18938 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:07:05.436199 18938 solver.cpp:192] Iteration 20, loss = 0.172772
I0626 03:07:05.436257 18938 solver.cpp:207]     Train net output #0: loss = 0.172772 (* 1 = 0.172772 loss)
I0626 03:07:05.436273 18938 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:05.436290 18938 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:07:05.539629 18938 solver.cpp:192] Iteration 30, loss = 0.148524
I0626 03:07:05.539708 18938 solver.cpp:207]     Train net output #0: loss = 0.148524 (* 1 = 0.148524 loss)
I0626 03:07:05.539727 18938 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:05.539746 18938 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:07:05.654992 18938 solver.cpp:192] Iteration 40, loss = 0.14213
I0626 03:07:05.655051 18938 solver.cpp:207]     Train net output #0: loss = 0.14213 (* 1 = 0.14213 loss)
I0626 03:07:05.655063 18938 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:05.655088 18938 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:07:05.761528 18938 solver.cpp:192] Iteration 50, loss = 0.117747
I0626 03:07:05.761576 18938 solver.cpp:207]     Train net output #0: loss = 0.117747 (* 1 = 0.117747 loss)
I0626 03:07:05.761588 18938 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:05.761600 18938 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:07:05.865428 18938 solver.cpp:192] Iteration 60, loss = 0.114092
I0626 03:07:05.865489 18938 solver.cpp:207]     Train net output #0: loss = 0.114092 (* 1 = 0.114092 loss)
I0626 03:07:05.865504 18938 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:05.865520 18938 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:07:05.973274 18938 solver.cpp:192] Iteration 70, loss = 0.120732
I0626 03:07:05.973345 18938 solver.cpp:207]     Train net output #0: loss = 0.120732 (* 1 = 0.120732 loss)
I0626 03:07:05.973383 18938 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:05.973399 18938 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:07:06.081957 18938 solver.cpp:192] Iteration 80, loss = 0.119294
I0626 03:07:06.082036 18938 solver.cpp:207]     Train net output #0: loss = 0.119294 (* 1 = 0.119294 loss)
I0626 03:07:06.082056 18938 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:06.082074 18938 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:07:06.190533 18938 solver.cpp:192] Iteration 90, loss = 0.130364
I0626 03:07:06.190600 18938 solver.cpp:207]     Train net output #0: loss = 0.130364 (* 1 = 0.130364 loss)
I0626 03:07:06.190614 18938 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:06.190629 18938 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:07:06.309686 18938 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:07:06.350457 18938 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:07:06.378288 18938 solver.cpp:229] Iteration 100, loss = 0.123298
I0626 03:07:06.378334 18938 solver.cpp:234] Optimization Done.
I0626 03:07:06.378342 18938 caffe.cpp:121] Optimization Done.
I0626 03:07:06.450119  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:06.450151  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:06.450268  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:07:06.450343  8898 net.cpp:67] Creating Layer data
I0626 03:07:06.450356  8898 net.cpp:358] data -> data
I0626 03:07:06.450372  8898 net.cpp:96] Setting up data
I0626 03:07:06.450383  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:06.680089  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:06.681854  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:06.681915  8898 net.cpp:67] Creating Layer label
I0626 03:07:06.681938  8898 net.cpp:358] label -> label
I0626 03:07:06.681970  8898 net.cpp:96] Setting up label
I0626 03:07:06.681989  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:06.780377  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:06.780481  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:06.780514  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:06.780531  8898 net.cpp:396] label_label_0_split <- label
I0626 03:07:06.780553  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:06.780580  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:06.780612  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:07:06.780630  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:06.780645  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:06.780666  8898 net.cpp:67] Creating Layer inner1
I0626 03:07:06.780683  8898 net.cpp:396] inner1 <- data
I0626 03:07:06.780702  8898 net.cpp:358] inner1 -> inner1
I0626 03:07:06.780728  8898 net.cpp:96] Setting up inner1
I0626 03:07:06.805420  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:06.805467  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:07:06.805479  8898 net.cpp:396] inner1relu <- inner1
I0626 03:07:06.805490  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:06.805500  8898 net.cpp:96] Setting up inner1relu
I0626 03:07:06.805510  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:06.805521  8898 net.cpp:67] Creating Layer inner2
I0626 03:07:06.805528  8898 net.cpp:396] inner2 <- inner1
I0626 03:07:06.805539  8898 net.cpp:358] inner2 -> inner2
I0626 03:07:06.805550  8898 net.cpp:96] Setting up inner2
I0626 03:07:06.808534  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:06.808552  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:07:06.808560  8898 net.cpp:396] inner2relu <- inner2
I0626 03:07:06.808569  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:06.808579  8898 net.cpp:96] Setting up inner2relu
I0626 03:07:06.808588  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:06.808598  8898 net.cpp:67] Creating Layer inner3
I0626 03:07:06.808604  8898 net.cpp:396] inner3 <- inner2
I0626 03:07:06.808614  8898 net.cpp:358] inner3 -> inner3
I0626 03:07:06.808625  8898 net.cpp:96] Setting up inner3
I0626 03:07:06.820515  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:06.820555  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:07:06.820564  8898 net.cpp:396] inner3relu <- inner3
I0626 03:07:06.820590  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:06.820601  8898 net.cpp:96] Setting up inner3relu
I0626 03:07:06.820610  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:06.820621  8898 net.cpp:67] Creating Layer output
I0626 03:07:06.820628  8898 net.cpp:396] output <- inner3
I0626 03:07:06.820637  8898 net.cpp:358] output -> output
I0626 03:07:06.820648  8898 net.cpp:96] Setting up output
I0626 03:07:06.821841  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:06.821859  8898 net.cpp:67] Creating Layer loss
I0626 03:07:06.821867  8898 net.cpp:396] loss <- output
I0626 03:07:06.821877  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:06.821887  8898 net.cpp:358] loss -> loss
I0626 03:07:06.821899  8898 net.cpp:358] loss -> std
I0626 03:07:06.821910  8898 net.cpp:358] loss -> ind
I0626 03:07:06.821920  8898 net.cpp:358] loss -> proba
I0626 03:07:06.821930  8898 net.cpp:96] Setting up loss
I0626 03:07:06.821938  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:06.821951  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:06.821959  8898 net.cpp:109]     with loss weight 1
I0626 03:07:06.821975  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:06.821983  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:06.821990  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:06.822002  8898 net.cpp:67] Creating Layer silence
I0626 03:07:06.822010  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:06.822019  8898 net.cpp:396] silence <- ind
I0626 03:07:06.822027  8898 net.cpp:396] silence <- proba
I0626 03:07:06.822036  8898 net.cpp:96] Setting up silence
I0626 03:07:06.822043  8898 net.cpp:172] silence does not need backward computation.
I0626 03:07:06.822051  8898 net.cpp:170] loss needs backward computation.
I0626 03:07:06.822058  8898 net.cpp:170] output needs backward computation.
I0626 03:07:06.822065  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:06.822072  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:07:06.822080  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:06.822088  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:07:06.822094  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:06.822103  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:07:06.822109  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:06.822118  8898 net.cpp:172] label does not need backward computation.
I0626 03:07:06.822124  8898 net.cpp:172] data does not need backward computation.
I0626 03:07:06.822131  8898 net.cpp:208] This network produces output loss
I0626 03:07:06.822139  8898 net.cpp:208] This network produces output std
I0626 03:07:06.822152  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:06.822162  8898 net.cpp:219] Network initialization done.
I0626 03:07:06.822170  8898 net.cpp:220] Memory required for data: 9903112
I0626 03:07:08.284974 19169 caffe.cpp:99] Use GPU with device ID 0
I0626 03:07:08.528267 19169 caffe.cpp:107] Starting Optimization
I0626 03:07:08.528360 19169 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:07:08.528384 19169 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:07:08.528574 19169 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:08.528589 19169 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:08.528679 19169 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00001338"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00001338"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:07:08.528761 19169 net.cpp:67] Creating Layer data
I0626 03:07:08.528774 19169 net.cpp:358] data -> data
I0626 03:07:08.528790 19169 net.cpp:96] Setting up data
I0626 03:07:08.528822 19169 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:08.624135 19169 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:08.624389 19169 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:08.624429 19169 net.cpp:67] Creating Layer label
I0626 03:07:08.624452 19169 net.cpp:358] label -> label
I0626 03:07:08.624485 19169 net.cpp:96] Setting up label
I0626 03:07:08.624505 19169 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:08.732825 19169 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:08.732947 19169 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:08.732982 19169 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:08.733001 19169 net.cpp:396] label_label_0_split <- label
I0626 03:07:08.733037 19169 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:08.733067 19169 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:08.733088 19169 net.cpp:96] Setting up label_label_0_split
I0626 03:07:08.733117 19169 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:08.733135 19169 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:08.733157 19169 net.cpp:67] Creating Layer inner1
I0626 03:07:08.733175 19169 net.cpp:396] inner1 <- data
I0626 03:07:08.733196 19169 net.cpp:358] inner1 -> inner1
I0626 03:07:08.733220 19169 net.cpp:96] Setting up inner1
I0626 03:07:08.760113 19169 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:08.760184 19169 net.cpp:67] Creating Layer inner1relu
I0626 03:07:08.760195 19169 net.cpp:396] inner1relu <- inner1
I0626 03:07:08.760206 19169 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:08.760218 19169 net.cpp:96] Setting up inner1relu
I0626 03:07:08.760231 19169 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:08.760243 19169 net.cpp:67] Creating Layer inner2
I0626 03:07:08.760251 19169 net.cpp:396] inner2 <- inner1
I0626 03:07:08.760260 19169 net.cpp:358] inner2 -> inner2
I0626 03:07:08.760272 19169 net.cpp:96] Setting up inner2
I0626 03:07:08.763291 19169 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:08.763309 19169 net.cpp:67] Creating Layer inner2relu
I0626 03:07:08.763317 19169 net.cpp:396] inner2relu <- inner2
I0626 03:07:08.763327 19169 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:08.763336 19169 net.cpp:96] Setting up inner2relu
I0626 03:07:08.763345 19169 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:08.763355 19169 net.cpp:67] Creating Layer inner3
I0626 03:07:08.763361 19169 net.cpp:396] inner3 <- inner2
I0626 03:07:08.763371 19169 net.cpp:358] inner3 -> inner3
I0626 03:07:08.763381 19169 net.cpp:96] Setting up inner3
I0626 03:07:08.775287 19169 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:08.775319 19169 net.cpp:67] Creating Layer inner3relu
I0626 03:07:08.775328 19169 net.cpp:396] inner3relu <- inner3
I0626 03:07:08.775338 19169 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:08.775349 19169 net.cpp:96] Setting up inner3relu
I0626 03:07:08.775358 19169 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:08.775368 19169 net.cpp:67] Creating Layer output
I0626 03:07:08.775377 19169 net.cpp:396] output <- inner3
I0626 03:07:08.775387 19169 net.cpp:358] output -> output
I0626 03:07:08.775396 19169 net.cpp:96] Setting up output
I0626 03:07:08.776628 19169 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:08.776645 19169 net.cpp:67] Creating Layer loss
I0626 03:07:08.776654 19169 net.cpp:396] loss <- output
I0626 03:07:08.776664 19169 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:08.776674 19169 net.cpp:358] loss -> loss
I0626 03:07:08.776687 19169 net.cpp:358] loss -> std
I0626 03:07:08.776698 19169 net.cpp:358] loss -> ind
I0626 03:07:08.776710 19169 net.cpp:358] loss -> proba
I0626 03:07:08.776721 19169 net.cpp:96] Setting up loss
I0626 03:07:08.776732 19169 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:08.776746 19169 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:08.776754 19169 net.cpp:109]     with loss weight 1
I0626 03:07:08.776789 19169 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:08.776798 19169 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:08.776806 19169 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:08.776819 19169 net.cpp:67] Creating Layer silence
I0626 03:07:08.776829 19169 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:08.776837 19169 net.cpp:396] silence <- ind
I0626 03:07:08.776846 19169 net.cpp:396] silence <- proba
I0626 03:07:08.776855 19169 net.cpp:96] Setting up silence
I0626 03:07:08.776877 19169 net.cpp:172] silence does not need backward computation.
I0626 03:07:08.776885 19169 net.cpp:170] loss needs backward computation.
I0626 03:07:08.776895 19169 net.cpp:170] output needs backward computation.
I0626 03:07:08.776902 19169 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:08.776911 19169 net.cpp:170] inner3 needs backward computation.
I0626 03:07:08.776918 19169 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:08.776926 19169 net.cpp:170] inner2 needs backward computation.
I0626 03:07:08.776933 19169 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:08.776942 19169 net.cpp:170] inner1 needs backward computation.
I0626 03:07:08.776949 19169 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:08.776957 19169 net.cpp:172] label does not need backward computation.
I0626 03:07:08.776965 19169 net.cpp:172] data does not need backward computation.
I0626 03:07:08.776973 19169 net.cpp:208] This network produces output loss
I0626 03:07:08.776986 19169 net.cpp:208] This network produces output std
I0626 03:07:08.777001 19169 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:08.777011 19169 net.cpp:219] Network initialization done.
I0626 03:07:08.777024 19169 net.cpp:220] Memory required for data: 9903112
I0626 03:07:08.777065 19169 solver.cpp:41] Solver scaffolding done.
I0626 03:07:08.777074 19169 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:07:08.787959 19169 solver.cpp:160] Solving net
I0626 03:07:08.809626 19169 solver.cpp:192] Iteration 0, loss = 0.17106
I0626 03:07:08.809675 19169 solver.cpp:207]     Train net output #0: loss = 0.17106 (* 1 = 0.17106 loss)
I0626 03:07:08.809687 19169 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:08.809710 19169 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:07:08.903851 19169 solver.cpp:192] Iteration 10, loss = 0.156846
I0626 03:07:08.903908 19169 solver.cpp:207]     Train net output #0: loss = 0.156846 (* 1 = 0.156846 loss)
I0626 03:07:08.903920 19169 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:08.903931 19169 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:07:09.012298 19169 solver.cpp:192] Iteration 20, loss = 0.157091
I0626 03:07:09.012359 19169 solver.cpp:207]     Train net output #0: loss = 0.157091 (* 1 = 0.157091 loss)
I0626 03:07:09.012372 19169 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:09.012383 19169 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:07:09.121498 19169 solver.cpp:192] Iteration 30, loss = 0.152582
I0626 03:07:09.121567 19169 solver.cpp:207]     Train net output #0: loss = 0.152582 (* 1 = 0.152582 loss)
I0626 03:07:09.121583 19169 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:09.121598 19169 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:07:09.234621 19169 solver.cpp:192] Iteration 40, loss = 0.13026
I0626 03:07:09.234691 19169 solver.cpp:207]     Train net output #0: loss = 0.13026 (* 1 = 0.13026 loss)
I0626 03:07:09.234709 19169 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:09.234726 19169 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:07:09.346576 19169 solver.cpp:192] Iteration 50, loss = 0.147605
I0626 03:07:09.346643 19169 solver.cpp:207]     Train net output #0: loss = 0.147605 (* 1 = 0.147605 loss)
I0626 03:07:09.346662 19169 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:09.346678 19169 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:07:09.454499 19169 solver.cpp:192] Iteration 60, loss = 0.150421
I0626 03:07:09.454571 19169 solver.cpp:207]     Train net output #0: loss = 0.150421 (* 1 = 0.150421 loss)
I0626 03:07:09.454587 19169 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:09.454603 19169 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:07:09.558812 19169 solver.cpp:192] Iteration 70, loss = 0.131825
I0626 03:07:09.558873 19169 solver.cpp:207]     Train net output #0: loss = 0.131825 (* 1 = 0.131825 loss)
I0626 03:07:09.558905 19169 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:09.558917 19169 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:07:09.672269 19169 solver.cpp:192] Iteration 80, loss = 0.139781
I0626 03:07:09.672340 19169 solver.cpp:207]     Train net output #0: loss = 0.139781 (* 1 = 0.139781 loss)
I0626 03:07:09.672356 19169 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:09.672372 19169 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:07:09.777941 19169 solver.cpp:192] Iteration 90, loss = 0.130725
I0626 03:07:09.778017 19169 solver.cpp:207]     Train net output #0: loss = 0.130725 (* 1 = 0.130725 loss)
I0626 03:07:09.778038 19169 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:09.778056 19169 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:07:09.891374 19169 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:07:09.933277 19169 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:07:09.960950 19169 solver.cpp:229] Iteration 100, loss = 0.148445
I0626 03:07:09.960994 19169 solver.cpp:234] Optimization Done.
I0626 03:07:09.961002 19169 caffe.cpp:121] Optimization Done.
I0626 03:07:10.030745  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:10.030776  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:10.030889  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:07:10.030972  8898 net.cpp:67] Creating Layer data
I0626 03:07:10.030984  8898 net.cpp:358] data -> data
I0626 03:07:10.030998  8898 net.cpp:96] Setting up data
I0626 03:07:10.031008  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:10.250444  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:10.252226  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:10.252280  8898 net.cpp:67] Creating Layer label
I0626 03:07:10.252310  8898 net.cpp:358] label -> label
I0626 03:07:10.252341  8898 net.cpp:96] Setting up label
I0626 03:07:10.252360  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:10.334087  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:10.334194  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:10.334228  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:10.334247  8898 net.cpp:396] label_label_0_split <- label
I0626 03:07:10.334290  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:10.334316  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:10.334337  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:07:10.334355  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:10.334370  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:10.334393  8898 net.cpp:67] Creating Layer inner1
I0626 03:07:10.334409  8898 net.cpp:396] inner1 <- data
I0626 03:07:10.334429  8898 net.cpp:358] inner1 -> inner1
I0626 03:07:10.334456  8898 net.cpp:96] Setting up inner1
I0626 03:07:10.360105  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:10.360147  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:07:10.360157  8898 net.cpp:396] inner1relu <- inner1
I0626 03:07:10.360169  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:10.360180  8898 net.cpp:96] Setting up inner1relu
I0626 03:07:10.360188  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:10.360199  8898 net.cpp:67] Creating Layer inner2
I0626 03:07:10.360208  8898 net.cpp:396] inner2 <- inner1
I0626 03:07:10.360219  8898 net.cpp:358] inner2 -> inner2
I0626 03:07:10.360229  8898 net.cpp:96] Setting up inner2
I0626 03:07:10.363216  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:10.363234  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:07:10.363242  8898 net.cpp:396] inner2relu <- inner2
I0626 03:07:10.363251  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:10.363260  8898 net.cpp:96] Setting up inner2relu
I0626 03:07:10.363267  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:10.363277  8898 net.cpp:67] Creating Layer inner3
I0626 03:07:10.363284  8898 net.cpp:396] inner3 <- inner2
I0626 03:07:10.363294  8898 net.cpp:358] inner3 -> inner3
I0626 03:07:10.363304  8898 net.cpp:96] Setting up inner3
I0626 03:07:10.375193  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:10.375236  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:07:10.375246  8898 net.cpp:396] inner3relu <- inner3
I0626 03:07:10.375257  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:10.375267  8898 net.cpp:96] Setting up inner3relu
I0626 03:07:10.375275  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:10.375286  8898 net.cpp:67] Creating Layer output
I0626 03:07:10.375294  8898 net.cpp:396] output <- inner3
I0626 03:07:10.375305  8898 net.cpp:358] output -> output
I0626 03:07:10.375315  8898 net.cpp:96] Setting up output
I0626 03:07:10.376514  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:10.376533  8898 net.cpp:67] Creating Layer loss
I0626 03:07:10.376540  8898 net.cpp:396] loss <- output
I0626 03:07:10.376549  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:10.376559  8898 net.cpp:358] loss -> loss
I0626 03:07:10.376572  8898 net.cpp:358] loss -> std
I0626 03:07:10.376583  8898 net.cpp:358] loss -> ind
I0626 03:07:10.376595  8898 net.cpp:358] loss -> proba
I0626 03:07:10.376605  8898 net.cpp:96] Setting up loss
I0626 03:07:10.376612  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:10.376628  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:10.376636  8898 net.cpp:109]     with loss weight 1
I0626 03:07:10.376652  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:10.376659  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:10.376667  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:10.376682  8898 net.cpp:67] Creating Layer silence
I0626 03:07:10.376690  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:10.376699  8898 net.cpp:396] silence <- ind
I0626 03:07:10.376708  8898 net.cpp:396] silence <- proba
I0626 03:07:10.376715  8898 net.cpp:96] Setting up silence
I0626 03:07:10.376724  8898 net.cpp:172] silence does not need backward computation.
I0626 03:07:10.376731  8898 net.cpp:170] loss needs backward computation.
I0626 03:07:10.376739  8898 net.cpp:170] output needs backward computation.
I0626 03:07:10.376746  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:10.376765  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:07:10.376772  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:10.376780  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:07:10.376788  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:10.376796  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:07:10.376803  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:10.376811  8898 net.cpp:172] label does not need backward computation.
I0626 03:07:10.376818  8898 net.cpp:172] data does not need backward computation.
I0626 03:07:10.376827  8898 net.cpp:208] This network produces output loss
I0626 03:07:10.376833  8898 net.cpp:208] This network produces output std
I0626 03:07:10.376847  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:10.376857  8898 net.cpp:219] Network initialization done.
I0626 03:07:10.376864  8898 net.cpp:220] Memory required for data: 9903112
06  0.09701786  0.11847434  0.6048524 ]
 [ 0.04059937  0.17064893  0.61977465  0.07298573  0.09599132]
 [ 0.08572287  0.06359427  0.09194437  0.11774675  0.64099173]
 [ 0.03957871  0.15610888  0.63965125  0.07558648  0.08907469]
 [ 0.03283573  0.13963025  0.69106176  0.06429973  0.07217254]
 [ 0.12704852  0.06570748  0.09358531  0.13682081  0.57683788]]
[2 4 4 ..., 4 3 4]
0.0814911140009
[[ 0.03934525  0.23904973  0.56351996  0.0554877   0.10259736]
 [ 0.09257448  0.04259286  0.05096404  0.0641125   0.74975612]
 [ 0.05188953  0.03262853  0.04823437  0.04472737  0.8225202 ]
 [ 0.04486337  0.18528291  0.62644563  0.05527006  0.08813804]
 [ 0.09500966  0.04277673  0.05835144  0.06243198  0.74143018]
 [ 0.03244863  0.10607873  0.75949069  0.04020475  0.0617772 ]
 [ 0.10206165  0.02937509  0.05153536  0.06936669  0.74766121]
 [ 0.03140686  0.09484397  0.77714661  0.03964927  0.05695329]
 [ 0.02111496  0.07208489  0.83634353  0.03023627  0.04022035]
 [ 0.16329686  0.03172656  0.05012436  0.07553435  0.67931787]]
[2 4 4 ..., 4 3 4]
0.0671868227135
[[ 0.04881774  0.18739528  0.6327529   0.04257187  0.08846222]
 [ 0.02506983  0.04135787  0.02977461  0.02045761  0.88334008]
 [ 0.02129819  0.03793105  0.03392205  0.01934379  0.88750492]
 [ 0.04857976  0.13571426  0.70958983  0.03722966  0.06888649]
 [ 0.02903833  0.04240547  0.03754144  0.02293934  0.86807542]
 [ 0.03174174  0.06542188  0.8367295   0.02416755  0.04193932]
 [ 0.01676987  0.01063801  0.01625397  0.01511059  0.94122757]
 [ 0.02733665  0.05863662  0.85584114  0.02152861  0.03665698]
 [ 0.01723058  0.04809554  0.89194397  0.01666556  0.02606435]
 [ 0.0193254   0.0118515   0.01467499  0.01308413  0.94106398]]
[2 4 4 ..., 4 3 4]
0.0329432162982
[[ 0.03560465  0.08283618  0.78517552  0.03197136  0.06441228]
 [ 0.02523861  0.05352188  0.03979847  0.01883246  0.86260858]
 [ 0.01986303  0.0415381   0.03535404  0.01637892  0.88686592]
 [ 0.03477827  0.04897403  0.84598048  0.02500982  0.04525741]
 [ 0.02793036  0.05631736  0.04818337  0.02314928  0.84441963]
 [ 0.01824827  0.02005365  0.92563968  0.01372643  0.02233197]
 [ 0.00989189  0.00690532  0.01254396  0.01064326  0.96001557]
 [ 0.01848964  0.02173981  0.92478595  0.01286195  0.02212265]
 [ 0.0083989   0.01448883  0.95686473  0.00758958  0.01265796]
 [ 0.00934658  0.00706419  0.0099048   0.00720655  0.96647789]]
[2 4 4 ..., 4 3 4]
0.0576506285219
[[ 0.01670628  0.03089855  0.90828019  0.01442409  0.02969089]
 [ 0.02428138  0.03223502  0.03093373  0.01474009  0.89780979]
 [ 0.01759177  0.01973304  0.02290354  0.01153251  0.92823915]
 [ 0.00898099  0.01335235  0.95733664  0.00682593  0.01350409]
 [ 0.02508715  0.03149872  0.03539563  0.01724125  0.89077725]
 [ 0.00467619  0.00516133  0.98044328  0.00353005  0.00618915]
 [ 0.01769035  0.00673007  0.01536928  0.01560983  0.94460047]
 [ 0.00493911  0.00575817  0.97941098  0.0035176   0.00637414]
 [ 0.00456288  0.0055574   0.98050862  0.00346588  0.00590522]
 [ 0.01157794  0.00532526  0.00940155  0.00732668  0.96636858]]
[2 4 4 ..., 4 3 4]
0.0199393151279
[[ 0.01643588  0.02622015  0.91626376  0.01354282  0.02753739]
 [ 0.01861556  0.01581604  0.01763356  0.00778669  0.94014814]
 [ 0.01383451  0.0112285   0.0138002   0.00617943  0.95495736]
 [ 0.00777981  0.00915375  0.96613358  0.00561582  0.01131705]
 [ 0.0229329   0.02199007  0.02519387  0.01067689  0.91920627]
 [ 0.00385975  0.00343885  0.98501722  0.00273397  0.0049502 ]
 [ 0.00914005  0.00305076  0.00643892  0.00533105  0.97603921]
 [ 0.00429726  0.00396808  0.98338039  0.00292605  0.00542822]
 [ 0.0032486   0.00342892  0.9867351   0.00235104  0.00423634]
 [ 0.00776827  0.00308087  0.00517631  0.00346371  0.98051084]]
[2 4 4 ..., 4 3 4]
0.0147377546597
[[ 0.00923986  0.01147069  0.95838596  0.00734288  0.01356061]
 [ 0.00856925  0.00511745  0.00732774  0.00374386  0.97524171]
 [ 0.00723387  0.00405074  0.00652677  0.00332893  0.97885969]
 [ 0.00386417  0.00352388  0.98506944  0.00264889  0.00489362]
 [ 0.01084044  0.00715365  0.0108742   0.00500947  0.96612224]
 [ 0.00283182  0.00203504  0.99001254  0.00191919  0.0032014I0626 03:07:12.072630 19400 caffe.cpp:99] Use GPU with device ID 0
I0626 03:07:12.315068 19400 caffe.cpp:107] Starting Optimization
I0626 03:07:12.315158 19400 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:07:12.315182 19400 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:07:12.315374 19400 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:12.315388 19400 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:12.315495 19400 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00001561"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00001561"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:07:12.315577 19400 net.cpp:67] Creating Layer data
I0626 03:07:12.315590 19400 net.cpp:358] data -> data
I0626 03:07:12.315608 19400 net.cpp:96] Setting up data
I0626 03:07:12.315641 19400 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:12.395540 19400 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:12.395776 19400 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:12.395815 19400 net.cpp:67] Creating Layer label
I0626 03:07:12.395838 19400 net.cpp:358] label -> label
I0626 03:07:12.395870 19400 net.cpp:96] Setting up label
I0626 03:07:12.395890 19400 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:12.520856 19400 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:12.520978 19400 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:12.521008 19400 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:12.521025 19400 net.cpp:396] label_label_0_split <- label
I0626 03:07:12.521060 19400 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:12.521085 19400 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:12.521104 19400 net.cpp:96] Setting up label_label_0_split
I0626 03:07:12.521128 19400 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:12.521145 19400 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:12.521164 19400 net.cpp:67] Creating Layer inner1
I0626 03:07:12.521179 19400 net.cpp:396] inner1 <- data
I0626 03:07:12.521198 19400 net.cpp:358] inner1 -> inner1
I0626 03:07:12.521219 19400 net.cpp:96] Setting up inner1
I0626 03:07:12.547083 19400 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:12.547142 19400 net.cpp:67] Creating Layer inner1relu
I0626 03:07:12.547152 19400 net.cpp:396] inner1relu <- inner1
I0626 03:07:12.547163 19400 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:12.547176 19400 net.cpp:96] Setting up inner1relu
I0626 03:07:12.547192 19400 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:12.547204 19400 net.cpp:67] Creating Layer inner2
I0626 03:07:12.547212 19400 net.cpp:396] inner2 <- inner1
I0626 03:07:12.547224 19400 net.cpp:358] inner2 -> inner2
I0626 03:07:12.547235 19400 net.cpp:96] Setting up inner2
I0626 03:07:12.550254 19400 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:12.550273 19400 net.cpp:67] Creating Layer inner2relu
I0626 03:07:12.550282 19400 net.cpp:396] inner2relu <- inner2
I0626 03:07:12.550292 19400 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:12.550302 19400 net.cpp:96] Setting up inner2relu
I0626 03:07:12.550309 19400 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:12.550319 19400 net.cpp:67] Creating Layer inner3
I0626 03:07:12.550328 19400 net.cpp:396] inner3 <- inner2
I0626 03:07:12.550338 19400 net.cpp:358] inner3 -> inner3
I0626 03:07:12.550348 19400 net.cpp:96] Setting up inner3
I0626 03:07:12.562100 19400 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:12.562137 19400 net.cpp:67] Creating Layer inner3relu
I0626 03:07:12.562147 19400 net.cpp:396] inner3relu <- inner3
I0626 03:07:12.562157 19400 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:12.562168 19400 net.cpp:96] Setting up inner3relu
I0626 03:07:12.562177 19400 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:12.562187 19400 net.cpp:67] Creating Layer output
I0626 03:07:12.562196 19400 net.cpp:396] output <- inner3
I0626 03:07:12.562206 19400 net.cpp:358] output -> output
I0626 03:07:12.562216 19400 net.cpp:96] Setting up output
I0626 03:07:12.563405 19400 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:12.563432 19400 net.cpp:67] Creating Layer loss
I0626 03:07:12.563441 19400 net.cpp:396] loss <- output
I0626 03:07:12.563452 19400 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:12.563462 19400 net.cpp:358] loss -> loss
I0626 03:07:12.563482 19400 net.cpp:358] loss -> std
I0626 03:07:12.563493 19400 net.cpp:358] loss -> ind
I0626 03:07:12.563504 19400 net.cpp:358] loss -> proba
I0626 03:07:12.563514 19400 net.cpp:96] Setting up loss
I0626 03:07:12.563527 19400 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:12.563541 19400 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:12.563550 19400 net.cpp:109]     with loss weight 1
I0626 03:07:12.563585 19400 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:12.563594 19400 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:12.563602 19400 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:12.563616 19400 net.cpp:67] Creating Layer silence
I0626 03:07:12.563625 19400 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:12.563634 19400 net.cpp:396] silence <- ind
I0626 03:07:12.563643 19400 net.cpp:396] silence <- proba
I0626 03:07:12.563652 19400 net.cpp:96] Setting up silence
I0626 03:07:12.563673 19400 net.cpp:172] silence does not need backward computation.
I0626 03:07:12.563681 19400 net.cpp:170] loss needs backward computation.
I0626 03:07:12.563690 19400 net.cpp:170] output needs backward computation.
I0626 03:07:12.563699 19400 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:12.563706 19400 net.cpp:170] inner3 needs backward computation.
I0626 03:07:12.563714 19400 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:12.563722 19400 net.cpp:170] inner2 needs backward computation.
I0626 03:07:12.563730 19400 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:12.563737 19400 net.cpp:170] inner1 needs backward computation.
I0626 03:07:12.563746 19400 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:12.563755 19400 net.cpp:172] label does not need backward computation.
I0626 03:07:12.563761 19400 net.cpp:172] data does not need backward computation.
I0626 03:07:12.563769 19400 net.cpp:208] This network produces output loss
I0626 03:07:12.563777 19400 net.cpp:208] This network produces output std
I0626 03:07:12.563792 19400 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:12.563802 19400 net.cpp:219] Network initialization done.
I0626 03:07:12.563815 19400 net.cpp:220] Memory required for data: 9903112
I0626 03:07:12.563856 19400 solver.cpp:41] Solver scaffolding done.
I0626 03:07:12.563865 19400 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:07:12.574869 19400 solver.cpp:160] Solving net
I0626 03:07:12.596545 19400 solver.cpp:192] Iteration 0, loss = 0.159583
I0626 03:07:12.596590 19400 solver.cpp:207]     Train net output #0: loss = 0.159583 (* 1 = 0.159583 loss)
I0626 03:07:12.596601 19400 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:12.596624 19400 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:07:12.695349 19400 solver.cpp:192] Iteration 10, loss = 0.14847
I0626 03:07:12.695421 19400 solver.cpp:207]     Train net output #0: loss = 0.14847 (* 1 = 0.14847 loss)
I0626 03:07:12.695439 19400 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:12.695456 19400 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:07:12.811414 19400 solver.cpp:192] Iteration 20, loss = 0.156592
I0626 03:07:12.811522 19400 solver.cpp:207]     Train net output #0: loss = 0.156592 (* 1 = 0.156592 loss)
I0626 03:07:12.811543 19400 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:12.811563 19400 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:07:12.921624 19400 solver.cpp:192] Iteration 30, loss = 0.13971
I0626 03:07:12.921681 19400 solver.cpp:207]     Train net output #0: loss = 0.13971 (* 1 = 0.13971 loss)
I0626 03:07:12.921700 19400 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:12.921715 19400 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:07:13.016805 19400 solver.cpp:192] Iteration 40, loss = 0.147058
I0626 03:07:13.016880 19400 solver.cpp:207]     Train net output #0: loss = 0.147058 (* 1 = 0.147058 loss)
I0626 03:07:13.016892 19400 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:13.016904 19400 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:07:13.104955 19400 solver.cpp:192] Iteration 50, loss = 0.164448
I0626 03:07:13.105005 19400 solver.cpp:207]     Train net output #0: loss = 0.164448 (* 1 = 0.164448 loss)
I0626 03:07:13.105017 19400 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:13.105029 19400 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:07:13.196640 19400 solver.cpp:192] Iteration 60, loss = 0.150637
I0626 03:07:13.196708 19400 solver.cpp:207]     Train net output #0: loss = 0.150637 (* 1 = 0.150637 loss)
I0626 03:07:13.196724 19400 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:13.196740 19400 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:07:13.301307 19400 solver.cpp:192] Iteration 70, loss = 0.145814
I0626 03:07:13.301364 19400 solver.cpp:207]     Train net output #0: loss = 0.145814 (* 1 = 0.145814 loss)
I0626 03:07:13.301393 19400 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:13.301405 19400 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:07:13.396519 19400 solver.cpp:192] Iteration 80, loss = 0.178637
I0626 03:07:13.396579 19400 solver.cpp:207]     Train net output #0: loss = 0.178637 (* 1 = 0.178637 loss)
I0626 03:07:13.396590 19400 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:13.396602 19400 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:07:13.498026 19400 solver.cpp:192] Iteration 90, loss = 0.153325
I0626 03:07:13.498087 19400 solver.cpp:207]     Train net output #0: loss = 0.153325 (* 1 = 0.153325 loss)
I0626 03:07:13.498106 19400 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:13.498122 19400 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:07:13.609529 19400 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:07:13.650915 19400 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:07:13.678604 19400 solver.cpp:229] Iteration 100, loss = 0.131464
I0626 03:07:13.678643 19400 solver.cpp:234] Optimization Done.
I0626 03:07:13.678653 19400 caffe.cpp:121] Optimization Done.
I0626 03:07:13.742218  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:13.742255  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:13.742372  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:07:13.742465  8898 net.cpp:67] Creating Layer data
I0626 03:07:13.742477  8898 net.cpp:358] data -> data
I0626 03:07:13.742492  8898 net.cpp:96] Setting up data
I0626 03:07:13.742501  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:13.988539  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:13.990300  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:13.990362  8898 net.cpp:67] Creating Layer label
I0626 03:07:13.990384  8898 net.cpp:358] label -> label
I0626 03:07:13.990411  8898 net.cpp:96] Setting up label
I0626 03:07:13.990429  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:14.063810  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:14.063916  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:14.063951  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:14.063969  8898 net.cpp:396] label_label_0_split <- label
I0626 03:07:14.063993  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:14.064023  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:14.064059  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:07:14.064079  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:14.064095  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:14.064116  8898 net.cpp:67] Creating Layer inner1
I0626 03:07:14.064132  8898 net.cpp:396] inner1 <- data
I0626 03:07:14.064152  8898 net.cpp:358] inner1 -> inner1
I0626 03:07:14.064175  8898 net.cpp:96] Setting up inner1
I0626 03:07:14.092953  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:14.093006  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:07:14.093015  8898 net.cpp:396] inner1relu <- inner1
I0626 03:07:14.093026  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:14.093037  8898 net.cpp:96] Setting up inner1relu
I0626 03:07:14.093045  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:14.093056  8898 net.cpp:67] Creating Layer inner2
I0626 03:07:14.093065  8898 net.cpp:396] inner2 <- inner1
I0626 03:07:14.093073  8898 net.cpp:358] inner2 -> inner2
I0626 03:07:14.093087  8898 net.cpp:96] Setting up inner2
I0626 03:07:14.096035  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:14.096051  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:07:14.096060  8898 net.cpp:396] inner2relu <- inner2
I0626 03:07:14.096068  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:14.096077  8898 net.cpp:96] Setting up inner2relu
I0626 03:07:14.096086  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:14.096094  8898 net.cpp:67] Creating Layer inner3
I0626 03:07:14.096101  8898 net.cpp:396] inner3 <- inner2
I0626 03:07:14.096112  8898 net.cpp:358] inner3 -> inner3
I0626 03:07:14.096120  8898 net.cpp:96] Setting up inner3
I0626 03:07:14.108253  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:14.108300  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:07:14.108310  8898 net.cpp:396] inner3relu <- inner3
I0626 03:07:14.108322  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:14.108333  8898 net.cpp:96] Setting up inner3relu
I0626 03:07:14.108341  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:14.108352  8898 net.cpp:67] Creating Layer output
I0626 03:07:14.108361  8898 net.cpp:396] output <- inner3
I0626 03:07:14.108371  8898 net.cpp:358] output -> output
I0626 03:07:14.108381  8898 net.cpp:96] Setting up output
I0626 03:07:14.109568  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:14.109586  8898 net.cpp:67] Creating Layer loss
I0626 03:07:14.109593  8898 net.cpp:396] loss <- output
I0626 03:07:14.109602  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:14.109612  8898 net.cpp:358] loss -> loss
I0626 03:07:14.109625  8898 net.cpp:358] loss -> std
I0626 03:07:14.109635  8898 net.cpp:358] loss -> ind
I0626 03:07:14.109647  8898 net.cpp:358] loss -> proba
I0626 03:07:14.109655  8898 net.cpp:96] Setting up loss
I0626 03:07:14.109663  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:14.109678  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:14.109685  8898 net.cpp:109]     with loss weight 1
I0626 03:07:14.109701  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:14.109709  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:14.109716  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:14.109730  8898 net.cpp:67] Creating Layer silence
I0626 03:07:14.109738  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:14.109747  8898 net.cpp:396] silence <- ind
I0626 03:07:14.109755  8898 net.cpp:396] silence <- proba
I0626 03:07:14.109763  8898 net.cpp:96] Setting up silence
I0626 03:07:14.109771  8898 net.cpp:172] silence does not need backward computation.
I0626 03:07:14.109778  8898 net.cpp:170] loss needs backward computation.
I0626 03:07:14.109786  8898 net.cpp:170] output needs backward computation.
I0626 03:07:14.109793  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:14.109800  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:07:14.109808  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:14.109815  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:07:14.109822  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:14.109829  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:07:14.109838  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:14.109846  8898 net.cpp:172] label does not need backward computation.
I0626 03:07:14.109853  8898 net.cpp:172] data does not need backward computation.
I0626 03:07:14.109860  8898 net.cpp:208] This network produces output loss
I0626 03:07:14.109868  8898 net.cpp:208] This network produces output std
I0626 03:07:14.109884  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:14.109894  8898 net.cpp:219] Network initialization done.
I0626 03:07:14.109900  8898 net.cpp:220] Memory required for data: 9903112
I0626 03:07:15.576800 19631 caffe.cpp:99] Use GPU with device ID 0
I0626 03:07:15.819931 19631 caffe.cpp:107] Starting Optimization
I0626 03:07:15.820024 19631 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:07:15.820049 19631 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:07:15.820241 19631 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:15.820255 19631 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:15.820358 19631 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00001784"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00001784"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:07:15.820431 19631 net.cpp:67] Creating Layer data
I0626 03:07:15.820444 19631 net.cpp:358] data -> data
I0626 03:07:15.820461 19631 net.cpp:96] Setting up data
I0626 03:07:15.820494 19631 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:15.915971 19631 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:15.916229 19631 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:15.916270 19631 net.cpp:67] Creating Layer label
I0626 03:07:15.916291 19631 net.cpp:358] label -> label
I0626 03:07:15.916322 19631 net.cpp:96] Setting up label
I0626 03:07:15.916342 19631 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:16.024704 19631 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:16.024837 19631 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:16.024871 19631 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:16.024890 19631 net.cpp:396] label_label_0_split <- label
I0626 03:07:16.024929 19631 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:16.024958 19631 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:16.024981 19631 net.cpp:96] Setting up label_label_0_split
I0626 03:07:16.025009 19631 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:16.025027 19631 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:16.025050 19631 net.cpp:67] Creating Layer inner1
I0626 03:07:16.025068 19631 net.cpp:396] inner1 <- data
I0626 03:07:16.025100 19631 net.cpp:358] inner1 -> inner1
I0626 03:07:16.025126 19631 net.cpp:96] Setting up inner1
I0626 03:07:16.053973 19631 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:16.054028 19631 net.cpp:67] Creating Layer inner1relu
I0626 03:07:16.054039 19631 net.cpp:396] inner1relu <- inner1
I0626 03:07:16.054049 19631 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:16.054061 19631 net.cpp:96] Setting up inner1relu
I0626 03:07:16.054075 19631 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:16.054086 19631 net.cpp:67] Creating Layer inner2
I0626 03:07:16.054095 19631 net.cpp:396] inner2 <- inner1
I0626 03:07:16.054105 19631 net.cpp:358] inner2 -> inner2
I0626 03:07:16.054116 19631 net.cpp:96] Setting up inner2
I0626 03:07:16.057204 19631 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:16.057224 19631 net.cpp:67] Creating Layer inner2relu
I0626 03:07:16.057231 19631 net.cpp:396] inner2relu <- inner2
I0626 03:07:16.057241 19631 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:16.057250 19631 net.cpp:96] Setting up inner2relu
I0626 03:07:16.057260 19631 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:16.057269 19631 net.cpp:67] Creating Layer inner3
I0626 03:07:16.057277 19631 net.cpp:396] inner3 <- inner2
I0626 03:07:16.057287 19631 net.cpp:358] inner3 -> inner3
I0626 03:07:16.057297 19631 net.cpp:96] Setting up inner3
I0626 03:07:16.069342 19631 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:16.069375 19631 net.cpp:67] Creating Layer inner3relu
I0626 03:07:16.069386 19631 net.cpp:396] inner3relu <- inner3
I0626 03:07:16.069396 19631 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:16.069406 19631 net.cpp:96] Setting up inner3relu
I0626 03:07:16.069413 19631 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:16.069423 19631 net.cpp:67] Creating Layer output
I0626 03:07:16.069432 19631 net.cpp:396] output <- inner3
I0626 03:07:16.069442 19631 net.cpp:358] output -> output
I0626 03:07:16.069452 19631 net.cpp:96] Setting up output
I0626 03:07:16.070654 19631 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:16.070672 19631 net.cpp:67] Creating Layer loss
I0626 03:07:16.070679 19631 net.cpp:396] loss <- output
I0626 03:07:16.070688 19631 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:16.070699 19631 net.cpp:358] loss -> loss
I0626 03:07:16.070711 19631 net.cpp:358] loss -> std
I0626 03:07:16.070722 19631 net.cpp:358] loss -> ind
I0626 03:07:16.070734 19631 net.cpp:358] loss -> proba
I0626 03:07:16.070744 19631 net.cpp:96] Setting up loss
I0626 03:07:16.070755 19631 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:16.070770 19631 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:16.070778 19631 net.cpp:109]     with loss weight 1
I0626 03:07:16.070812 19631 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:16.070821 19631 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:16.070828 19631 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:16.070842 19631 net.cpp:67] Creating Layer silence
I0626 03:07:16.070849 19631 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:16.070858 19631 net.cpp:396] silence <- ind
I0626 03:07:16.070868 19631 net.cpp:396] silence <- proba
I0626 03:07:16.070875 19631 net.cpp:96] Setting up silence
I0626 03:07:16.070897 19631 net.cpp:172] silence does not need backward computation.
I0626 03:07:16.070905 19631 net.cpp:170] loss needs backward computation.
I0626 03:07:16.070914 19631 net.cpp:170] output needs backward computation.
I0626 03:07:16.070921 19631 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:16.070928 19631 net.cpp:170] inner3 needs backward computation.
I0626 03:07:16.070936 19631 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:16.070945 19631 net.cpp:170] inner2 needs backward computation.
I0626 03:07:16.070951 19631 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:16.070960 19631 net.cpp:170] inner1 needs backward computation.
I0626 03:07:16.070966 19631 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:16.070979 19631 net.cpp:172] label does not need backward computation.
I0626 03:07:16.070987 19631 net.cpp:172] data does not need backward computation.
I0626 03:07:16.070994 19631 net.cpp:208] This network produces output loss
I0626 03:07:16.071002 19631 net.cpp:208] This network produces output std
I0626 03:07:16.071017 19631 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:16.071027 19631 net.cpp:219] Network initialization done.
I0626 03:07:16.071039 19631 net.cpp:220] Memory required for data: 9903112
I0626 03:07:16.071079 19631 solver.cpp:41] Solver scaffolding done.
I0626 03:07:16.071089 19631 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:07:16.081933 19631 solver.cpp:160] Solving net
I0626 03:07:16.103677 19631 solver.cpp:192] Iteration 0, loss = 0.136162
I0626 03:07:16.103727 19631 solver.cpp:207]     Train net output #0: loss = 0.136162 (* 1 = 0.136162 loss)
I0626 03:07:16.103740 19631 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:16.103763 19631 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:07:16.201951 19631 solver.cpp:192] Iteration 10, loss = 0.154141
I0626 03:07:16.202029 19631 solver.cpp:207]     Train net output #0: loss = 0.154141 (* 1 = 0.154141 loss)
I0626 03:07:16.202050 19631 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:16.202069 19631 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:07:16.321887 19631 solver.cpp:192] Iteration 20, loss = 0.131873
I0626 03:07:16.321964 19631 solver.cpp:207]     Train net output #0: loss = 0.131873 (* 1 = 0.131873 loss)
I0626 03:07:16.321983 19631 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:16.322001 19631 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:07:16.434248 19631 solver.cpp:192] Iteration 30, loss = 0.154937
I0626 03:07:16.434326 19631 solver.cpp:207]     Train net output #0: loss = 0.154937 (* 1 = 0.154937 loss)
I0626 03:07:16.434346 19631 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:16.434365 19631 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:07:16.549295 19631 solver.cpp:192] Iteration 40, loss = 0.317081
I0626 03:07:16.549366 19631 solver.cpp:207]     Train net output #0: loss = 0.317081 (* 1 = 0.317081 loss)
I0626 03:07:16.549384 19631 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:16.549401 19631 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:07:16.649158 19631 solver.cpp:192] Iteration 50, loss = 0.139162
I0626 03:07:16.649217 19631 solver.cpp:207]     Train net output #0: loss = 0.139162 (* 1 = 0.139162 loss)
I0626 03:07:16.649230 19631 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:16.649240 19631 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:07:16.762097 19631 solver.cpp:192] Iteration 60, loss = 0.14243
I0626 03:07:16.762156 19631 solver.cpp:207]     Train net output #0: loss = 0.14243 (* 1 = 0.14243 loss)
I0626 03:07:16.762173 19631 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:16.762189 19631 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:07:16.874701 19631 solver.cpp:192] Iteration 70, loss = 0.137783
I0626 03:07:16.874763 19631 solver.cpp:207]     Train net output #0: loss = 0.137783 (* 1 = 0.137783 loss)
I0626 03:07:16.874794 19631 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:16.874807 19631 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:07:16.977879 19631 solver.cpp:192] Iteration 80, loss = 0.162743
I0626 03:07:16.977926 19631 solver.cpp:207]     Train net output #0: loss = 0.162743 (* 1 = 0.162743 loss)
I0626 03:07:16.977938 19631 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:16.977951 19631 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:07:17.078888 19631 solver.cpp:192] Iteration 90, loss = 0.123056
I0626 03:07:17.078949 19631 solver.cpp:207]     Train net output #0: loss = 0.123056 (* 1 = 0.123056 loss)
I0626 03:07:17.078961 19631 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:17.078974 19631 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:07:17.196796 19631 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:07:17.238860 19631 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:07:17.266997 19631 solver.cpp:229] Iteration 100, loss = 0.132048
I0626 03:07:17.267042 19631 solver.cpp:234] Optimization Done.
I0626 03:07:17.267051 19631 caffe.cpp:121] Optimization Done.
I0626 03:07:17.337733  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:17.337764  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:17.337878  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:07:17.337961  8898 net.cpp:67] Creating Layer data
I0626 03:07:17.337975  8898 net.cpp:358] data -> data
I0626 03:07:17.337990  8898 net.cpp:96] Setting up data
I0626 03:07:17.337999  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:17.592402  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:17.594174  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:17.594228  8898 net.cpp:67] Creating Layer label
I0626 03:07:17.594251  8898 net.cpp:358] label -> label
I0626 03:07:17.594280  8898 net.cpp:96] Setting up label
I0626 03:07:17.594298  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:17.675972  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:17.676092  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:17.676127  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:17.676146  8898 net.cpp:396] label_label_0_split <- label
I0626 03:07:17.676169  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:17.676196  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:17.676218  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:07:17.676237  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:17.676254  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:17.676275  8898 net.cpp:67] Creating Layer inner1
I0626 03:07:17.676291  8898 net.cpp:396] inner1 <- data
I0626 03:07:17.676316  8898 net.cpp:358] inner1 -> inner1
I0626 03:07:17.676342  8898 net.cpp:96] Setting up inner1
I0626 03:07:17.703392  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:17.703438  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:07:17.703447  8898 net.cpp:396] inner1relu <- inner1
I0626 03:07:17.703459  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:17.703475  8898 net.cpp:96] Setting up inner1relu
I0626 03:07:17.703485  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:17.703495  8898 net.cpp:67] Creating Layer inner2
I0626 03:07:17.703502  8898 net.cpp:396] inner2 <- inner1
I0626 03:07:17.703512  8898 net.cpp:358] inner2 -> inner2
I0626 03:07:17.703522  8898 net.cpp:96] Setting up inner2
I0626 03:07:17.706493  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:17.706511  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:07:17.706519  8898 net.cpp:396] inner2relu <- inner2
I0626 03:07:17.706528  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:17.706537  8898 net.cpp:96] Setting up inner2relu
I0626 03:07:17.706544  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:17.706554  8898 net.cpp:67] Creating Layer inner3
I0626 03:07:17.706562  8898 net.cpp:396] inner3 <- inner2
I0626 03:07:17.706571  8898 net.cpp:358] inner3 -> inner3
I0626 03:07:17.706581  8898 net.cpp:96] Setting up inner3
I0626 03:07:17.718571  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:17.718611  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:07:17.718621  8898 net.cpp:396] inner3relu <- inner3
I0626 03:07:17.718631  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:17.718642  8898 net.cpp:96] Setting up inner3relu
I0626 03:07:17.718650  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:17.718660  8898 net.cpp:67] Creating Layer output
I0626 03:07:17.718668  8898 net.cpp:396] output <- inner3
I0626 03:07:17.718677  8898 net.cpp:358] output -> output
I0626 03:07:17.718688  8898 net.cpp:96] Setting up output
I0626 03:07:17.719882  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:17.719899  8898 net.cpp:67] Creating Layer loss
I0626 03:07:17.719907  8898 net.cpp:396] loss <- output
I0626 03:07:17.719916  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:17.719928  8898 net.cpp:358] loss -> loss
I0626 03:07:17.719941  8898 net.cpp:358] loss -> std
I0626 03:07:17.719952  8898 net.cpp:358] loss -> ind
I0626 03:07:17.719964  8898 net.cpp:358] loss -> proba
I0626 03:07:17.719974  8898 net.cpp:96] Setting up loss
I0626 03:07:17.719980  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:17.719996  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:17.720005  8898 net.cpp:109]     with loss weight 1
I0626 03:07:17.720021  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:17.720029  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:17.720036  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:17.720050  8898 net.cpp:67] Creating Layer silence
I0626 03:07:17.720058  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:17.720067  8898 net.cpp:396] silence <- ind
I0626 03:07:17.720075  8898 net.cpp:396] silence <- proba
I0626 03:07:17.720083  8898 net.cpp:96] Setting up silence
I0626 03:07:17.720091  8898 net.cpp:172] silence does not need backward computation.
I0626 03:07:17.720098  8898 net.cpp:170] loss needs backward computation.
I0626 03:07:17.720122  8898 net.cpp:170] output needs backward computation.
I0626 03:07:17.720139  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:17.720145  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:07:17.720154  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:17.720160  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:07:17.720168  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:17.720175  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:07:17.720183  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:17.720191  8898 net.cpp:172] label does not need backward computation.
I0626 03:07:17.720198  8898 net.cpp:172] data does not need backward computation.
I0626 03:07:17.720206  8898 net.cpp:208] This network produces output loss
I0626 03:07:17.720212  8898 net.cpp:208] This network produces output std
I0626 03:07:17.720228  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:17.720239  8898 net.cpp:219] Network initialization done.
I0626 03:07:17.720247  8898 net.cpp:220] Memory required for data: 9903112
I0626 03:07:19.156756 19862 caffe.cpp:99] Use GPU with device ID 0
I0626 03:07:19.404456 19862 caffe.cpp:107] Starting Optimization
I0626 03:07:19.404541 19862 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:07:19.404564 19862 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:07:19.404759 19862 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:19.404773 19862 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:19.404873 19862 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00002007"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00002007"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:07:19.404958 19862 net.cpp:67] Creating Layer data
I0626 03:07:19.404973 19862 net.cpp:358] data -> data
I0626 03:07:19.404989 19862 net.cpp:96] Setting up data
I0626 03:07:19.405022 19862 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:19.504142 19862 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:19.504323 19862 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:19.504362 19862 net.cpp:67] Creating Layer label
I0626 03:07:19.504384 19862 net.cpp:358] label -> label
I0626 03:07:19.504416 19862 net.cpp:96] Setting up label
I0626 03:07:19.504437 19862 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:19.629544 19862 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:19.629689 19862 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:19.629724 19862 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:19.629742 19862 net.cpp:396] label_label_0_split <- label
I0626 03:07:19.629781 19862 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:19.629817 19862 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:19.629840 19862 net.cpp:96] Setting up label_label_0_split
I0626 03:07:19.629869 19862 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:19.629887 19862 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:19.629909 19862 net.cpp:67] Creating Layer inner1
I0626 03:07:19.629927 19862 net.cpp:396] inner1 <- data
I0626 03:07:19.629948 19862 net.cpp:358] inner1 -> inner1
I0626 03:07:19.629973 19862 net.cpp:96] Setting up inner1
I0626 03:07:19.658349 19862 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:19.658401 19862 net.cpp:67] Creating Layer inner1relu
I0626 03:07:19.658411 19862 net.cpp:396] inner1relu <- inner1
I0626 03:07:19.658421 19862 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:19.658433 19862 net.cpp:96] Setting up inner1relu
I0626 03:07:19.658447 19862 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:19.658458 19862 net.cpp:67] Creating Layer inner2
I0626 03:07:19.658466 19862 net.cpp:396] inner2 <- inner1
I0626 03:07:19.658476 19862 net.cpp:358] inner2 -> inner2
I0626 03:07:19.658486 19862 net.cpp:96] Setting up inner2
I0626 03:07:19.661432 19862 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:19.661450 19862 net.cpp:67] Creating Layer inner2relu
I0626 03:07:19.661458 19862 net.cpp:396] inner2relu <- inner2
I0626 03:07:19.661468 19862 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:19.661478 19862 net.cpp:96] Setting up inner2relu
I0626 03:07:19.661486 19862 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:19.661495 19862 net.cpp:67] Creating Layer inner3
I0626 03:07:19.661504 19862 net.cpp:396] inner3 <- inner2
I0626 03:07:19.661514 19862 net.cpp:358] inner3 -> inner3
I0626 03:07:19.661530 19862 net.cpp:96] Setting up inner3
I0626 03:07:19.673259 19862 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:19.673292 19862 net.cpp:67] Creating Layer inner3relu
I0626 03:07:19.673301 19862 net.cpp:396] inner3relu <- inner3
I0626 03:07:19.673311 19862 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:19.673321 19862 net.cpp:96] Setting up inner3relu
I0626 03:07:19.673331 19862 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:19.673341 19862 net.cpp:67] Creating Layer output
I0626 03:07:19.673347 19862 net.cpp:396] output <- inner3
I0626 03:07:19.673357 19862 net.cpp:358] output -> output
I0626 03:07:19.673378 19862 net.cpp:96] Setting up output
I0626 03:07:19.674577 19862 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:19.674594 19862 net.cpp:67] Creating Layer loss
I0626 03:07:19.674603 19862 net.cpp:396] loss <- output
I0626 03:07:19.674612 19862 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:19.674623 19862 net.cpp:358] loss -> loss
I0626 03:07:19.674635 19862 net.cpp:358] loss -> std
I0626 03:07:19.674646 19862 net.cpp:358] loss -> ind
I0626 03:07:19.674656 19862 net.cpp:358] loss -> proba
I0626 03:07:19.674666 19862 net.cpp:96] Setting up loss
I0626 03:07:19.674679 19862 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:19.674693 19862 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:19.674701 19862 net.cpp:109]     with loss weight 1
I0626 03:07:19.674736 19862 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:19.674743 19862 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:19.674751 19862 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:19.674764 19862 net.cpp:67] Creating Layer silence
I0626 03:07:19.674773 19862 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:19.674782 19862 net.cpp:396] silence <- ind
I0626 03:07:19.674791 19862 net.cpp:396] silence <- proba
I0626 03:07:19.674799 19862 net.cpp:96] Setting up silence
I0626 03:07:19.674821 19862 net.cpp:172] silence does not need backward computation.
I0626 03:07:19.674829 19862 net.cpp:170] loss needs backward computation.
I0626 03:07:19.674839 19862 net.cpp:170] output needs backward computation.
I0626 03:07:19.674845 19862 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:19.674854 19862 net.cpp:170] inner3 needs backward computation.
I0626 03:07:19.674860 19862 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:19.674868 19862 net.cpp:170] inner2 needs backward computation.
I0626 03:07:19.674875 19862 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:19.674883 19862 net.cpp:170] inner1 needs backward computation.
I0626 03:07:19.674891 19862 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:19.674898 19862 net.cpp:172] label does not need backward computation.
I0626 03:07:19.674906 19862 net.cpp:172] data does not need backward computation.
I0626 03:07:19.674913 19862 net.cpp:208] This network produces output loss
I0626 03:07:19.674921 19862 net.cpp:208] This network produces output std
I0626 03:07:19.674935 19862 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:19.674945 19862 net.cpp:219] Network initialization done.
I0626 03:07:19.674958 19862 net.cpp:220] Memory required for data: 9903112
I0626 03:07:19.674998 19862 solver.cpp:41] Solver scaffolding done.
I0626 03:07:19.675006 19862 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:07:19.685837 19862 solver.cpp:160] Solving net
I0626 03:07:19.707572 19862 solver.cpp:192] Iteration 0, loss = 0.167221
I0626 03:07:19.707615 19862 solver.cpp:207]     Train net output #0: loss = 0.167221 (* 1 = 0.167221 loss)
I0626 03:07:19.707628 19862 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:19.707649 19862 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:07:19.811970 19862 solver.cpp:192] Iteration 10, loss = 0.125939
I0626 03:07:19.812029 19862 solver.cpp:207]     Train net output #0: loss = 0.125939 (* 1 = 0.125939 loss)
I0626 03:07:19.812045 19862 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:19.812062 19862 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:07:19.925349 19862 solver.cpp:192] Iteration 20, loss = 0.140698
I0626 03:07:19.925407 19862 solver.cpp:207]     Train net output #0: loss = 0.140698 (* 1 = 0.140698 loss)
I0626 03:07:19.925418 19862 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:19.925431 19862 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:07:20.034670 19862 solver.cpp:192] Iteration 30, loss = 0.130733
I0626 03:07:20.034744 19862 solver.cpp:207]     Train net output #0: loss = 0.130733 (* 1 = 0.130733 loss)
I0626 03:07:20.034783 19862 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:20.034802 19862 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:07:20.144062 19862 solver.cpp:192] Iteration 40, loss = 0.125731
I0626 03:07:20.144111 19862 solver.cpp:207]     Train net output #0: loss = 0.125731 (* 1 = 0.125731 loss)
I0626 03:07:20.144124 19862 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:20.144134 19862 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:07:20.260224 19862 solver.cpp:192] Iteration 50, loss = 0.126236
I0626 03:07:20.260274 19862 solver.cpp:207]     Train net output #0: loss = 0.126236 (* 1 = 0.126236 loss)
I0626 03:07:20.260287 19862 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:20.260298 19862 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:07:20.375551 19862 solver.cpp:192] Iteration 60, loss = 0.119176
I0626 03:07:20.375625 19862 solver.cpp:207]     Train net output #0: loss = 0.119176 (* 1 = 0.119176 loss)
I0626 03:07:20.375646 19862 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:20.375666 19862 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:07:20.490254 19862 solver.cpp:192] Iteration 70, loss = 0.127626
I0626 03:07:20.490336 19862 solver.cpp:207]     Train net output #0: loss = 0.127626 (* 1 = 0.127626 loss)
I0626 03:07:20.490384 19862 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:20.490406 19862 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:07:20.607527 19862 solver.cpp:192] Iteration 80, loss = 0.113837
I0626 03:07:20.607599 19862 solver.cpp:207]     Train net output #0: loss = 0.113837 (* 1 = 0.113837 loss)
I0626 03:07:20.607620 19862 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:20.607640 19862 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:07:20.723336 19862 solver.cpp:192] Iteration 90, loss = 0.112875
I0626 03:07:20.723405 19862 solver.cpp:207]     Train net output #0: loss = 0.112875 (* 1 = 0.112875 loss)
I0626 03:07:20.723428 19862 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:20.723441 19862 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:07:20.850661 19862 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:07:20.892503 19862 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:07:20.920189 19862 solver.cpp:229] Iteration 100, loss = 0.108516
I0626 03:07:20.920233 19862 solver.cpp:234] Optimization Done.
I0626 03:07:20.920243 19862 caffe.cpp:121] Optimization Done.
I0626 03:07:20.986384  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:20.986415  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:20.986536  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:07:20.986634  8898 net.cpp:67] Creating Layer data
I0626 03:07:20.986649  8898 net.cpp:358] data -> data
I0626 03:07:20.986663  8898 net.cpp:96] Setting up data
I0626 03:07:20.986673  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:21.213750  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:21.215528  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:21.215590  8898 net.cpp:67] Creating Layer label
I0626 03:07:21.215613  8898 net.cpp:358] label -> label
I0626 03:07:21.215643  8898 net.cpp:96] Setting up label
I0626 03:07:21.215662  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:21.330864  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:21.330971  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:21.331007  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:21.331024  8898 net.cpp:396] label_label_0_split <- label
I0626 03:07:21.331048  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:21.331075  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:21.331097  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:07:21.331115  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:21.331131  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:21.331162  8898 net.cpp:67] Creating Layer inner1
I0626 03:07:21.331179  8898 net.cpp:396] inner1 <- data
I0626 03:07:21.331200  8898 net.cpp:358] inner1 -> inner1
I0626 03:07:21.331228  8898 net.cpp:96] Setting up inner1
I0626 03:07:21.359436  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:21.359485  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:07:21.359496  8898 net.cpp:396] inner1relu <- inner1
I0626 03:07:21.359508  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:21.359519  8898 net.cpp:96] Setting up inner1relu
I0626 03:07:21.359527  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:21.359539  8898 net.cpp:67] Creating Layer inner2
I0626 03:07:21.359546  8898 net.cpp:396] inner2 <- inner1
I0626 03:07:21.359555  8898 net.cpp:358] inner2 -> inner2
I0626 03:07:21.359566  8898 net.cpp:96] Setting up inner2
I0626 03:07:21.362571  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:21.362587  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:07:21.362596  8898 net.cpp:396] inner2relu <- inner2
I0626 03:07:21.362606  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:21.362614  8898 net.cpp:96] Setting up inner2relu
I0626 03:07:21.362622  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:21.362632  8898 net.cpp:67] Creating Layer inner3
I0626 03:07:21.362639  8898 net.cpp:396] inner3 <- inner2
I0626 03:07:21.362656  8898 net.cpp:358] inner3 -> inner3
I0626 03:07:21.362668  8898 net.cpp:96] Setting up inner3
I0626 03:07:21.374697  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:21.374732  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:07:21.374748  8898 net.cpp:396] inner3relu <- inner3
I0626 03:07:21.374759  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:21.374775  8898 net.cpp:96] Setting up inner3relu
I0626 03:07:21.374783  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:21.374794  8898 net.cpp:67] Creating Layer output
I0626 03:07:21.374802  8898 net.cpp:396] output <- inner3
I0626 03:07:21.374812  8898 net.cpp:358] output -> output
I0626 03:07:21.374822  8898 net.cpp:96] Setting up output
I0626 03:07:21.376027  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:21.376047  8898 net.cpp:67] Creating Layer loss
I0626 03:07:21.376056  8898 net.cpp:396] loss <- output
I0626 03:07:21.376066  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:21.376078  8898 net.cpp:358] loss -> loss
I0626 03:07:21.376093  8898 net.cpp:358] loss -> std
I0626 03:07:21.376104  8898 net.cpp:358] loss -> ind
I0626 03:07:21.376116  8898 net.cpp:358] loss -> proba
I0626 03:07:21.376125  8898 net.cpp:96] Setting up loss
I0626 03:07:21.376133  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:21.376147  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:21.376154  8898 net.cpp:109]     with loss weight 1
I0626 03:07:21.376171  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:21.376178  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:21.376185  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:21.376199  8898 net.cpp:67] Creating Layer silence
I0626 03:07:21.376207  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:21.376216  8898 net.cpp:396] silence <- ind
I0626 03:07:21.376224  8898 net.cpp:396] silence <- proba
I0626 03:07:21.376232  8898 net.cpp:96] Setting up silence
I0626 03:07:21.376240  8898 net.cpp:172] silence does not need backward computation.
I0626 03:07:21.376247  8898 net.cpp:170] loss needs backward computation.
I0626 03:07:21.376255  8898 net.cpp:170] output needs backward computation.
I0626 03:07:21.376263  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:21.376271  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:07:21.376278  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:21.376286  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:07:21.376292  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:21.376299  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:07:21.376307  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:21.376317  8898 net.cpp:172] label does not need backward computation.
I0626 03:07:21.376324  8898 net.cpp:172] data does not need backward computation.
I0626 03:07:21.376332  8898 net.cpp:208] This network produces output loss
I0626 03:07:21.376339  8898 net.cpp:208] This network produces output std
I0626 03:07:21.376358  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:21.376368  8898 net.cpp:219] Network initialization done.
I0626 03:07:21.376376  8898 net.cpp:220] Memory required for data: 9903112
I0626 03:07:22.894888 20093 caffe.cpp:99] Use GPU with device ID 0
I0626 03:07:23.146946 20093 caffe.cpp:107] Starting Optimization
I0626 03:07:23.147042 20093 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:07:23.147065 20093 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:07:23.147262 20093 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:23.147285 20093 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:23.147384 20093 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00002230"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00002230"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:07:23.147461 20093 net.cpp:67] Creating Layer data
I0626 03:07:23.147486 20093 net.cpp:358] data -> data
I0626 03:07:23.147503 20093 net.cpp:96] Setting up data
I0626 03:07:23.147536 20093 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:23.224532 20093 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:23.224723 20093 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:23.224764 20093 net.cpp:67] Creating Layer label
I0626 03:07:23.224786 20093 net.cpp:358] label -> label
I0626 03:07:23.224818 20093 net.cpp:96] Setting up label
I0626 03:07:23.224838 20093 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:23.333317 20093 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:23.333458 20093 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:23.333494 20093 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:23.333514 20093 net.cpp:396] label_label_0_split <- label
I0626 03:07:23.333554 20093 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:23.333585 20093 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:23.333607 20093 net.cpp:96] Setting up label_label_0_split
I0626 03:07:23.333636 20093 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:23.333667 20093 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:23.333690 20093 net.cpp:67] Creating Layer inner1
I0626 03:07:23.333708 20093 net.cpp:396] inner1 <- data
I0626 03:07:23.333730 20093 net.cpp:358] inner1 -> inner1
I0626 03:07:23.333755 20093 net.cpp:96] Setting up inner1
I0626 03:07:23.360599 20093 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:23.360647 20093 net.cpp:67] Creating Layer inner1relu
I0626 03:07:23.360658 20093 net.cpp:396] inner1relu <- inner1
I0626 03:07:23.360671 20093 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:23.360682 20093 net.cpp:96] Setting up inner1relu
I0626 03:07:23.360697 20093 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:23.360708 20093 net.cpp:67] Creating Layer inner2
I0626 03:07:23.360716 20093 net.cpp:396] inner2 <- inner1
I0626 03:07:23.360728 20093 net.cpp:358] inner2 -> inner2
I0626 03:07:23.360738 20093 net.cpp:96] Setting up inner2
I0626 03:07:23.363790 20093 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:23.363809 20093 net.cpp:67] Creating Layer inner2relu
I0626 03:07:23.363817 20093 net.cpp:396] inner2relu <- inner2
I0626 03:07:23.363827 20093 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:23.363837 20093 net.cpp:96] Setting up inner2relu
I0626 03:07:23.363847 20093 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:23.363857 20093 net.cpp:67] Creating Layer inner3
I0626 03:07:23.363864 20093 net.cpp:396] inner3 <- inner2
I0626 03:07:23.363874 20093 net.cpp:358] inner3 -> inner3
I0626 03:07:23.363884 20093 net.cpp:96] Setting up inner3
I0626 03:07:23.376004 20093 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:23.376044 20093 net.cpp:67] Creating Layer inner3relu
I0626 03:07:23.376054 20093 net.cpp:396] inner3relu <- inner3
I0626 03:07:23.376065 20093 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:23.376076 20093 net.cpp:96] Setting up inner3relu
I0626 03:07:23.376085 20093 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:23.376096 20093 net.cpp:67] Creating Layer output
I0626 03:07:23.376104 20093 net.cpp:396] output <- inner3
I0626 03:07:23.376114 20093 net.cpp:358] output -> output
I0626 03:07:23.376124 20093 net.cpp:96] Setting up output
I0626 03:07:23.377372 20093 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:23.377390 20093 net.cpp:67] Creating Layer loss
I0626 03:07:23.377399 20093 net.cpp:396] loss <- output
I0626 03:07:23.377408 20093 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:23.377418 20093 net.cpp:358] loss -> loss
I0626 03:07:23.377431 20093 net.cpp:358] loss -> std
I0626 03:07:23.377442 20093 net.cpp:358] loss -> ind
I0626 03:07:23.377454 20093 net.cpp:358] loss -> proba
I0626 03:07:23.377470 20093 net.cpp:96] Setting up loss
I0626 03:07:23.377483 20093 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:23.377498 20093 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:23.377506 20093 net.cpp:109]     with loss weight 1
I0626 03:07:23.377542 20093 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:23.377550 20093 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:23.377558 20093 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:23.377571 20093 net.cpp:67] Creating Layer silence
I0626 03:07:23.377580 20093 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:23.377590 20093 net.cpp:396] silence <- ind
I0626 03:07:23.377599 20093 net.cpp:396] silence <- proba
I0626 03:07:23.377607 20093 net.cpp:96] Setting up silence
I0626 03:07:23.377629 20093 net.cpp:172] silence does not need backward computation.
I0626 03:07:23.377637 20093 net.cpp:170] loss needs backward computation.
I0626 03:07:23.377646 20093 net.cpp:170] output needs backward computation.
I0626 03:07:23.377655 20093 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:23.377662 20093 net.cpp:170] inner3 needs backward computation.
I0626 03:07:23.377671 20093 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:23.377678 20093 net.cpp:170] inner2 needs backward computation.
I0626 03:07:23.377686 20093 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:23.377699 20093 net.cpp:170] inner1 needs backward computation.
I0626 03:07:23.377707 20093 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:23.377717 20093 net.cpp:172] label does not need backward computation.
I0626 03:07:23.377724 20093 net.cpp:172] data does not need backward computation.
I0626 03:07:23.377732 20093 net.cpp:208] This network produces output loss
I0626 03:07:23.377740 20093 net.cpp:208] This network produces output std
I0626 03:07:23.377755 20093 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:23.377765 20093 net.cpp:219] Network initialization done.
I0626 03:07:23.377779 20093 net.cpp:220] Memory required for data: 9903112
I0626 03:07:23.377821 20093 solver.cpp:41] Solver scaffolding done.
I0626 03:07:23.377830 20093 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:07:23.388746 20093 solver.cpp:160] Solving net
I0626 03:07:23.410738 20093 solver.cpp:192] Iteration 0, loss = 0.125905
I0626 03:07:23.410790 20093 solver.cpp:207]     Train net output #0: loss = 0.125905 (* 1 = 0.125905 loss)
I0626 03:07:23.410802 20093 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:23.410825 20093 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:07:23.519038 20093 solver.cpp:192] Iteration 10, loss = 0.13448
I0626 03:07:23.519109 20093 solver.cpp:207]     Train net output #0: loss = 0.13448 (* 1 = 0.13448 loss)
I0626 03:07:23.519125 20093 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:23.519141 20093 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:07:23.620723 20093 solver.cpp:192] Iteration 20, loss = 0.143511
I0626 03:07:23.620779 20093 solver.cpp:207]     Train net output #0: loss = 0.143511 (* 1 = 0.143511 loss)
I0626 03:07:23.620790 20093 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:23.620802 20093 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:07:23.709324 20093 solver.cpp:192] Iteration 30, loss = 0.132507
I0626 03:07:23.709396 20093 solver.cpp:207]     Train net output #0: loss = 0.132507 (* 1 = 0.132507 loss)
I0626 03:07:23.709415 20093 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:23.709434 20093 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:07:23.810664 20093 solver.cpp:192] Iteration 40, loss = 0.123508
I0626 03:07:23.810734 20093 solver.cpp:207]     Train net output #0: loss = 0.123508 (* 1 = 0.123508 loss)
I0626 03:07:23.810752 20093 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:23.810770 20093 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:07:23.920542 20093 solver.cpp:192] Iteration 50, loss = 0.126838
I0626 03:07:23.920611 20093 solver.cpp:207]     Train net output #0: loss = 0.126838 (* 1 = 0.126838 loss)
I0626 03:07:23.920631 20093 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:23.920651 20093 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:07:24.031313 20093 solver.cpp:192] Iteration 60, loss = 0.139009
I0626 03:07:24.031375 20093 solver.cpp:207]     Train net output #0: loss = 0.139009 (* 1 = 0.139009 loss)
I0626 03:07:24.031392 20093 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:24.031409 20093 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:07:24.143101 20093 solver.cpp:192] Iteration 70, loss = 0.116644
I0626 03:07:24.143177 20093 solver.cpp:207]     Train net output #0: loss = 0.116644 (* 1 = 0.116644 loss)
I0626 03:07:24.143224 20093 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:24.143245 20093 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:07:24.261704 20093 solver.cpp:192] Iteration 80, loss = 0.123975
I0626 03:07:24.261762 20093 solver.cpp:207]     Train net output #0: loss = 0.123975 (* 1 = 0.123975 loss)
I0626 03:07:24.261778 20093 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:24.261795 20093 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:07:24.369506 20093 solver.cpp:192] Iteration 90, loss = 0.114342
I0626 03:07:24.369567 20093 solver.cpp:207]     Train net output #0: loss = 0.114342 (* 1 = 0.114342 loss)
I0626 03:07:24.369596 20093 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:24.369612 20093 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:07:24.484376 20093 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:07:24.526955 20093 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:07:24.555536 20093 solver.cpp:229] Iteration 100, loss = 0.118484
I0626 03:07:24.555574 20093 solver.cpp:234] Optimization Done.
I0626 03:07:24.555583 20093 caffe.cpp:121] Optimization Done.
I0626 03:07:24.629781  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:24.629818  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:24.629942  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:07:24.630043  8898 net.cpp:67] Creating Layer data
I0626 03:07:24.630056  8898 net.cpp:358] data -> data
I0626 03:07:24.630070  8898 net.cpp:96] Setting up data
I0626 03:07:24.630080  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:24.845173  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:24.846845  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:24.846900  8898 net.cpp:67] Creating Layer label
I0626 03:07:24.846922  8898 net.cpp:358] label -> label
I0626 03:07:24.846951  8898 net.cpp:96] Setting up label
I0626 03:07:24.846981  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:24.946450  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:24.946560  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:24.946594  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:24.946612  8898 net.cpp:396] label_label_0_split <- label
I0626 03:07:24.946636  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:24.946665  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:24.946686  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:07:24.946704  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:24.946720  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:24.946743  8898 net.cpp:67] Creating Layer inner1
I0626 03:07:24.946758  8898 net.cpp:396] inner1 <- data
I0626 03:07:24.946779  8898 net.cpp:358] inner1 -> inner1
I0626 03:07:24.946802  8898 net.cpp:96] Setting up inner1
I0626 03:07:24.974282  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:24.974334  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:07:24.974344  8898 net.cpp:396] inner1relu <- inner1
I0626 03:07:24.974355  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:24.974366  8898 net.cpp:96] Setting up inner1relu
I0626 03:07:24.974375  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:24.974386  8898 net.cpp:67] Creating Layer inner2
I0626 03:07:24.974393  8898 net.cpp:396] inner2 <- inner1
I0626 03:07:24.974406  8898 net.cpp:358] inner2 -> inner2
I0626 03:07:24.974417  8898 net.cpp:96] Setting up inner2
I0626 03:07:24.977387  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:24.977404  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:07:24.977412  8898 net.cpp:396] inner2relu <- inner2
I0626 03:07:24.977422  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:24.977432  8898 net.cpp:96] Setting up inner2relu
I0626 03:07:24.977439  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:24.977448  8898 net.cpp:67] Creating Layer inner3
I0626 03:07:24.977457  8898 net.cpp:396] inner3 <- inner2
I0626 03:07:24.977465  8898 net.cpp:358] inner3 -> inner3
I0626 03:07:24.977476  8898 net.cpp:96] Setting up inner3
I0626 03:07:24.989042  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:24.989080  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:07:24.989089  8898 net.cpp:396] inner3relu <- inner3
I0626 03:07:24.989100  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:24.989110  8898 net.cpp:96] Setting up inner3relu
I0626 03:07:24.989117  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:24.989128  8898 net.cpp:67] Creating Layer output
I0626 03:07:24.989135  8898 net.cpp:396] output <- inner3
I0626 03:07:24.989145  8898 net.cpp:358] output -> output
I0626 03:07:24.989156  8898 net.cpp:96] Setting up output
I0626 03:07:24.990320  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:24.990339  8898 net.cpp:67] Creating Layer loss
I0626 03:07:24.990346  8898 net.cpp:396] loss <- output
I0626 03:07:24.990355  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:24.990365  8898 net.cpp:358] loss -> loss
I0626 03:07:24.990377  8898 net.cpp:358] loss -> std
I0626 03:07:24.990388  8898 net.cpp:358] loss -> ind
I0626 03:07:24.990398  8898 net.cpp:358] loss -> proba
I0626 03:07:24.990408  8898 net.cpp:96] Setting up loss
I0626 03:07:24.990416  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:24.990430  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:24.990438  8898 net.cpp:109]     with loss weight 1
I0626 03:07:24.990454  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:24.990461  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:24.990468  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:24.990481  8898 net.cpp:67] Creating Layer silence
I0626 03:07:24.990489  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:24.990497  8898 net.cpp:396] silence <- ind
I0626 03:07:24.990505  8898 net.cpp:396] silence <- proba
I0626 03:07:24.990528  8898 net.cpp:96] Setting up silence
I0626 03:07:24.990537  8898 net.cpp:172] silence does not need backward computation.
I0626 03:07:24.990545  8898 net.cpp:170] loss needs backward computation.
I0626 03:07:24.990552  8898 net.cpp:170] output needs backward computation.
I0626 03:07:24.990559  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:24.990566  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:07:24.990574  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:24.990581  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:07:24.990588  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:24.990595  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:07:24.990602  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:24.990610  8898 net.cpp:172] label does not need backward computation.
I0626 03:07:24.990617  8898 net.cpp:172] data does not need backward computation.
I0626 03:07:24.990624  8898 net.cpp:208] This network produces output loss
I0626 03:07:24.990631  8898 net.cpp:208] This network produces output std
I0626 03:07:24.990648  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:24.990659  8898 net.cpp:219] Network initialization done.
I0626 03:07:24.990666  8898 net.cpp:220] Memory required for data: 9903112
I0626 03:07:26.456974 20324 caffe.cpp:99] Use GPU with device ID 0
I0626 03:07:26.700165 20324 caffe.cpp:107] Starting Optimization
I0626 03:07:26.700261 20324 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:07:26.700286 20324 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:07:26.700477 20324 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:26.700491 20324 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:26.700584 20324 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000146"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000146"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:07:26.700666 20324 net.cpp:67] Creating Layer data
I0626 03:07:26.700680 20324 net.cpp:358] data -> data
I0626 03:07:26.700696 20324 net.cpp:96] Setting up data
I0626 03:07:26.700729 20324 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:26.790184 20324 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:26.790443 20324 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:26.790485 20324 net.cpp:67] Creating Layer label
I0626 03:07:26.790508 20324 net.cpp:358] label -> label
I0626 03:07:26.790539 20324 net.cpp:96] Setting up label
I0626 03:07:26.790560 20324 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:26.907258 20324 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:26.907402 20324 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:26.907446 20324 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:26.907495 20324 net.cpp:396] label_label_0_split <- label
I0626 03:07:26.907541 20324 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:26.907572 20324 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:26.907594 20324 net.cpp:96] Setting up label_label_0_split
I0626 03:07:26.907624 20324 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:26.907641 20324 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:26.907665 20324 net.cpp:67] Creating Layer inner1
I0626 03:07:26.907682 20324 net.cpp:396] inner1 <- data
I0626 03:07:26.907703 20324 net.cpp:358] inner1 -> inner1
I0626 03:07:26.907728 20324 net.cpp:96] Setting up inner1
I0626 03:07:26.935583 20324 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:26.935632 20324 net.cpp:67] Creating Layer inner1relu
I0626 03:07:26.935643 20324 net.cpp:396] inner1relu <- inner1
I0626 03:07:26.935655 20324 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:26.935667 20324 net.cpp:96] Setting up inner1relu
I0626 03:07:26.935683 20324 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:26.935693 20324 net.cpp:67] Creating Layer inner2
I0626 03:07:26.935703 20324 net.cpp:396] inner2 <- inner1
I0626 03:07:26.935712 20324 net.cpp:358] inner2 -> inner2
I0626 03:07:26.935724 20324 net.cpp:96] Setting up inner2
I0626 03:07:26.938736 20324 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:26.938753 20324 net.cpp:67] Creating Layer inner2relu
I0626 03:07:26.938762 20324 net.cpp:396] inner2relu <- inner2
I0626 03:07:26.938772 20324 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:26.938781 20324 net.cpp:96] Setting up inner2relu
I0626 03:07:26.938791 20324 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:26.938799 20324 net.cpp:67] Creating Layer inner3
I0626 03:07:26.938808 20324 net.cpp:396] inner3 <- inner2
I0626 03:07:26.938818 20324 net.cpp:358] inner3 -> inner3
I0626 03:07:26.938828 20324 net.cpp:96] Setting up inner3
I0626 03:07:26.950889 20324 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:26.950925 20324 net.cpp:67] Creating Layer inner3relu
I0626 03:07:26.950935 20324 net.cpp:396] inner3relu <- inner3
I0626 03:07:26.950945 20324 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:26.950956 20324 net.cpp:96] Setting up inner3relu
I0626 03:07:26.950965 20324 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:26.950983 20324 net.cpp:67] Creating Layer output
I0626 03:07:26.950991 20324 net.cpp:396] output <- inner3
I0626 03:07:26.951001 20324 net.cpp:358] output -> output
I0626 03:07:26.951012 20324 net.cpp:96] Setting up output
I0626 03:07:26.952239 20324 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:26.952257 20324 net.cpp:67] Creating Layer loss
I0626 03:07:26.952266 20324 net.cpp:396] loss <- output
I0626 03:07:26.952275 20324 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:26.952286 20324 net.cpp:358] loss -> loss
I0626 03:07:26.952299 20324 net.cpp:358] loss -> std
I0626 03:07:26.952311 20324 net.cpp:358] loss -> ind
I0626 03:07:26.952322 20324 net.cpp:358] loss -> proba
I0626 03:07:26.952333 20324 net.cpp:96] Setting up loss
I0626 03:07:26.952345 20324 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:26.952360 20324 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:26.952368 20324 net.cpp:109]     with loss weight 1
I0626 03:07:26.952399 20324 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:26.952407 20324 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:26.952415 20324 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:26.952430 20324 net.cpp:67] Creating Layer silence
I0626 03:07:26.952438 20324 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:26.952447 20324 net.cpp:396] silence <- ind
I0626 03:07:26.952456 20324 net.cpp:396] silence <- proba
I0626 03:07:26.952466 20324 net.cpp:96] Setting up silence
I0626 03:07:26.952488 20324 net.cpp:172] silence does not need backward computation.
I0626 03:07:26.952497 20324 net.cpp:170] loss needs backward computation.
I0626 03:07:26.952505 20324 net.cpp:170] output needs backward computation.
I0626 03:07:26.952514 20324 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:26.952522 20324 net.cpp:170] inner3 needs backward computation.
I0626 03:07:26.952530 20324 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:26.952538 20324 net.cpp:170] inner2 needs backward computation.
I0626 03:07:26.952546 20324 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:26.952554 20324 net.cpp:170] inner1 needs backward computation.
I0626 03:07:26.952564 20324 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:26.952571 20324 net.cpp:172] label does not need backward computation.
I0626 03:07:26.952580 20324 net.cpp:172] data does not need backward computation.
I0626 03:07:26.952587 20324 net.cpp:208] This network produces output loss
I0626 03:07:26.952596 20324 net.cpp:208] This network produces output std
I0626 03:07:26.952611 20324 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:26.952621 20324 net.cpp:219] Network initialization done.
I0626 03:07:26.952635 20324 net.cpp:220] Memory required for data: 9903112
I0626 03:07:26.952677 20324 solver.cpp:41] Solver scaffolding done.
I0626 03:07:26.952687 20324 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:07:26.963659 20324 solver.cpp:160] Solving net
I0626 03:07:26.985440 20324 solver.cpp:192] Iteration 0, loss = 0.123578
I0626 03:07:26.985491 20324 solver.cpp:207]     Train net output #0: loss = 0.123578 (* 1 = 0.123578 loss)
I0626 03:07:26.985503 20324 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:26.985519 20324 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:07:27.085741 20324 solver.cpp:192] Iteration 10, loss = 0.14481
I0626 03:07:27.085803 20324 solver.cpp:207]     Train net output #0: loss = 0.14481 (* 1 = 0.14481 loss)
I0626 03:07:27.085820 20324 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:27.085837 20324 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:07:27.198302 20324 solver.cpp:192] Iteration 20, loss = 0.126538
I0626 03:07:27.198369 20324 solver.cpp:207]     Train net output #0: loss = 0.126538 (* 1 = 0.126538 loss)
I0626 03:07:27.198386 20324 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:27.198402 20324 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:07:27.309484 20324 solver.cpp:192] Iteration 30, loss = 0.119865
I0626 03:07:27.309559 20324 solver.cpp:207]     Train net output #0: loss = 0.119865 (* 1 = 0.119865 loss)
I0626 03:07:27.309579 20324 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:27.309598 20324 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:07:27.420024 20324 solver.cpp:192] Iteration 40, loss = 0.111228
I0626 03:07:27.420094 20324 solver.cpp:207]     Train net output #0: loss = 0.111228 (* 1 = 0.111228 loss)
I0626 03:07:27.420112 20324 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:27.420130 20324 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:07:27.520023 20324 solver.cpp:192] Iteration 50, loss = 0.126551
I0626 03:07:27.520097 20324 solver.cpp:207]     Train net output #0: loss = 0.126551 (* 1 = 0.126551 loss)
I0626 03:07:27.520109 20324 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:27.520121 20324 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:07:27.612715 20324 solver.cpp:192] Iteration 60, loss = 0.112941
I0626 03:07:27.612777 20324 solver.cpp:207]     Train net output #0: loss = 0.112941 (* 1 = 0.112941 loss)
I0626 03:07:27.612804 20324 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:27.612817 20324 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:07:27.697167 20324 solver.cpp:192] Iteration 70, loss = 0.119541
I0626 03:07:27.697233 20324 solver.cpp:207]     Train net output #0: loss = 0.119541 (* 1 = 0.119541 loss)
I0626 03:07:27.697273 20324 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:27.697288 20324 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:07:27.806635 20324 solver.cpp:192] Iteration 80, loss = 0.116509
I0626 03:07:27.806716 20324 solver.cpp:207]     Train net output #0: loss = 0.116509 (* 1 = 0.116509 loss)
I0626 03:07:27.806738 20324 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:27.806759 20324 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:07:27.915325 20324 solver.cpp:192] Iteration 90, loss = 0.135393
I0626 03:07:27.915395 20324 solver.cpp:207]     Train net output #0: loss = 0.135393 (* 1 = 0.135393 loss)
I0626 03:07:27.915413 20324 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:27.915429 20324 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:07:28.032483 20324 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:07:28.075742 20324 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:07:28.103404 20324 solver.cpp:229] Iteration 100, loss = 0.156247
I0626 03:07:28.103449 20324 solver.cpp:234] Optimization Done.
I0626 03:07:28.103458 20324 caffe.cpp:121] Optimization Done.
I0626 03:07:28.166339  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:28.166370  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:28.166488  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:07:28.166579  8898 net.cpp:67] Creating Layer data
I0626 03:07:28.166592  8898 net.cpp:358] data -> data
I0626 03:07:28.166607  8898 net.cpp:96] Setting up data
I0626 03:07:28.166617  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:28.391645  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:28.393414  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:28.393472  8898 net.cpp:67] Creating Layer label
I0626 03:07:28.393497  8898 net.cpp:358] label -> label
I0626 03:07:28.393527  8898 net.cpp:96] Setting up label
I0626 03:07:28.393545  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:28.475257  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:28.475376  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:28.475411  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:28.475430  8898 net.cpp:396] label_label_0_split <- label
I0626 03:07:28.475455  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:28.475510  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:28.475533  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:07:28.475551  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:28.475567  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:28.475589  8898 net.cpp:67] Creating Layer inner1
I0626 03:07:28.475605  8898 net.cpp:396] inner1 <- data
I0626 03:07:28.475626  8898 net.cpp:358] inner1 -> inner1
I0626 03:07:28.475654  8898 net.cpp:96] Setting up inner1
I0626 03:07:28.501879  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:28.501932  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:07:28.501942  8898 net.cpp:396] inner1relu <- inner1
I0626 03:07:28.501953  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:28.501965  8898 net.cpp:96] Setting up inner1relu
I0626 03:07:28.501972  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:28.501983  8898 net.cpp:67] Creating Layer inner2
I0626 03:07:28.501991  8898 net.cpp:396] inner2 <- inner1
I0626 03:07:28.502001  8898 net.cpp:358] inner2 -> inner2
I0626 03:07:28.502010  8898 net.cpp:96] Setting up inner2
I0626 03:07:28.504992  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:28.505010  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:07:28.505019  8898 net.cpp:396] inner2relu <- inner2
I0626 03:07:28.505028  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:28.505038  8898 net.cpp:96] Setting up inner2relu
I0626 03:07:28.505060  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:28.505070  8898 net.cpp:67] Creating Layer inner3
I0626 03:07:28.505079  8898 net.cpp:396] inner3 <- inner2
I0626 03:07:28.505089  8898 net.cpp:358] inner3 -> inner3
I0626 03:07:28.505098  8898 net.cpp:96] Setting up inner3
I0626 03:07:28.517033  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:28.517076  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:07:28.517086  8898 net.cpp:396] inner3relu <- inner3
I0626 03:07:28.517096  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:28.517107  8898 net.cpp:96] Setting up inner3relu
I0626 03:07:28.517115  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:28.517127  8898 net.cpp:67] Creating Layer output
I0626 03:07:28.517134  8898 net.cpp:396] output <- inner3
I0626 03:07:28.517144  8898 net.cpp:358] output -> output
I0626 03:07:28.517155  8898 net.cpp:96] Setting up output
I0626 03:07:28.518357  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:28.518376  8898 net.cpp:67] Creating Layer loss
I0626 03:07:28.518385  8898 net.cpp:396] loss <- output
I0626 03:07:28.518395  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:28.518406  8898 net.cpp:358] loss -> loss
I0626 03:07:28.518420  8898 net.cpp:358] loss -> std
I0626 03:07:28.518431  8898 net.cpp:358] loss -> ind
I0626 03:07:28.518442  8898 net.cpp:358] loss -> proba
I0626 03:07:28.518453  8898 net.cpp:96] Setting up loss
I0626 03:07:28.518461  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:28.518477  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:28.518486  8898 net.cpp:109]     with loss weight 1
I0626 03:07:28.518502  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:28.518509  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:28.518517  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:28.518532  8898 net.cpp:67] Creating Layer silence
I0626 03:07:28.518539  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:28.518548  8898 net.cpp:396] silence <- ind
I0626 03:07:28.518558  8898 net.cpp:396] silence <- proba
I0626 03:07:28.518565  8898 net.cpp:96] Setting up silence
I0626 03:07:28.518573  8898 net.cpp:172] silence does not need backward computation.
I0626 03:07:28.518580  8898 net.cpp:170] loss needs backward computation.
I0626 03:07:28.518589  8898 net.cpp:170] output needs backward computation.
I0626 03:07:28.518596  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:28.518604  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:07:28.518611  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:28.518618  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:07:28.518626  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:28.518633  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:07:28.518641  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:28.518649  8898 net.cpp:172] label does not need backward computation.
I0626 03:07:28.518657  8898 net.cpp:172] data does not need backward computation.
I0626 03:07:28.518664  8898 net.cpp:208] This network produces output loss
I0626 03:07:28.518671  8898 net.cpp:208] This network produces output std
I0626 03:07:28.518689  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:28.518699  8898 net.cpp:219] Network initialization done.
I0626 03:07:28.518707  8898 net.cpp:220] Memory required for data: 9903112
1]
 [ 0.01236192  0.0034886   0.00801746  0.00716465  0.96896737]
 [ 0.00279839  0.00209454  0.9900886   0.00185633  0.00316214]
 [ 0.00253709  0.00212173  0.99070755  0.00173039  0.00290325]
 [ 0.00858322  0.00283866  0.00538467  0.00396498  0.97922846]]
[2 4 4 ..., 4 3 4]
0.0277416558301
[[ 0.00735632  0.00668903  0.97471324  0.00416938  0.00707204]
 [ 0.01323744  0.0069794   0.00870946  0.00429152  0.96678218]
 [ 0.01337404  0.00716188  0.00949253  0.00436175  0.96560979]
 [ 0.00341331  0.00236059  0.98954219  0.00174387  0.00294004]
 [ 0.02165281  0.01434434  0.01737562  0.00689987  0.93972737]
 [ 0.00340451  0.0018739   0.99031421  0.00172793  0.00267944]
 [ 0.00919131  0.00232032  0.00478217  0.0037266   0.9799796 ]
 [ 0.00323853  0.00186951  0.99068346  0.0016347   0.0025738 ]
 [ 0.0033541   0.00208042  0.99026507  0.00167666  0.00262375]
 [ 0.00795102  0.00244403  0.00397075  0.00264657  0.98298763]]
[2 4 4 ..., 4 3 4]
0.0290420459471
[[ 0.00576936  0.00478347  0.97981931  0.00358891  0.00603894]
 [ 0.00686927  0.00285807  0.00409671  0.00207107  0.98410487]
 [ 0.00619618  0.00235541  0.0037135   0.00186612  0.98586878]
 [ 0.00259437  0.00164503  0.99176068  0.00151684  0.00248308]
 [ 0.00946473  0.00429701  0.00622367  0.00278839  0.97722621]
 [ 0.00263776  0.00141728  0.99207528  0.00152485  0.00234484]
 [ 0.0110183   0.00238794  0.00512058  0.00404666  0.97742652]
 [ 0.00245947  0.00137911  0.99256415  0.00140646  0.00219081]
 [ 0.00217278  0.00136212  0.99330189  0.00122042  0.0019428 ]
 [ 0.00785774  0.00206877  0.00368312  0.00245905  0.98393132]]
[2 4 4 ..., 4 3 4]
0.00996965756394
[[ 0.00395075  0.00382622  0.98548813  0.00246785  0.00426705]
 [ 0.00483791  0.00206223  0.00320264  0.00161639  0.98828082]
 [ 0.00463669  0.00192534  0.003131    0.0015788   0.98872817]
 [ 0.00191722  0.0013838   0.99373056  0.00110136  0.00186706]
 [ 0.00609336  0.00285045  0.00442167  0.00203353  0.984601  ]
 [ 0.00200926  0.00121293  0.99377277  0.00116955  0.0018355 ]
 [ 0.00726141  0.00188001  0.00393604  0.00290783  0.98401472]
 [ 0.00189826  0.0011846   0.99405836  0.0011153   0.00174348]
 [ 0.00177391  0.00123025  0.99429377  0.00105242  0.00164965]
 [ 0.00546974  0.00167479  0.00299766  0.00192001  0.9879378 ]]
[2 4 4 ..., 4 3 4]
0.0104031209363
[[  3.19642226e-03   2.99230531e-03   9.88543918e-01   1.91175282e-03
    3.35560211e-03]
 [  8.29223564e-03   4.62580000e-03   6.01606020e-03   2.72099145e-03
    9.78344913e-01]
 [  7.28665014e-03   3.82125043e-03   5.34680919e-03   2.45078689e-03
    9.81094503e-01]
 [  1.87915246e-03   1.27236148e-03   9.94117610e-01   9.94970374e-04
    1.73590551e-03]
 [  1.33264599e-02   8.62132973e-03   1.07837396e-02   4.26539846e-03
    9.63003072e-01]
 [  1.92274782e-03   1.10045231e-03   9.94224768e-01   1.05646365e-03
    1.69556845e-03]
 [  3.70448651e-03   1.12014595e-03   2.11903780e-03   1.45151581e-03
    9.91604814e-01]
 [  1.79207767e-03   1.06309956e-03   9.94545734e-01   1.00191404e-03
    1.59717507e-03]
 [  1.60702760e-03   1.09871002e-03   9.94897250e-01   9.13551668e-04
    1.48346095e-03]
 [  3.79884482e-03   1.42229806e-03   2.22046707e-03   1.28894917e-03
    9.91269441e-01]]
[2 4 4 ..., 4 3 4]
0.0160381447768
[[  2.41160982e-03   2.23285670e-03   9.91530726e-01   1.41228911e-03
    2.41251849e-03]
 [  4.41531684e-03   1.50062702e-03   2.49769931e-03   1.52574973e-03
    9.90060607e-01]
 [  5.94097255e-03   1.78385800e-03   3.24044338e-03   2.15727288e-03
    9.86877453e-01]
 [  1.89945674e-03   1.25535631e-03   9.94261293e-01   9.42114314e-04
    1.64177919e-03]
 [  4.23792822e-03   1.43664129e-03   2.42815340e-03   1.46001464e-03
    9.90437262e-01]
 [  1.91593391e-03   1.08970785e-03   9.94366456e-01   1.02532021e-03
    1.60258201e-03]
 [  1.80393417e-02   4.23981386e-03   8.73515207e-03   7.30810120e-03
    9.61677591e-01]
 [  1.71163379e-03   1.02513316e-03   9.94872343e-01   9.34821428e-04
    1.45606862e-03]
 [  1.48443013e-03   1.03694033e-03   9.95316575e-01   8.46740264e-04
    1.31531463e-03]
 [  1.32894581e-02   3.49968993e-03   6.46664882e-03   4.792I0626 03:07:29.957875 20555 caffe.cpp:99] Use GPU with device ID 0
I0626 03:07:30.200736 20555 caffe.cpp:107] Starting Optimization
I0626 03:07:30.200827 20555 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:07:30.200861 20555 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:07:30.201059 20555 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:30.201072 20555 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:30.201167 20555 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000369"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000369"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:07:30.201244 20555 net.cpp:67] Creating Layer data
I0626 03:07:30.201257 20555 net.cpp:358] data -> data
I0626 03:07:30.201274 20555 net.cpp:96] Setting up data
I0626 03:07:30.201306 20555 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:30.286164 20555 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:30.286346 20555 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:30.286386 20555 net.cpp:67] Creating Layer label
I0626 03:07:30.286408 20555 net.cpp:358] label -> label
I0626 03:07:30.286442 20555 net.cpp:96] Setting up label
I0626 03:07:30.286461 20555 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:30.403285 20555 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:30.403416 20555 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:30.403450 20555 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:30.403488 20555 net.cpp:396] label_label_0_split <- label
I0626 03:07:30.403529 20555 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:30.403559 20555 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:30.403594 20555 net.cpp:96] Setting up label_label_0_split
I0626 03:07:30.403623 20555 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:30.403641 20555 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:30.403666 20555 net.cpp:67] Creating Layer inner1
I0626 03:07:30.403682 20555 net.cpp:396] inner1 <- data
I0626 03:07:30.403705 20555 net.cpp:358] inner1 -> inner1
I0626 03:07:30.403730 20555 net.cpp:96] Setting up inner1
I0626 03:07:30.433207 20555 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:30.433265 20555 net.cpp:67] Creating Layer inner1relu
I0626 03:07:30.433276 20555 net.cpp:396] inner1relu <- inner1
I0626 03:07:30.433287 20555 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:30.433300 20555 net.cpp:96] Setting up inner1relu
I0626 03:07:30.433315 20555 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:30.433326 20555 net.cpp:67] Creating Layer inner2
I0626 03:07:30.433334 20555 net.cpp:396] inner2 <- inner1
I0626 03:07:30.433344 20555 net.cpp:358] inner2 -> inner2
I0626 03:07:30.433356 20555 net.cpp:96] Setting up inner2
I0626 03:07:30.436349 20555 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:30.436367 20555 net.cpp:67] Creating Layer inner2relu
I0626 03:07:30.436375 20555 net.cpp:396] inner2relu <- inner2
I0626 03:07:30.436384 20555 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:30.436394 20555 net.cpp:96] Setting up inner2relu
I0626 03:07:30.436403 20555 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:30.436413 20555 net.cpp:67] Creating Layer inner3
I0626 03:07:30.436420 20555 net.cpp:396] inner3 <- inner2
I0626 03:07:30.436430 20555 net.cpp:358] inner3 -> inner3
I0626 03:07:30.436439 20555 net.cpp:96] Setting up inner3
I0626 03:07:30.448470 20555 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:30.448513 20555 net.cpp:67] Creating Layer inner3relu
I0626 03:07:30.448523 20555 net.cpp:396] inner3relu <- inner3
I0626 03:07:30.448535 20555 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:30.448547 20555 net.cpp:96] Setting up inner3relu
I0626 03:07:30.448556 20555 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:30.448567 20555 net.cpp:67] Creating Layer output
I0626 03:07:30.448575 20555 net.cpp:396] output <- inner3
I0626 03:07:30.448586 20555 net.cpp:358] output -> output
I0626 03:07:30.448596 20555 net.cpp:96] Setting up output
I0626 03:07:30.449833 20555 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:30.449859 20555 net.cpp:67] Creating Layer loss
I0626 03:07:30.449868 20555 net.cpp:396] loss <- output
I0626 03:07:30.449879 20555 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:30.449890 20555 net.cpp:358] loss -> loss
I0626 03:07:30.449905 20555 net.cpp:358] loss -> std
I0626 03:07:30.449916 20555 net.cpp:358] loss -> ind
I0626 03:07:30.449928 20555 net.cpp:358] loss -> proba
I0626 03:07:30.449939 20555 net.cpp:96] Setting up loss
I0626 03:07:30.449952 20555 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:30.449967 20555 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:30.449976 20555 net.cpp:109]     with loss weight 1
I0626 03:07:30.450013 20555 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:30.450022 20555 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:30.450031 20555 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:30.450044 20555 net.cpp:67] Creating Layer silence
I0626 03:07:30.450053 20555 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:30.450062 20555 net.cpp:396] silence <- ind
I0626 03:07:30.450071 20555 net.cpp:396] silence <- proba
I0626 03:07:30.450080 20555 net.cpp:96] Setting up silence
I0626 03:07:30.450103 20555 net.cpp:172] silence does not need backward computation.
I0626 03:07:30.450112 20555 net.cpp:170] loss needs backward computation.
I0626 03:07:30.450120 20555 net.cpp:170] output needs backward computation.
I0626 03:07:30.450129 20555 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:30.450136 20555 net.cpp:170] inner3 needs backward computation.
I0626 03:07:30.450145 20555 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:30.450157 20555 net.cpp:170] inner2 needs backward computation.
I0626 03:07:30.450166 20555 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:30.450175 20555 net.cpp:170] inner1 needs backward computation.
I0626 03:07:30.450182 20555 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:30.450191 20555 net.cpp:172] label does not need backward computation.
I0626 03:07:30.450198 20555 net.cpp:172] data does not need backward computation.
I0626 03:07:30.450206 20555 net.cpp:208] This network produces output loss
I0626 03:07:30.450215 20555 net.cpp:208] This network produces output std
I0626 03:07:30.450229 20555 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:30.450240 20555 net.cpp:219] Network initialization done.
I0626 03:07:30.450253 20555 net.cpp:220] Memory required for data: 9903112
I0626 03:07:30.450294 20555 solver.cpp:41] Solver scaffolding done.
I0626 03:07:30.450304 20555 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:07:30.462033 20555 solver.cpp:160] Solving net
I0626 03:07:30.485023 20555 solver.cpp:192] Iteration 0, loss = 0.143961
I0626 03:07:30.485077 20555 solver.cpp:207]     Train net output #0: loss = 0.143961 (* 1 = 0.143961 loss)
I0626 03:07:30.485090 20555 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:30.485118 20555 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:07:30.576458 20555 solver.cpp:192] Iteration 10, loss = 0.118977
I0626 03:07:30.576530 20555 solver.cpp:207]     Train net output #0: loss = 0.118977 (* 1 = 0.118977 loss)
I0626 03:07:30.576547 20555 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:30.576565 20555 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:07:30.686183 20555 solver.cpp:192] Iteration 20, loss = 0.109397
I0626 03:07:30.686244 20555 solver.cpp:207]     Train net output #0: loss = 0.109397 (* 1 = 0.109397 loss)
I0626 03:07:30.686256 20555 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:30.686269 20555 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:07:30.791218 20555 solver.cpp:192] Iteration 30, loss = 0.104714
I0626 03:07:30.791278 20555 solver.cpp:207]     Train net output #0: loss = 0.104714 (* 1 = 0.104714 loss)
I0626 03:07:30.791290 20555 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:30.791301 20555 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:07:30.898939 20555 solver.cpp:192] Iteration 40, loss = 0.116379
I0626 03:07:30.899008 20555 solver.cpp:207]     Train net output #0: loss = 0.116379 (* 1 = 0.116379 loss)
I0626 03:07:30.899027 20555 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:30.899044 20555 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:07:31.007161 20555 solver.cpp:192] Iteration 50, loss = 0.102457
I0626 03:07:31.007217 20555 solver.cpp:207]     Train net output #0: loss = 0.102457 (* 1 = 0.102457 loss)
I0626 03:07:31.007231 20555 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:31.007244 20555 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:07:31.124867 20555 solver.cpp:192] Iteration 60, loss = 0.110032
I0626 03:07:31.124927 20555 solver.cpp:207]     Train net output #0: loss = 0.110032 (* 1 = 0.110032 loss)
I0626 03:07:31.124939 20555 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:31.124950 20555 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:07:31.235604 20555 solver.cpp:192] Iteration 70, loss = 0.103486
I0626 03:07:31.235663 20555 solver.cpp:207]     Train net output #0: loss = 0.103486 (* 1 = 0.103486 loss)
I0626 03:07:31.235698 20555 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:31.235715 20555 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:07:31.345930 20555 solver.cpp:192] Iteration 80, loss = 0.105956
I0626 03:07:31.346000 20555 solver.cpp:207]     Train net output #0: loss = 0.105956 (* 1 = 0.105956 loss)
I0626 03:07:31.346017 20555 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:31.346047 20555 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:07:31.454571 20555 solver.cpp:192] Iteration 90, loss = 0.110684
I0626 03:07:31.454634 20555 solver.cpp:207]     Train net output #0: loss = 0.110684 (* 1 = 0.110684 loss)
I0626 03:07:31.454650 20555 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:31.454666 20555 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:07:31.572263 20555 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:07:31.614367 20555 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:07:31.642390 20555 solver.cpp:229] Iteration 100, loss = 0.10725
I0626 03:07:31.642434 20555 solver.cpp:234] Optimization Done.
I0626 03:07:31.642443 20555 caffe.cpp:121] Optimization Done.
I0626 03:07:31.714493  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:31.714524  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:31.714649  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:07:31.714743  8898 net.cpp:67] Creating Layer data
I0626 03:07:31.714758  8898 net.cpp:358] data -> data
I0626 03:07:31.714773  8898 net.cpp:96] Setting up data
I0626 03:07:31.714783  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:31.945931  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:31.947723  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:31.947785  8898 net.cpp:67] Creating Layer label
I0626 03:07:31.947800  8898 net.cpp:358] label -> label
I0626 03:07:31.947818  8898 net.cpp:96] Setting up label
I0626 03:07:31.947829  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:32.055547  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:32.055642  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:32.055672  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:32.055688  8898 net.cpp:396] label_label_0_split <- label
I0626 03:07:32.055709  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:32.055732  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:32.055752  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:07:32.055766  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:32.055780  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:32.055799  8898 net.cpp:67] Creating Layer inner1
I0626 03:07:32.055812  8898 net.cpp:396] inner1 <- data
I0626 03:07:32.055830  8898 net.cpp:358] inner1 -> inner1
I0626 03:07:32.055852  8898 net.cpp:96] Setting up inner1
I0626 03:07:32.082175  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:32.082226  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:07:32.082237  8898 net.cpp:396] inner1relu <- inner1
I0626 03:07:32.082247  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:32.082258  8898 net.cpp:96] Setting up inner1relu
I0626 03:07:32.082267  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:32.082278  8898 net.cpp:67] Creating Layer inner2
I0626 03:07:32.082284  8898 net.cpp:396] inner2 <- inner1
I0626 03:07:32.082296  8898 net.cpp:358] inner2 -> inner2
I0626 03:07:32.082307  8898 net.cpp:96] Setting up inner2
I0626 03:07:32.085250  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:32.085269  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:07:32.085278  8898 net.cpp:396] inner2relu <- inner2
I0626 03:07:32.085288  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:32.085296  8898 net.cpp:96] Setting up inner2relu
I0626 03:07:32.085304  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:32.085314  8898 net.cpp:67] Creating Layer inner3
I0626 03:07:32.085321  8898 net.cpp:396] inner3 <- inner2
I0626 03:07:32.085330  8898 net.cpp:358] inner3 -> inner3
I0626 03:07:32.085340  8898 net.cpp:96] Setting up inner3
I0626 03:07:32.097134  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:32.097172  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:07:32.097182  8898 net.cpp:396] inner3relu <- inner3
I0626 03:07:32.097192  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:32.097201  8898 net.cpp:96] Setting up inner3relu
I0626 03:07:32.097209  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:32.097219  8898 net.cpp:67] Creating Layer output
I0626 03:07:32.097226  8898 net.cpp:396] output <- inner3
I0626 03:07:32.097237  8898 net.cpp:358] output -> output
I0626 03:07:32.097247  8898 net.cpp:96] Setting up output
I0626 03:07:32.098410  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:32.098428  8898 net.cpp:67] Creating Layer loss
I0626 03:07:32.098435  8898 net.cpp:396] loss <- output
I0626 03:07:32.098443  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:32.098454  8898 net.cpp:358] loss -> loss
I0626 03:07:32.098469  8898 net.cpp:358] loss -> std
I0626 03:07:32.098479  8898 net.cpp:358] loss -> ind
I0626 03:07:32.098489  8898 net.cpp:358] loss -> proba
I0626 03:07:32.098498  8898 net.cpp:96] Setting up loss
I0626 03:07:32.098506  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:32.098520  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:32.098527  8898 net.cpp:109]     with loss weight 1
I0626 03:07:32.098543  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:32.098551  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:32.098557  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:32.098570  8898 net.cpp:67] Creating Layer silence
I0626 03:07:32.098592  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:32.098600  8898 net.cpp:396] silence <- ind
I0626 03:07:32.098609  8898 net.cpp:396] silence <- proba
I0626 03:07:32.098618  8898 net.cpp:96] Setting up silence
I0626 03:07:32.098624  8898 net.cpp:172] silence does not need backward computation.
I0626 03:07:32.098631  8898 net.cpp:170] loss needs backward computation.
I0626 03:07:32.098639  8898 net.cpp:170] output needs backward computation.
I0626 03:07:32.098646  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:32.098654  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:07:32.098661  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:32.098668  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:07:32.098675  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:32.098683  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:07:32.098690  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:32.098698  8898 net.cpp:172] label does not need backward computation.
I0626 03:07:32.098706  8898 net.cpp:172] data does not need backward computation.
I0626 03:07:32.098712  8898 net.cpp:208] This network produces output loss
I0626 03:07:32.098719  8898 net.cpp:208] This network produces output std
I0626 03:07:32.098734  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:32.098745  8898 net.cpp:219] Network initialization done.
I0626 03:07:32.098752  8898 net.cpp:220] Memory required for data: 9903112
I0626 03:07:33.560961 20786 caffe.cpp:99] Use GPU with device ID 0
I0626 03:07:33.801545 20786 caffe.cpp:107] Starting Optimization
I0626 03:07:33.801645 20786 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:07:33.801668 20786 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:07:33.801862 20786 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:33.801875 20786 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:33.801970 20786 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000592"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000592"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:07:33.802052 20786 net.cpp:67] Creating Layer data
I0626 03:07:33.802065 20786 net.cpp:358] data -> data
I0626 03:07:33.802083 20786 net.cpp:96] Setting up data
I0626 03:07:33.802117 20786 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:33.882556 20786 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:33.882784 20786 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:33.882822 20786 net.cpp:67] Creating Layer label
I0626 03:07:33.882843 20786 net.cpp:358] label -> label
I0626 03:07:33.882874 20786 net.cpp:96] Setting up label
I0626 03:07:33.882892 20786 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:33.991322 20786 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:33.991456 20786 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:33.991509 20786 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:33.991528 20786 net.cpp:396] label_label_0_split <- label
I0626 03:07:33.991567 20786 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:33.991596 20786 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:33.991619 20786 net.cpp:96] Setting up label_label_0_split
I0626 03:07:33.991647 20786 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:33.991665 20786 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:33.991688 20786 net.cpp:67] Creating Layer inner1
I0626 03:07:33.991704 20786 net.cpp:396] inner1 <- data
I0626 03:07:33.991724 20786 net.cpp:358] inner1 -> inner1
I0626 03:07:33.991749 20786 net.cpp:96] Setting up inner1
I0626 03:07:34.021436 20786 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:34.021499 20786 net.cpp:67] Creating Layer inner1relu
I0626 03:07:34.021510 20786 net.cpp:396] inner1relu <- inner1
I0626 03:07:34.021522 20786 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:34.021533 20786 net.cpp:96] Setting up inner1relu
I0626 03:07:34.021548 20786 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:34.021559 20786 net.cpp:67] Creating Layer inner2
I0626 03:07:34.021567 20786 net.cpp:396] inner2 <- inner1
I0626 03:07:34.021576 20786 net.cpp:358] inner2 -> inner2
I0626 03:07:34.021587 20786 net.cpp:96] Setting up inner2
I0626 03:07:34.024582 20786 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:34.024600 20786 net.cpp:67] Creating Layer inner2relu
I0626 03:07:34.024608 20786 net.cpp:396] inner2relu <- inner2
I0626 03:07:34.024617 20786 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:34.024627 20786 net.cpp:96] Setting up inner2relu
I0626 03:07:34.024636 20786 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:34.024644 20786 net.cpp:67] Creating Layer inner3
I0626 03:07:34.024652 20786 net.cpp:396] inner3 <- inner2
I0626 03:07:34.024662 20786 net.cpp:358] inner3 -> inner3
I0626 03:07:34.024672 20786 net.cpp:96] Setting up inner3
I0626 03:07:34.036394 20786 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:34.036427 20786 net.cpp:67] Creating Layer inner3relu
I0626 03:07:34.036435 20786 net.cpp:396] inner3relu <- inner3
I0626 03:07:34.036458 20786 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:34.036468 20786 net.cpp:96] Setting up inner3relu
I0626 03:07:34.036476 20786 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:34.036486 20786 net.cpp:67] Creating Layer output
I0626 03:07:34.036494 20786 net.cpp:396] output <- inner3
I0626 03:07:34.036504 20786 net.cpp:358] output -> output
I0626 03:07:34.036514 20786 net.cpp:96] Setting up output
I0626 03:07:34.037714 20786 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:34.037732 20786 net.cpp:67] Creating Layer loss
I0626 03:07:34.037740 20786 net.cpp:396] loss <- output
I0626 03:07:34.037750 20786 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:34.037760 20786 net.cpp:358] loss -> loss
I0626 03:07:34.037772 20786 net.cpp:358] loss -> std
I0626 03:07:34.037782 20786 net.cpp:358] loss -> ind
I0626 03:07:34.037793 20786 net.cpp:358] loss -> proba
I0626 03:07:34.037803 20786 net.cpp:96] Setting up loss
I0626 03:07:34.037816 20786 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:34.037829 20786 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:34.037837 20786 net.cpp:109]     with loss weight 1
I0626 03:07:34.037865 20786 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:34.037873 20786 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:34.037881 20786 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:34.037894 20786 net.cpp:67] Creating Layer silence
I0626 03:07:34.037902 20786 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:34.037911 20786 net.cpp:396] silence <- ind
I0626 03:07:34.037919 20786 net.cpp:396] silence <- proba
I0626 03:07:34.037928 20786 net.cpp:96] Setting up silence
I0626 03:07:34.037948 20786 net.cpp:172] silence does not need backward computation.
I0626 03:07:34.037956 20786 net.cpp:170] loss needs backward computation.
I0626 03:07:34.037964 20786 net.cpp:170] output needs backward computation.
I0626 03:07:34.037972 20786 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:34.037979 20786 net.cpp:170] inner3 needs backward computation.
I0626 03:07:34.037987 20786 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:34.037994 20786 net.cpp:170] inner2 needs backward computation.
I0626 03:07:34.038002 20786 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:34.038009 20786 net.cpp:170] inner1 needs backward computation.
I0626 03:07:34.038017 20786 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:34.038024 20786 net.cpp:172] label does not need backward computation.
I0626 03:07:34.038033 20786 net.cpp:172] data does not need backward computation.
I0626 03:07:34.038039 20786 net.cpp:208] This network produces output loss
I0626 03:07:34.038048 20786 net.cpp:208] This network produces output std
I0626 03:07:34.038061 20786 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:34.038071 20786 net.cpp:219] Network initialization done.
I0626 03:07:34.038084 20786 net.cpp:220] Memory required for data: 9903112
I0626 03:07:34.038123 20786 solver.cpp:41] Solver scaffolding done.
I0626 03:07:34.038133 20786 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:07:34.048961 20786 solver.cpp:160] Solving net
I0626 03:07:34.070546 20786 solver.cpp:192] Iteration 0, loss = 0.118172
I0626 03:07:34.070600 20786 solver.cpp:207]     Train net output #0: loss = 0.118172 (* 1 = 0.118172 loss)
I0626 03:07:34.070611 20786 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:34.070627 20786 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:07:34.168702 20786 solver.cpp:192] Iteration 10, loss = 0.112439
I0626 03:07:34.168774 20786 solver.cpp:207]     Train net output #0: loss = 0.112439 (* 1 = 0.112439 loss)
I0626 03:07:34.168792 20786 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:34.168808 20786 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:07:34.276476 20786 solver.cpp:192] Iteration 20, loss = 0.108787
I0626 03:07:34.276552 20786 solver.cpp:207]     Train net output #0: loss = 0.108787 (* 1 = 0.108787 loss)
I0626 03:07:34.276589 20786 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:34.276608 20786 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:07:34.390169 20786 solver.cpp:192] Iteration 30, loss = 0.112562
I0626 03:07:34.390242 20786 solver.cpp:207]     Train net output #0: loss = 0.112562 (* 1 = 0.112562 loss)
I0626 03:07:34.390260 20786 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:34.390276 20786 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:07:34.496789 20786 solver.cpp:192] Iteration 40, loss = 0.106628
I0626 03:07:34.496850 20786 solver.cpp:207]     Train net output #0: loss = 0.106628 (* 1 = 0.106628 loss)
I0626 03:07:34.496861 20786 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:34.496872 20786 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:07:34.586683 20786 solver.cpp:192] Iteration 50, loss = 0.115625
I0626 03:07:34.586762 20786 solver.cpp:207]     Train net output #0: loss = 0.115625 (* 1 = 0.115625 loss)
I0626 03:07:34.586782 20786 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:34.586800 20786 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:07:34.695044 20786 solver.cpp:192] Iteration 60, loss = 0.106298
I0626 03:07:34.695116 20786 solver.cpp:207]     Train net output #0: loss = 0.106298 (* 1 = 0.106298 loss)
I0626 03:07:34.695134 20786 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:34.695152 20786 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:07:34.810319 20786 solver.cpp:192] Iteration 70, loss = 0.0996527
I0626 03:07:34.810401 20786 solver.cpp:207]     Train net output #0: loss = 0.0996527 (* 1 = 0.0996527 loss)
I0626 03:07:34.810447 20786 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:34.810468 20786 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:07:34.918546 20786 solver.cpp:192] Iteration 80, loss = 0.114644
I0626 03:07:34.918608 20786 solver.cpp:207]     Train net output #0: loss = 0.114644 (* 1 = 0.114644 loss)
I0626 03:07:34.918620 20786 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:34.918632 20786 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:07:35.029739 20786 solver.cpp:192] Iteration 90, loss = 0.109949
I0626 03:07:35.029814 20786 solver.cpp:207]     Train net output #0: loss = 0.109949 (* 1 = 0.109949 loss)
I0626 03:07:35.029834 20786 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:35.029850 20786 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:07:35.138720 20786 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:07:35.179778 20786 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:07:35.207286 20786 solver.cpp:229] Iteration 100, loss = 0.108602
I0626 03:07:35.207332 20786 solver.cpp:234] Optimization Done.
I0626 03:07:35.207341 20786 caffe.cpp:121] Optimization Done.
I0626 03:07:35.274526  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:35.274557  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:35.274672  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:07:35.274757  8898 net.cpp:67] Creating Layer data
I0626 03:07:35.274770  8898 net.cpp:358] data -> data
I0626 03:07:35.274785  8898 net.cpp:96] Setting up data
I0626 03:07:35.274796  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:35.500458  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:35.502225  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:35.502288  8898 net.cpp:67] Creating Layer label
I0626 03:07:35.502312  8898 net.cpp:358] label -> label
I0626 03:07:35.502341  8898 net.cpp:96] Setting up label
I0626 03:07:35.502360  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:35.584203  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:35.584306  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:35.584345  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:35.584372  8898 net.cpp:396] label_label_0_split <- label
I0626 03:07:35.584395  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:35.584421  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:35.584442  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:07:35.584465  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:35.584480  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:35.584499  8898 net.cpp:67] Creating Layer inner1
I0626 03:07:35.584516  8898 net.cpp:396] inner1 <- data
I0626 03:07:35.584534  8898 net.cpp:358] inner1 -> inner1
I0626 03:07:35.584561  8898 net.cpp:96] Setting up inner1
I0626 03:07:35.613121  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:35.613173  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:07:35.613183  8898 net.cpp:396] inner1relu <- inner1
I0626 03:07:35.613193  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:35.613204  8898 net.cpp:96] Setting up inner1relu
I0626 03:07:35.613212  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:35.613227  8898 net.cpp:67] Creating Layer inner2
I0626 03:07:35.613235  8898 net.cpp:396] inner2 <- inner1
I0626 03:07:35.613245  8898 net.cpp:358] inner2 -> inner2
I0626 03:07:35.613255  8898 net.cpp:96] Setting up inner2
I0626 03:07:35.616164  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:35.616181  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:07:35.616205  8898 net.cpp:396] inner2relu <- inner2
I0626 03:07:35.616215  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:35.616225  8898 net.cpp:96] Setting up inner2relu
I0626 03:07:35.616232  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:35.616241  8898 net.cpp:67] Creating Layer inner3
I0626 03:07:35.616248  8898 net.cpp:396] inner3 <- inner2
I0626 03:07:35.616258  8898 net.cpp:358] inner3 -> inner3
I0626 03:07:35.616267  8898 net.cpp:96] Setting up inner3
I0626 03:07:35.627921  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:35.627959  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:07:35.627967  8898 net.cpp:396] inner3relu <- inner3
I0626 03:07:35.627979  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:35.627988  8898 net.cpp:96] Setting up inner3relu
I0626 03:07:35.627996  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:35.628006  8898 net.cpp:67] Creating Layer output
I0626 03:07:35.628013  8898 net.cpp:396] output <- inner3
I0626 03:07:35.628023  8898 net.cpp:358] output -> output
I0626 03:07:35.628033  8898 net.cpp:96] Setting up output
I0626 03:07:35.629194  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:35.629211  8898 net.cpp:67] Creating Layer loss
I0626 03:07:35.629220  8898 net.cpp:396] loss <- output
I0626 03:07:35.629227  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:35.629237  8898 net.cpp:358] loss -> loss
I0626 03:07:35.629251  8898 net.cpp:358] loss -> std
I0626 03:07:35.629261  8898 net.cpp:358] loss -> ind
I0626 03:07:35.629271  8898 net.cpp:358] loss -> proba
I0626 03:07:35.629281  8898 net.cpp:96] Setting up loss
I0626 03:07:35.629289  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:35.629303  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:35.629312  8898 net.cpp:109]     with loss weight 1
I0626 03:07:35.629326  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:35.629333  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:35.629341  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:35.629354  8898 net.cpp:67] Creating Layer silence
I0626 03:07:35.629361  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:35.629370  8898 net.cpp:396] silence <- ind
I0626 03:07:35.629379  8898 net.cpp:396] silence <- proba
I0626 03:07:35.629386  8898 net.cpp:96] Setting up silence
I0626 03:07:35.629395  8898 net.cpp:172] silence does not need backward computation.
I0626 03:07:35.629401  8898 net.cpp:170] loss needs backward computation.
I0626 03:07:35.629410  8898 net.cpp:170] output needs backward computation.
I0626 03:07:35.629416  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:35.629425  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:07:35.629431  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:35.629438  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:07:35.629446  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:35.629453  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:07:35.629460  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:35.629468  8898 net.cpp:172] label does not need backward computation.
I0626 03:07:35.629475  8898 net.cpp:172] data does not need backward computation.
I0626 03:07:35.629482  8898 net.cpp:208] This network produces output loss
I0626 03:07:35.629490  8898 net.cpp:208] This network produces output std
I0626 03:07:35.629505  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:35.629515  8898 net.cpp:219] Network initialization done.
I0626 03:07:35.629523  8898 net.cpp:220] Memory required for data: 9903112
I0626 03:07:37.065124 21017 caffe.cpp:99] Use GPU with device ID 0
I0626 03:07:37.306095 21017 caffe.cpp:107] Starting Optimization
I0626 03:07:37.306190 21017 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:07:37.306224 21017 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:07:37.306417 21017 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:37.306430 21017 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:37.306521 21017 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000815"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000815"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:07:37.306594 21017 net.cpp:67] Creating Layer data
I0626 03:07:37.306607 21017 net.cpp:358] data -> data
I0626 03:07:37.306623 21017 net.cpp:96] Setting up data
I0626 03:07:37.306656 21017 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:37.394475 21017 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:37.394659 21017 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:37.394686 21017 net.cpp:67] Creating Layer label
I0626 03:07:37.394701 21017 net.cpp:358] label -> label
I0626 03:07:37.394722 21017 net.cpp:96] Setting up label
I0626 03:07:37.394737 21017 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:37.520118 21017 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:37.520251 21017 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:37.520287 21017 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:37.520318 21017 net.cpp:396] label_label_0_split <- label
I0626 03:07:37.520356 21017 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:37.520385 21017 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:37.520409 21017 net.cpp:96] Setting up label_label_0_split
I0626 03:07:37.520438 21017 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:37.520457 21017 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:37.520479 21017 net.cpp:67] Creating Layer inner1
I0626 03:07:37.520496 21017 net.cpp:396] inner1 <- data
I0626 03:07:37.520519 21017 net.cpp:358] inner1 -> inner1
I0626 03:07:37.520543 21017 net.cpp:96] Setting up inner1
I0626 03:07:37.549507 21017 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:37.549563 21017 net.cpp:67] Creating Layer inner1relu
I0626 03:07:37.549573 21017 net.cpp:396] inner1relu <- inner1
I0626 03:07:37.549584 21017 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:37.549597 21017 net.cpp:96] Setting up inner1relu
I0626 03:07:37.549612 21017 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:37.549623 21017 net.cpp:67] Creating Layer inner2
I0626 03:07:37.549630 21017 net.cpp:396] inner2 <- inner1
I0626 03:07:37.549640 21017 net.cpp:358] inner2 -> inner2
I0626 03:07:37.549651 21017 net.cpp:96] Setting up inner2
I0626 03:07:37.552590 21017 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:37.552608 21017 net.cpp:67] Creating Layer inner2relu
I0626 03:07:37.552616 21017 net.cpp:396] inner2relu <- inner2
I0626 03:07:37.552625 21017 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:37.552635 21017 net.cpp:96] Setting up inner2relu
I0626 03:07:37.552644 21017 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:37.552654 21017 net.cpp:67] Creating Layer inner3
I0626 03:07:37.552661 21017 net.cpp:396] inner3 <- inner2
I0626 03:07:37.552671 21017 net.cpp:358] inner3 -> inner3
I0626 03:07:37.552680 21017 net.cpp:96] Setting up inner3
I0626 03:07:37.564350 21017 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:37.564383 21017 net.cpp:67] Creating Layer inner3relu
I0626 03:07:37.564393 21017 net.cpp:396] inner3relu <- inner3
I0626 03:07:37.564402 21017 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:37.564414 21017 net.cpp:96] Setting up inner3relu
I0626 03:07:37.564421 21017 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:37.564431 21017 net.cpp:67] Creating Layer output
I0626 03:07:37.564440 21017 net.cpp:396] output <- inner3
I0626 03:07:37.564450 21017 net.cpp:358] output -> output
I0626 03:07:37.564460 21017 net.cpp:96] Setting up output
I0626 03:07:37.565645 21017 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:37.565663 21017 net.cpp:67] Creating Layer loss
I0626 03:07:37.565671 21017 net.cpp:396] loss <- output
I0626 03:07:37.565680 21017 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:37.565691 21017 net.cpp:358] loss -> loss
I0626 03:07:37.565703 21017 net.cpp:358] loss -> std
I0626 03:07:37.565714 21017 net.cpp:358] loss -> ind
I0626 03:07:37.565726 21017 net.cpp:358] loss -> proba
I0626 03:07:37.565735 21017 net.cpp:96] Setting up loss
I0626 03:07:37.565748 21017 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:37.565762 21017 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:37.565770 21017 net.cpp:109]     with loss weight 1
I0626 03:07:37.565804 21017 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:37.565811 21017 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:37.565819 21017 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:37.565832 21017 net.cpp:67] Creating Layer silence
I0626 03:07:37.565841 21017 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:37.565850 21017 net.cpp:396] silence <- ind
I0626 03:07:37.565860 21017 net.cpp:396] silence <- proba
I0626 03:07:37.565867 21017 net.cpp:96] Setting up silence
I0626 03:07:37.565896 21017 net.cpp:172] silence does not need backward computation.
I0626 03:07:37.565903 21017 net.cpp:170] loss needs backward computation.
I0626 03:07:37.565912 21017 net.cpp:170] output needs backward computation.
I0626 03:07:37.565923 21017 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:37.565932 21017 net.cpp:170] inner3 needs backward computation.
I0626 03:07:37.565938 21017 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:37.565946 21017 net.cpp:170] inner2 needs backward computation.
I0626 03:07:37.565954 21017 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:37.565961 21017 net.cpp:170] inner1 needs backward computation.
I0626 03:07:37.565969 21017 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:37.565977 21017 net.cpp:172] label does not need backward computation.
I0626 03:07:37.565984 21017 net.cpp:172] data does not need backward computation.
I0626 03:07:37.565992 21017 net.cpp:208] This network produces output loss
I0626 03:07:37.566000 21017 net.cpp:208] This network produces output std
I0626 03:07:37.566015 21017 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:37.566025 21017 net.cpp:219] Network initialization done.
I0626 03:07:37.566036 21017 net.cpp:220] Memory required for data: 9903112
I0626 03:07:37.566076 21017 solver.cpp:41] Solver scaffolding done.
I0626 03:07:37.566085 21017 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:07:37.576789 21017 solver.cpp:160] Solving net
I0626 03:07:37.598670 21017 solver.cpp:192] Iteration 0, loss = 0.11449
I0626 03:07:37.598723 21017 solver.cpp:207]     Train net output #0: loss = 0.11449 (* 1 = 0.11449 loss)
I0626 03:07:37.598736 21017 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:37.598760 21017 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:07:37.703074 21017 solver.cpp:192] Iteration 10, loss = 0.103999
I0626 03:07:37.703133 21017 solver.cpp:207]     Train net output #0: loss = 0.103999 (* 1 = 0.103999 loss)
I0626 03:07:37.703145 21017 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:37.703156 21017 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:07:37.811884 21017 solver.cpp:192] Iteration 20, loss = 0.105081
I0626 03:07:37.811945 21017 solver.cpp:207]     Train net output #0: loss = 0.105081 (* 1 = 0.105081 loss)
I0626 03:07:37.811957 21017 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:37.811969 21017 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:07:37.916684 21017 solver.cpp:192] Iteration 30, loss = 0.103328
I0626 03:07:37.916754 21017 solver.cpp:207]     Train net output #0: loss = 0.103328 (* 1 = 0.103328 loss)
I0626 03:07:37.916766 21017 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:37.916779 21017 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:07:38.019172 21017 solver.cpp:192] Iteration 40, loss = 0.138918
I0626 03:07:38.019233 21017 solver.cpp:207]     Train net output #0: loss = 0.138918 (* 1 = 0.138918 loss)
I0626 03:07:38.019245 21017 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:38.019258 21017 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:07:38.136147 21017 solver.cpp:192] Iteration 50, loss = 0.103109
I0626 03:07:38.136209 21017 solver.cpp:207]     Train net output #0: loss = 0.103109 (* 1 = 0.103109 loss)
I0626 03:07:38.136221 21017 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:38.136234 21017 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:07:38.232336 21017 solver.cpp:192] Iteration 60, loss = 0.099557
I0626 03:07:38.232395 21017 solver.cpp:207]     Train net output #0: loss = 0.099557 (* 1 = 0.099557 loss)
I0626 03:07:38.232406 21017 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:38.232419 21017 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:07:38.333173 21017 solver.cpp:192] Iteration 70, loss = 0.116937
I0626 03:07:38.333232 21017 solver.cpp:207]     Train net output #0: loss = 0.116937 (* 1 = 0.116937 loss)
I0626 03:07:38.333262 21017 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:38.333274 21017 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:07:38.438370 21017 solver.cpp:192] Iteration 80, loss = 0.111391
I0626 03:07:38.438442 21017 solver.cpp:207]     Train net output #0: loss = 0.111391 (* 1 = 0.111391 loss)
I0626 03:07:38.438454 21017 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:38.438467 21017 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:07:38.544101 21017 solver.cpp:192] Iteration 90, loss = 0.130703
I0626 03:07:38.544170 21017 solver.cpp:207]     Train net output #0: loss = 0.130703 (* 1 = 0.130703 loss)
I0626 03:07:38.544186 21017 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:38.544201 21017 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:07:38.655741 21017 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:07:38.696810 21017 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:07:38.724550 21017 solver.cpp:229] Iteration 100, loss = 0.136009
I0626 03:07:38.724594 21017 solver.cpp:234] Optimization Done.
I0626 03:07:38.724603 21017 caffe.cpp:121] Optimization Done.
I0626 03:07:38.791406  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:38.791437  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:38.791576  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:07:38.791659  8898 net.cpp:67] Creating Layer data
I0626 03:07:38.791673  8898 net.cpp:358] data -> data
I0626 03:07:38.791688  8898 net.cpp:96] Setting up data
I0626 03:07:38.791698  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:39.021018  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:39.022792  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:39.022851  8898 net.cpp:67] Creating Layer label
I0626 03:07:39.022876  8898 net.cpp:358] label -> label
I0626 03:07:39.022904  8898 net.cpp:96] Setting up label
I0626 03:07:39.022923  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:39.122233  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:39.122335  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:39.122370  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:39.122390  8898 net.cpp:396] label_label_0_split <- label
I0626 03:07:39.122413  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:39.122440  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:39.122462  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:07:39.122481  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:39.122498  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:39.122519  8898 net.cpp:67] Creating Layer inner1
I0626 03:07:39.122535  8898 net.cpp:396] inner1 <- data
I0626 03:07:39.122556  8898 net.cpp:358] inner1 -> inner1
I0626 03:07:39.122579  8898 net.cpp:96] Setting up inner1
I0626 03:07:39.150105  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:39.150157  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:07:39.150167  8898 net.cpp:396] inner1relu <- inner1
I0626 03:07:39.150178  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:39.150189  8898 net.cpp:96] Setting up inner1relu
I0626 03:07:39.150197  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:39.150208  8898 net.cpp:67] Creating Layer inner2
I0626 03:07:39.150215  8898 net.cpp:396] inner2 <- inner1
I0626 03:07:39.150225  8898 net.cpp:358] inner2 -> inner2
I0626 03:07:39.150235  8898 net.cpp:96] Setting up inner2
I0626 03:07:39.153174  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:39.153192  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:07:39.153200  8898 net.cpp:396] inner2relu <- inner2
I0626 03:07:39.153209  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:39.153218  8898 net.cpp:96] Setting up inner2relu
I0626 03:07:39.153226  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:39.153236  8898 net.cpp:67] Creating Layer inner3
I0626 03:07:39.153251  8898 net.cpp:396] inner3 <- inner2
I0626 03:07:39.153260  8898 net.cpp:358] inner3 -> inner3
I0626 03:07:39.153270  8898 net.cpp:96] Setting up inner3
I0626 03:07:39.164942  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:39.164980  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:07:39.164989  8898 net.cpp:396] inner3relu <- inner3
I0626 03:07:39.164999  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:39.165009  8898 net.cpp:96] Setting up inner3relu
I0626 03:07:39.165016  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:39.165027  8898 net.cpp:67] Creating Layer output
I0626 03:07:39.165035  8898 net.cpp:396] output <- inner3
I0626 03:07:39.165045  8898 net.cpp:358] output -> output
I0626 03:07:39.165055  8898 net.cpp:96] Setting up output
I0626 03:07:39.166215  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:39.166232  8898 net.cpp:67] Creating Layer loss
I0626 03:07:39.166240  8898 net.cpp:396] loss <- output
I0626 03:07:39.166249  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:39.166258  8898 net.cpp:358] loss -> loss
I0626 03:07:39.166271  8898 net.cpp:358] loss -> std
I0626 03:07:39.166281  8898 net.cpp:358] loss -> ind
I0626 03:07:39.166292  8898 net.cpp:358] loss -> proba
I0626 03:07:39.166301  8898 net.cpp:96] Setting up loss
I0626 03:07:39.166309  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:39.166326  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:39.166333  8898 net.cpp:109]     with loss weight 1
I0626 03:07:39.166349  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:39.166369  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:39.166378  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:39.166391  8898 net.cpp:67] Creating Layer silence
I0626 03:07:39.166400  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:39.166409  8898 net.cpp:396] silence <- ind
I0626 03:07:39.166417  8898 net.cpp:396] silence <- proba
I0626 03:07:39.166425  8898 net.cpp:96] Setting up silence
I0626 03:07:39.166432  8898 net.cpp:172] silence does not need backward computation.
I0626 03:07:39.166440  8898 net.cpp:170] loss needs backward computation.
I0626 03:07:39.166447  8898 net.cpp:170] output needs backward computation.
I0626 03:07:39.166455  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:39.166462  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:07:39.166469  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:39.166477  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:07:39.166484  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:39.166491  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:07:39.166498  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:39.166507  8898 net.cpp:172] label does not need backward computation.
I0626 03:07:39.166513  8898 net.cpp:172] data does not need backward computation.
I0626 03:07:39.166520  8898 net.cpp:208] This network produces output loss
I0626 03:07:39.166528  8898 net.cpp:208] This network produces output std
I0626 03:07:39.166543  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:39.166553  8898 net.cpp:219] Network initialization done.
I0626 03:07:39.166560  8898 net.cpp:220] Memory required for data: 9903112
I0626 03:07:40.634344 21248 caffe.cpp:99] Use GPU with device ID 0
I0626 03:07:40.875881 21248 caffe.cpp:107] Starting Optimization
I0626 03:07:40.875973 21248 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:07:40.875998 21248 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:07:40.876194 21248 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:40.876207 21248 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:40.876300 21248 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00001038"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00001038"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:07:40.876386 21248 net.cpp:67] Creating Layer data
I0626 03:07:40.876406 21248 net.cpp:358] data -> data
I0626 03:07:40.876425 21248 net.cpp:96] Setting up data
I0626 03:07:40.876457 21248 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:40.957619 21248 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:40.957803 21248 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:40.957840 21248 net.cpp:67] Creating Layer label
I0626 03:07:40.957864 21248 net.cpp:358] label -> label
I0626 03:07:40.957895 21248 net.cpp:96] Setting up label
I0626 03:07:40.957913 21248 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:41.066407 21248 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:41.066540 21248 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:41.066573 21248 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:41.066593 21248 net.cpp:396] label_label_0_split <- label
I0626 03:07:41.066629 21248 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:41.066659 21248 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:41.066681 21248 net.cpp:96] Setting up label_label_0_split
I0626 03:07:41.066709 21248 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:41.066727 21248 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:41.066751 21248 net.cpp:67] Creating Layer inner1
I0626 03:07:41.066767 21248 net.cpp:396] inner1 <- data
I0626 03:07:41.066789 21248 net.cpp:358] inner1 -> inner1
I0626 03:07:41.066813 21248 net.cpp:96] Setting up inner1
I0626 03:07:41.096237 21248 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:41.096297 21248 net.cpp:67] Creating Layer inner1relu
I0626 03:07:41.096307 21248 net.cpp:396] inner1relu <- inner1
I0626 03:07:41.096320 21248 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:41.096333 21248 net.cpp:96] Setting up inner1relu
I0626 03:07:41.096348 21248 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:41.096359 21248 net.cpp:67] Creating Layer inner2
I0626 03:07:41.096367 21248 net.cpp:396] inner2 <- inner1
I0626 03:07:41.096377 21248 net.cpp:358] inner2 -> inner2
I0626 03:07:41.096388 21248 net.cpp:96] Setting up inner2
I0626 03:07:41.099498 21248 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:41.099516 21248 net.cpp:67] Creating Layer inner2relu
I0626 03:07:41.099525 21248 net.cpp:396] inner2relu <- inner2
I0626 03:07:41.099534 21248 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:41.099545 21248 net.cpp:96] Setting up inner2relu
I0626 03:07:41.099552 21248 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:41.099562 21248 net.cpp:67] Creating Layer inner3
I0626 03:07:41.099570 21248 net.cpp:396] inner3 <- inner2
I0626 03:07:41.099580 21248 net.cpp:358] inner3 -> inner3
I0626 03:07:41.099589 21248 net.cpp:96] Setting up inner3
I0626 03:07:41.111681 21248 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:41.111719 21248 net.cpp:67] Creating Layer inner3relu
I0626 03:07:41.111728 21248 net.cpp:396] inner3relu <- inner3
I0626 03:07:41.111738 21248 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:41.111749 21248 net.cpp:96] Setting up inner3relu
I0626 03:07:41.111757 21248 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:41.111768 21248 net.cpp:67] Creating Layer output
I0626 03:07:41.111776 21248 net.cpp:396] output <- inner3
I0626 03:07:41.111786 21248 net.cpp:358] output -> output
I0626 03:07:41.111796 21248 net.cpp:96] Setting up output
I0626 03:07:41.113013 21248 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:41.113031 21248 net.cpp:67] Creating Layer loss
I0626 03:07:41.113040 21248 net.cpp:396] loss <- output
I0626 03:07:41.113049 21248 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:41.113059 21248 net.cpp:358] loss -> loss
I0626 03:07:41.113072 21248 net.cpp:358] loss -> std
I0626 03:07:41.113085 21248 net.cpp:358] loss -> ind
I0626 03:07:41.113095 21248 net.cpp:358] loss -> proba
I0626 03:07:41.113106 21248 net.cpp:96] Setting up loss
I0626 03:07:41.113118 21248 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:41.113132 21248 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:41.113140 21248 net.cpp:109]     with loss weight 1
I0626 03:07:41.113176 21248 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:41.113184 21248 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:41.113193 21248 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:41.113205 21248 net.cpp:67] Creating Layer silence
I0626 03:07:41.113214 21248 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:41.113224 21248 net.cpp:396] silence <- ind
I0626 03:07:41.113231 21248 net.cpp:396] silence <- proba
I0626 03:07:41.113240 21248 net.cpp:96] Setting up silence
I0626 03:07:41.113265 21248 net.cpp:172] silence does not need backward computation.
I0626 03:07:41.113272 21248 net.cpp:170] loss needs backward computation.
I0626 03:07:41.113281 21248 net.cpp:170] output needs backward computation.
I0626 03:07:41.113289 21248 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:41.113296 21248 net.cpp:170] inner3 needs backward computation.
I0626 03:07:41.113304 21248 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:41.113312 21248 net.cpp:170] inner2 needs backward computation.
I0626 03:07:41.113320 21248 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:41.113327 21248 net.cpp:170] inner1 needs backward computation.
I0626 03:07:41.113335 21248 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:41.113343 21248 net.cpp:172] label does not need backward computation.
I0626 03:07:41.113351 21248 net.cpp:172] data does not need backward computation.
I0626 03:07:41.113359 21248 net.cpp:208] This network produces output loss
I0626 03:07:41.113368 21248 net.cpp:208] This network produces output std
I0626 03:07:41.113381 21248 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:41.113392 21248 net.cpp:219] Network initialization done.
I0626 03:07:41.113404 21248 net.cpp:220] Memory required for data: 9903112
I0626 03:07:41.113446 21248 solver.cpp:41] Solver scaffolding done.
I0626 03:07:41.113456 21248 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:07:41.124719 21248 solver.cpp:160] Solving net
I0626 03:07:41.146387 21248 solver.cpp:192] Iteration 0, loss = 0.12979
I0626 03:07:41.146440 21248 solver.cpp:207]     Train net output #0: loss = 0.12979 (* 1 = 0.12979 loss)
I0626 03:07:41.146451 21248 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:41.146474 21248 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:07:41.246127 21248 solver.cpp:192] Iteration 10, loss = 0.104754
I0626 03:07:41.246206 21248 solver.cpp:207]     Train net output #0: loss = 0.104754 (* 1 = 0.104754 loss)
I0626 03:07:41.246227 21248 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:41.246244 21248 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:07:41.358343 21248 solver.cpp:192] Iteration 20, loss = 0.104222
I0626 03:07:41.358412 21248 solver.cpp:207]     Train net output #0: loss = 0.104222 (* 1 = 0.104222 loss)
I0626 03:07:41.358428 21248 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:41.358443 21248 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:07:41.456039 21248 solver.cpp:192] Iteration 30, loss = 0.15818
I0626 03:07:41.456117 21248 solver.cpp:207]     Train net output #0: loss = 0.15818 (* 1 = 0.15818 loss)
I0626 03:07:41.456137 21248 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:41.456156 21248 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:07:41.568058 21248 solver.cpp:192] Iteration 40, loss = 0.114701
I0626 03:07:41.568115 21248 solver.cpp:207]     Train net output #0: loss = 0.114701 (* 1 = 0.114701 loss)
I0626 03:07:41.568126 21248 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:41.568138 21248 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:07:41.676883 21248 solver.cpp:192] Iteration 50, loss = 0.120435
I0626 03:07:41.676944 21248 solver.cpp:207]     Train net output #0: loss = 0.120435 (* 1 = 0.120435 loss)
I0626 03:07:41.676956 21248 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:41.676967 21248 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:07:41.764752 21248 solver.cpp:192] Iteration 60, loss = 0.145502
I0626 03:07:41.764807 21248 solver.cpp:207]     Train net output #0: loss = 0.145502 (* 1 = 0.145502 loss)
I0626 03:07:41.764819 21248 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:41.764830 21248 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:07:41.862752 21248 solver.cpp:192] Iteration 70, loss = 0.141178
I0626 03:07:41.862813 21248 solver.cpp:207]     Train net output #0: loss = 0.141178 (* 1 = 0.141178 loss)
I0626 03:07:41.862844 21248 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:41.862856 21248 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:07:41.965824 21248 solver.cpp:192] Iteration 80, loss = 0.122404
I0626 03:07:41.965896 21248 solver.cpp:207]     Train net output #0: loss = 0.122404 (* 1 = 0.122404 loss)
I0626 03:07:41.965914 21248 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:41.965929 21248 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:07:42.070179 21248 solver.cpp:192] Iteration 90, loss = 0.103065
I0626 03:07:42.070252 21248 solver.cpp:207]     Train net output #0: loss = 0.103065 (* 1 = 0.103065 loss)
I0626 03:07:42.070271 21248 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:42.070288 21248 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:07:42.186858 21248 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:07:42.227705 21248 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:07:42.256011 21248 solver.cpp:229] Iteration 100, loss = 0.0885642
I0626 03:07:42.256057 21248 solver.cpp:234] Optimization Done.
I0626 03:07:42.256067 21248 caffe.cpp:121] Optimization Done.
I0626 03:07:42.324731  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:42.324762  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:42.324875  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:07:42.324965  8898 net.cpp:67] Creating Layer data
I0626 03:07:42.324977  8898 net.cpp:358] data -> data
I0626 03:07:42.324991  8898 net.cpp:96] Setting up data
I0626 03:07:42.325001  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:42.942412  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:42.944219  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:42.944279  8898 net.cpp:67] Creating Layer label
I0626 03:07:42.944303  8898 net.cpp:358] label -> label
I0626 03:07:42.944331  8898 net.cpp:96] Setting up label
I0626 03:07:42.944350  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:43.026132  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:43.026244  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:43.026283  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:43.026304  8898 net.cpp:396] label_label_0_split <- label
I0626 03:07:43.026327  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:43.026357  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:43.026379  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:07:43.026398  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:43.026414  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:43.026437  8898 net.cpp:67] Creating Layer inner1
I0626 03:07:43.026453  8898 net.cpp:396] inner1 <- data
I0626 03:07:43.026473  8898 net.cpp:358] inner1 -> inner1
I0626 03:07:43.026496  8898 net.cpp:96] Setting up inner1
I0626 03:07:43.052558  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:43.052613  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:07:43.052623  8898 net.cpp:396] inner1relu <- inner1
I0626 03:07:43.052634  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:43.052645  8898 net.cpp:96] Setting up inner1relu
I0626 03:07:43.052654  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:43.052664  8898 net.cpp:67] Creating Layer inner2
I0626 03:07:43.052672  8898 net.cpp:396] inner2 <- inner1
I0626 03:07:43.052681  8898 net.cpp:358] inner2 -> inner2
I0626 03:07:43.052706  8898 net.cpp:96] Setting up inner2
I0626 03:07:43.055613  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:43.055630  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:07:43.055639  8898 net.cpp:396] inner2relu <- inner2
I0626 03:07:43.055647  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:43.055656  8898 net.cpp:96] Setting up inner2relu
I0626 03:07:43.055665  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:43.055675  8898 net.cpp:67] Creating Layer inner3
I0626 03:07:43.055681  8898 net.cpp:396] inner3 <- inner2
I0626 03:07:43.055692  8898 net.cpp:358] inner3 -> inner3
I0626 03:07:43.055702  8898 net.cpp:96] Setting up inner3
I0626 03:07:43.067270  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:43.067307  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:07:43.067317  8898 net.cpp:396] inner3relu <- inner3
I0626 03:07:43.067328  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:43.067339  8898 net.cpp:96] Setting up inner3relu
I0626 03:07:43.067348  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:43.067358  8898 net.cpp:67] Creating Layer output
I0626 03:07:43.067365  8898 net.cpp:396] output <- inner3
I0626 03:07:43.067374  8898 net.cpp:358] output -> output
I0626 03:07:43.067385  8898 net.cpp:96] Setting up output
I0626 03:07:43.068558  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:43.068576  8898 net.cpp:67] Creating Layer loss
I0626 03:07:43.068584  8898 net.cpp:396] loss <- output
I0626 03:07:43.068593  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:43.068604  8898 net.cpp:358] loss -> loss
I0626 03:07:43.068617  8898 net.cpp:358] loss -> std
I0626 03:07:43.068627  8898 net.cpp:358] loss -> ind
I0626 03:07:43.068639  8898 net.cpp:358] loss -> proba
I0626 03:07:43.068650  8898 net.cpp:96] Setting up loss
I0626 03:07:43.068657  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:43.068671  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:43.068680  8898 net.cpp:109]     with loss weight 1
I0626 03:07:43.068694  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:43.068702  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:43.068709  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:43.068722  8898 net.cpp:67] Creating Layer silence
I0626 03:07:43.068729  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:43.068738  8898 net.cpp:396] silence <- ind
I0626 03:07:43.068747  8898 net.cpp:396] silence <- proba
I0626 03:07:43.068754  8898 net.cpp:96] Setting up silence
I0626 03:07:43.068763  8898 net.cpp:172] silence does not need backward computation.
I0626 03:07:43.068769  8898 net.cpp:170] loss needs backward computation.
I0626 03:07:43.068778  8898 net.cpp:170] output needs backward computation.
I0626 03:07:43.068785  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:43.068792  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:07:43.068799  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:43.068807  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:07:43.068814  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:43.068821  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:07:43.068830  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:43.068837  8898 net.cpp:172] label does not need backward computation.
I0626 03:07:43.068845  8898 net.cpp:172] data does not need backward computation.
I0626 03:07:43.068851  8898 net.cpp:208] This network produces output loss
I0626 03:07:43.068858  8898 net.cpp:208] This network produces output std
I0626 03:07:43.068872  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:43.068883  8898 net.cpp:219] Network initialization done.
I0626 03:07:43.068892  8898 net.cpp:220] Memory required for data: 9903112
I0626 03:07:44.555692 21479 caffe.cpp:99] Use GPU with device ID 0
I0626 03:07:44.796710 21479 caffe.cpp:107] Starting Optimization
I0626 03:07:44.796809 21479 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:07:44.796845 21479 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:07:44.797045 21479 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:44.797060 21479 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:44.797158 21479 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00001261"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00001261"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:07:44.797238 21479 net.cpp:67] Creating Layer data
I0626 03:07:44.797251 21479 net.cpp:358] data -> data
I0626 03:07:44.797268 21479 net.cpp:96] Setting up data
I0626 03:07:44.797302 21479 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:44.886502 21479 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:44.886688 21479 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:44.886729 21479 net.cpp:67] Creating Layer label
I0626 03:07:44.886750 21479 net.cpp:358] label -> label
I0626 03:07:44.886782 21479 net.cpp:96] Setting up label
I0626 03:07:44.886803 21479 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:45.011879 21479 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:45.012027 21479 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:45.012063 21479 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:45.012082 21479 net.cpp:396] label_label_0_split <- label
I0626 03:07:45.012120 21479 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:45.012151 21479 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:45.012173 21479 net.cpp:96] Setting up label_label_0_split
I0626 03:07:45.012202 21479 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:45.012220 21479 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:45.012243 21479 net.cpp:67] Creating Layer inner1
I0626 03:07:45.012261 21479 net.cpp:396] inner1 <- data
I0626 03:07:45.012284 21479 net.cpp:358] inner1 -> inner1
I0626 03:07:45.012307 21479 net.cpp:96] Setting up inner1
I0626 03:07:45.041390 21479 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:45.041450 21479 net.cpp:67] Creating Layer inner1relu
I0626 03:07:45.041461 21479 net.cpp:396] inner1relu <- inner1
I0626 03:07:45.041473 21479 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:45.041486 21479 net.cpp:96] Setting up inner1relu
I0626 03:07:45.041501 21479 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:45.041512 21479 net.cpp:67] Creating Layer inner2
I0626 03:07:45.041520 21479 net.cpp:396] inner2 <- inner1
I0626 03:07:45.041529 21479 net.cpp:358] inner2 -> inner2
I0626 03:07:45.041540 21479 net.cpp:96] Setting up inner2
I0626 03:07:45.044489 21479 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:45.044507 21479 net.cpp:67] Creating Layer inner2relu
I0626 03:07:45.044515 21479 net.cpp:396] inner2relu <- inner2
I0626 03:07:45.044525 21479 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:45.044534 21479 net.cpp:96] Setting up inner2relu
I0626 03:07:45.044543 21479 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:45.044553 21479 net.cpp:67] Creating Layer inner3
I0626 03:07:45.044560 21479 net.cpp:396] inner3 <- inner2
I0626 03:07:45.044570 21479 net.cpp:358] inner3 -> inner3
I0626 03:07:45.044579 21479 net.cpp:96] Setting up inner3
I0626 03:07:45.056262 21479 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:45.056293 21479 net.cpp:67] Creating Layer inner3relu
I0626 03:07:45.056303 21479 net.cpp:396] inner3relu <- inner3
I0626 03:07:45.056313 21479 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:45.056324 21479 net.cpp:96] Setting up inner3relu
I0626 03:07:45.056331 21479 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:45.056342 21479 net.cpp:67] Creating Layer output
I0626 03:07:45.056350 21479 net.cpp:396] output <- inner3
I0626 03:07:45.056360 21479 net.cpp:358] output -> output
I0626 03:07:45.056370 21479 net.cpp:96] Setting up output
I0626 03:07:45.057555 21479 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:45.057574 21479 net.cpp:67] Creating Layer loss
I0626 03:07:45.057581 21479 net.cpp:396] loss <- output
I0626 03:07:45.057590 21479 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:45.057601 21479 net.cpp:358] loss -> loss
I0626 03:07:45.057613 21479 net.cpp:358] loss -> std
I0626 03:07:45.057624 21479 net.cpp:358] loss -> ind
I0626 03:07:45.057636 21479 net.cpp:358] loss -> proba
I0626 03:07:45.057646 21479 net.cpp:96] Setting up loss
I0626 03:07:45.057658 21479 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:45.057672 21479 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:45.057680 21479 net.cpp:109]     with loss weight 1
I0626 03:07:45.057709 21479 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:45.057718 21479 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:45.057724 21479 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:45.057739 21479 net.cpp:67] Creating Layer silence
I0626 03:07:45.057746 21479 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:45.057755 21479 net.cpp:396] silence <- ind
I0626 03:07:45.057765 21479 net.cpp:396] silence <- proba
I0626 03:07:45.057772 21479 net.cpp:96] Setting up silence
I0626 03:07:45.057798 21479 net.cpp:172] silence does not need backward computation.
I0626 03:07:45.057811 21479 net.cpp:170] loss needs backward computation.
I0626 03:07:45.057818 21479 net.cpp:170] output needs backward computation.
I0626 03:07:45.057826 21479 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:45.057834 21479 net.cpp:170] inner3 needs backward computation.
I0626 03:07:45.057842 21479 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:45.057849 21479 net.cpp:170] inner2 needs backward computation.
I0626 03:07:45.057857 21479 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:45.057865 21479 net.cpp:170] inner1 needs backward computation.
I0626 03:07:45.057873 21479 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:45.057881 21479 net.cpp:172] label does not need backward computation.
I0626 03:07:45.057888 21479 net.cpp:172] data does not need backward computation.
I0626 03:07:45.057896 21479 net.cpp:208] This network produces output loss
I0626 03:07:45.057904 21479 net.cpp:208] This network produces output std
I0626 03:07:45.057919 21479 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:45.057929 21479 net.cpp:219] Network initialization done.
I0626 03:07:45.057941 21479 net.cpp:220] Memory required for data: 9903112
I0626 03:07:45.057981 21479 solver.cpp:41] Solver scaffolding done.
I0626 03:07:45.057991 21479 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:07:45.068869 21479 solver.cpp:160] Solving net
I0626 03:07:45.090579 21479 solver.cpp:192] Iteration 0, loss = 0.103354
I0626 03:07:45.090631 21479 solver.cpp:207]     Train net output #0: loss = 0.103354 (* 1 = 0.103354 loss)
I0626 03:07:45.090644 21479 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:45.090665 21479 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:07:45.193042 21479 solver.cpp:192] Iteration 10, loss = 0.125972
I0626 03:07:45.193114 21479 solver.cpp:207]     Train net output #0: loss = 0.125972 (* 1 = 0.125972 loss)
I0626 03:07:45.193131 21479 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:45.193146 21479 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:07:45.302134 21479 solver.cpp:192] Iteration 20, loss = 0.110278
I0626 03:07:45.302206 21479 solver.cpp:207]     Train net output #0: loss = 0.110278 (* 1 = 0.110278 loss)
I0626 03:07:45.302222 21479 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:45.302238 21479 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:07:45.413105 21479 solver.cpp:192] Iteration 30, loss = 0.128283
I0626 03:07:45.413167 21479 solver.cpp:207]     Train net output #0: loss = 0.128283 (* 1 = 0.128283 loss)
I0626 03:07:45.413179 21479 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:45.413192 21479 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:07:45.522456 21479 solver.cpp:192] Iteration 40, loss = 0.0964763
I0626 03:07:45.522526 21479 solver.cpp:207]     Train net output #0: loss = 0.0964763 (* 1 = 0.0964763 loss)
I0626 03:07:45.522543 21479 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:45.522559 21479 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:07:45.633188 21479 solver.cpp:192] Iteration 50, loss = 0.101151
I0626 03:07:45.633260 21479 solver.cpp:207]     Train net output #0: loss = 0.101151 (* 1 = 0.101151 loss)
I0626 03:07:45.633276 21479 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:45.633292 21479 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:07:45.742619 21479 solver.cpp:192] Iteration 60, loss = 0.120215
I0626 03:07:45.742696 21479 solver.cpp:207]     Train net output #0: loss = 0.120215 (* 1 = 0.120215 loss)
I0626 03:07:45.742714 21479 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:45.742733 21479 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:07:45.851218 21479 solver.cpp:192] Iteration 70, loss = 0.0977623
I0626 03:07:45.851281 21479 solver.cpp:207]     Train net output #0: loss = 0.0977623 (* 1 = 0.0977623 loss)
I0626 03:07:45.851313 21479 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:45.851330 21479 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:07:45.959264 21479 solver.cpp:192] Iteration 80, loss = 0.106641
I0626 03:07:45.959326 21479 solver.cpp:207]     Train net output #0: loss = 0.106641 (* 1 = 0.106641 loss)
I0626 03:07:45.959337 21479 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:45.959348 21479 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:07:46.068933 21479 solver.cpp:192] Iteration 90, loss = 0.100749
I0626 03:07:46.069005 21479 solver.cpp:207]     Train net output #0: loss = 0.100749 (* 1 = 0.100749 loss)
I0626 03:07:46.069021 21479 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:46.069037 21479 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:07:46.181558 21479 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:07:46.223081 21479 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:07:46.251904 21479 solver.cpp:229] Iteration 100, loss = 0.119369
I0626 03:07:46.251950 21479 solver.cpp:234] Optimization Done.
I0626 03:07:46.251960 21479 caffe.cpp:121] Optimization Done.
I0626 03:07:46.320133  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:46.320163  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:46.320283  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:07:46.320364  8898 net.cpp:67] Creating Layer data
I0626 03:07:46.320384  8898 net.cpp:358] data -> data
I0626 03:07:46.320399  8898 net.cpp:96] Setting up data
I0626 03:07:46.320408  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:46.554437  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:46.556221  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:46.556277  8898 net.cpp:67] Creating Layer label
I0626 03:07:46.556299  8898 net.cpp:358] label -> label
I0626 03:07:46.556329  8898 net.cpp:96] Setting up label
I0626 03:07:46.556347  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:46.638124  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:46.638231  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:46.638267  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:46.638285  8898 net.cpp:396] label_label_0_split <- label
I0626 03:07:46.638309  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:46.638340  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:46.638361  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:07:46.638379  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:46.638394  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:46.638415  8898 net.cpp:67] Creating Layer inner1
I0626 03:07:46.638432  8898 net.cpp:396] inner1 <- data
I0626 03:07:46.638450  8898 net.cpp:358] inner1 -> inner1
I0626 03:07:46.638473  8898 net.cpp:96] Setting up inner1
I0626 03:07:46.665951  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:46.666007  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:07:46.666018  8898 net.cpp:396] inner1relu <- inner1
I0626 03:07:46.666028  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:46.666039  8898 net.cpp:96] Setting up inner1relu
I0626 03:07:46.666048  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:46.666059  8898 net.cpp:67] Creating Layer inner2
I0626 03:07:46.666066  8898 net.cpp:396] inner2 <- inner1
I0626 03:07:46.666076  8898 net.cpp:358] inner2 -> inner2
I0626 03:07:46.666086  8898 net.cpp:96] Setting up inner2
I0626 03:07:46.668992  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:46.669009  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:07:46.669018  8898 net.cpp:396] inner2relu <- inner2
I0626 03:07:46.669028  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:46.669037  8898 net.cpp:96] Setting up inner2relu
I0626 03:07:46.669044  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:46.669054  8898 net.cpp:67] Creating Layer inner3
I0626 03:07:46.669062  8898 net.cpp:396] inner3 <- inner2
I0626 03:07:46.669071  8898 net.cpp:358] inner3 -> inner3
I0626 03:07:46.669081  8898 net.cpp:96] Setting up inner3
I0626 03:07:46.680960  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:46.681005  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:07:46.681015  8898 net.cpp:396] inner3relu <- inner3
I0626 03:07:46.681026  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:46.681037  8898 net.cpp:96] Setting up inner3relu
I0626 03:07:46.681046  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:46.681056  8898 net.cpp:67] Creating Layer output
I0626 03:07:46.681063  8898 net.cpp:396] output <- inner3
I0626 03:07:46.681073  8898 net.cpp:358] output -> output
I0626 03:07:46.681083  8898 net.cpp:96] Setting up output
I0626 03:07:46.682248  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:46.682265  8898 net.cpp:67] Creating Layer loss
I0626 03:07:46.682273  8898 net.cpp:396] loss <- output
I0626 03:07:46.682282  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:46.682292  8898 net.cpp:358] loss -> loss
I0626 03:07:46.682305  8898 net.cpp:358] loss -> std
I0626 03:07:46.682317  8898 net.cpp:358] loss -> ind
I0626 03:07:46.682327  8898 net.cpp:358] loss -> proba
I0626 03:07:46.682337  8898 net.cpp:96] Setting up loss
I0626 03:07:46.682344  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:46.682373  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:46.682381  8898 net.cpp:109]     with loss weight 1
I0626 03:07:46.682396  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:46.682404  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:46.682411  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:46.682425  8898 net.cpp:67] Creating Layer silence
I0626 03:07:46.682432  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:46.682441  8898 net.cpp:396] silence <- ind
I0626 03:07:46.682449  8898 net.cpp:396] silence <- proba
I0626 03:07:46.682457  8898 net.cpp:96] Setting up silence
I0626 03:07:46.682466  8898 net.cpp:172] silence does not need backward computation.
I0626 03:07:46.682472  8898 net.cpp:170] loss needs backward computation.
I0626 03:07:46.682480  8898 net.cpp:170] output needs backward computation.
I0626 03:07:46.682488  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:46.682495  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:07:46.682502  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:46.682510  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:07:46.682518  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:46.682524  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:07:46.682533  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:46.682540  8898 net.cpp:172] label does not need backward computation.
I0626 03:07:46.682548  8898 net.cpp:172] data does not need backward computation.
I0626 03:07:46.682554  8898 net.cpp:208] This network produces output loss
I0626 03:07:46.682562  8898 net.cpp:208] This network produces output std
I0626 03:07:46.682577  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:46.682587  8898 net.cpp:219] Network initialization done.
I0626 03:07:46.682595  8898 net.cpp:220] Memory required for data: 9903112
17251e-03
    9.71952031e-01]]
[2 4 4 ..., 4 3 4]
0.00563502384049
[[  2.08630656e-03   1.86408023e-03   9.92953404e-01   1.19745906e-03
    1.89875052e-03]
 [  3.32041893e-03   1.35733752e-03   2.04274265e-03   1.17082697e-03
    9.92108674e-01]
 [  2.86914045e-03   1.01221774e-03   1.68722566e-03   1.07049741e-03
    9.93360919e-01]
 [  1.79006716e-03   1.12662966e-03   9.94836724e-01   8.62204959e-04
    1.38437373e-03]
 [  3.67892235e-03   1.54852375e-03   2.33163579e-03   1.29355292e-03
    9.91147365e-01]
 [  1.95900216e-03   1.06538912e-03   9.94459218e-01   1.02808495e-03
    1.48830613e-03]
 [  5.99970870e-03   1.59905449e-03   3.06013831e-03   2.43598940e-03
    9.86905109e-01]
 [  1.73492173e-03   9.97622701e-04   9.94981659e-01   9.33313521e-04
    1.35248318e-03]
 [  1.50891496e-03   1.00194732e-03   9.95406945e-01   8.45275193e-04
    1.23691759e-03]
 [  4.43952930e-03   1.37261137e-03   2.29633373e-03   1.58803280e-03
    9.90303493e-01]]
[2 4 4 ..., 4 3 4]
0.00346770697876
[[  3.47836579e-03   3.62006516e-03   9.87611649e-01   1.98336988e-03
    3.30654982e-03]
 [  6.73622980e-03   3.69274642e-03   4.68303357e-03   2.39389079e-03
    9.82494099e-01]
 [  4.38611908e-03   2.05383714e-03   2.94747866e-03   1.67111147e-03
    9.88941454e-01]
 [  3.19116404e-03   2.06945918e-03   9.90961721e-01   1.40030856e-03
    2.37734673e-03]
 [  1.01069141e-02   6.12102410e-03   7.51990940e-03   3.56729913e-03
    9.72684853e-01]
 [  1.63970873e-03   9.45789582e-04   9.95343365e-01   8.23340838e-04
    1.24779629e-03]
 [  2.61540146e-03   8.43209114e-04   1.45581475e-03   1.08497064e-03
    9.94000604e-01]
 [  1.42399358e-03   8.84662077e-04   9.95806270e-01   7.53204825e-04
    1.13186917e-03]
 [  1.29445207e-03   9.73364684e-04   9.95894773e-01   7.27556578e-04
    1.10985334e-03]
 [  3.01287363e-03   1.20815052e-03   1.74370099e-03   1.08457876e-03
    9.92950696e-01]]
[2 4 4 ..., 4 3 4]
0.00520156046814
[[  7.90426414e-03   1.05059825e-02   9.68998732e-01   4.57223448e-03
    8.01878682e-03]
 [  1.05630311e-02   7.19506653e-03   8.11594980e-03   3.89968098e-03
    9.70226272e-01]
 [  5.32135740e-03   2.81908220e-03   3.84599558e-03   2.16571748e-03
    9.85847847e-01]
 [  9.91184353e-03   7.25046415e-03   9.71786038e-01   3.88201670e-03
    7.16963802e-03]
 [  1.56096550e-02   1.18864608e-02   1.27155277e-02   5.78776870e-03
    9.54000588e-01]
 [  3.05313492e-03   2.05112682e-03   9.91157478e-01   1.40645311e-03
    2.33180705e-03]
 [  2.22020197e-03   8.30691866e-04   1.34629011e-03   9.60971671e-04
    9.94641844e-01]
 [  2.04564158e-03   1.44648086e-03   9.93803953e-01   1.04451630e-03
    1.65940813e-03]
 [  1.84493306e-03   1.69867791e-03   9.93705150e-01   1.05233876e-03
    1.69890027e-03]
 [  3.53860274e-03   1.65737456e-03   2.22549287e-03   1.33654902e-03
    9.91241981e-01]]
[2 4 4 ..., 4 3 4]
0.0047680970958
[[  4.20241056e-03   2.91978120e-03   9.87559816e-01   2.04371320e-03
    3.27427886e-03]
 [  5.40839141e-03   2.71608693e-03   3.42413457e-03   1.86765432e-03
    9.86583733e-01]
 [  3.03792148e-03   1.22274547e-03   1.84428107e-03   1.15704772e-03
    9.92738004e-01]
 [  1.98410762e-02   5.99774221e-03   9.60324062e-01   5.06394010e-03
    8.77317952e-03]
 [  6.97498220e-03   3.46115103e-03   4.56524856e-03   2.46934573e-03
    9.82529272e-01]
 [  6.64418309e-03   2.23565354e-03   9.85452182e-01   2.23920325e-03
    3.42877862e-03]
 [  2.27728751e-03   7.54701472e-04   1.16225075e-03   8.70421356e-04
    9.94935339e-01]
 [  4.29055223e-03   1.57229324e-03   9.90084766e-01   1.65618475e-03
    2.39620393e-03]
 [  1.41827575e-03   7.61788244e-04   9.96096139e-01   7.04698135e-04
    1.01909842e-03]
 [  2.33596158e-03   8.15547860e-04   1.19449601e-03   8.31646306e-04
    9.94822348e-01]]
[2 4 4 ..., 4 3 4]
0.00563502384049
[[  1.88922211e-03   1.51838235e-03   9.93691827e-01   1.18376754e-03
    1.71680140e-03]
 [  2.11224317e-03   6.99075109e-04   1.17801306e-03   8.22936587e-04
    9.95187732e-01]
 [  2.37503059e-03   7.34920916e-04   1.31491920e-03   9.81129269e-04
    9.94594000e-01]
 [  1.293359I0626 03:07:48.144520 21710 caffe.cpp:99] Use GPU with device ID 0
I0626 03:07:48.385377 21710 caffe.cpp:107] Starting Optimization
I0626 03:07:48.385471 21710 solver.cpp:32] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0
stepsize: 100000
snapshot: 100
snapshot_prefix: "modules/image30x30_dim50/exp/test"
solver_mode: GPU
device_id: 0
debug_info: false
net: "modules/image30x30_dim50/net.prototxt"
snapshot_after_train: true
sample_print: false
I0626 03:07:48.385495 21710 solver.cpp:67] Creating training net from net file: modules/image30x30_dim50/net.prototxt
I0626 03:07:48.385687 21710 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:48.385700 21710 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:48.385790 21710 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00001484"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00001484"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
state {
  phase: TRAIN
}
I0626 03:07:48.385871 21710 net.cpp:67] Creating Layer data
I0626 03:07:48.385885 21710 net.cpp:358] data -> data
I0626 03:07:48.385900 21710 net.cpp:96] Setting up data
I0626 03:07:48.385936 21710 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:48.465718 21710 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:48.465876 21710 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:48.465900 21710 net.cpp:67] Creating Layer label
I0626 03:07:48.465911 21710 net.cpp:358] label -> label
I0626 03:07:48.465929 21710 net.cpp:96] Setting up label
I0626 03:07:48.465939 21710 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:48.566442 21710 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:48.566577 21710 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:48.566613 21710 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:48.566632 21710 net.cpp:396] label_label_0_split <- label
I0626 03:07:48.566671 21710 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:48.566700 21710 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:48.566722 21710 net.cpp:96] Setting up label_label_0_split
I0626 03:07:48.566751 21710 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:48.566768 21710 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:48.566790 21710 net.cpp:67] Creating Layer inner1
I0626 03:07:48.566807 21710 net.cpp:396] inner1 <- data
I0626 03:07:48.566828 21710 net.cpp:358] inner1 -> inner1
I0626 03:07:48.566851 21710 net.cpp:96] Setting up inner1
I0626 03:07:48.595820 21710 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:48.595880 21710 net.cpp:67] Creating Layer inner1relu
I0626 03:07:48.595890 21710 net.cpp:396] inner1relu <- inner1
I0626 03:07:48.595902 21710 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:48.595914 21710 net.cpp:96] Setting up inner1relu
I0626 03:07:48.595929 21710 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:48.595940 21710 net.cpp:67] Creating Layer inner2
I0626 03:07:48.595948 21710 net.cpp:396] inner2 <- inner1
I0626 03:07:48.595958 21710 net.cpp:358] inner2 -> inner2
I0626 03:07:48.595969 21710 net.cpp:96] Setting up inner2
I0626 03:07:48.598968 21710 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:48.598984 21710 net.cpp:67] Creating Layer inner2relu
I0626 03:07:48.598992 21710 net.cpp:396] inner2relu <- inner2
I0626 03:07:48.599001 21710 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:48.599010 21710 net.cpp:96] Setting up inner2relu
I0626 03:07:48.599018 21710 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:48.599028 21710 net.cpp:67] Creating Layer inner3
I0626 03:07:48.599046 21710 net.cpp:396] inner3 <- inner2
I0626 03:07:48.599056 21710 net.cpp:358] inner3 -> inner3
I0626 03:07:48.599066 21710 net.cpp:96] Setting up inner3
I0626 03:07:48.610728 21710 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:48.610760 21710 net.cpp:67] Creating Layer inner3relu
I0626 03:07:48.610769 21710 net.cpp:396] inner3relu <- inner3
I0626 03:07:48.610780 21710 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:48.610790 21710 net.cpp:96] Setting up inner3relu
I0626 03:07:48.610798 21710 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:48.610808 21710 net.cpp:67] Creating Layer output
I0626 03:07:48.610816 21710 net.cpp:396] output <- inner3
I0626 03:07:48.610826 21710 net.cpp:358] output -> output
I0626 03:07:48.610836 21710 net.cpp:96] Setting up output
I0626 03:07:48.612030 21710 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:48.612048 21710 net.cpp:67] Creating Layer loss
I0626 03:07:48.612057 21710 net.cpp:396] loss <- output
I0626 03:07:48.612066 21710 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:48.612076 21710 net.cpp:358] loss -> loss
I0626 03:07:48.612088 21710 net.cpp:358] loss -> std
I0626 03:07:48.612099 21710 net.cpp:358] loss -> ind
I0626 03:07:48.612110 21710 net.cpp:358] loss -> proba
I0626 03:07:48.612120 21710 net.cpp:96] Setting up loss
I0626 03:07:48.612133 21710 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:48.612146 21710 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:48.612154 21710 net.cpp:109]     with loss weight 1
I0626 03:07:48.612182 21710 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:48.612190 21710 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:48.612197 21710 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:48.612210 21710 net.cpp:67] Creating Layer silence
I0626 03:07:48.612218 21710 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:48.612227 21710 net.cpp:396] silence <- ind
I0626 03:07:48.612236 21710 net.cpp:396] silence <- proba
I0626 03:07:48.612244 21710 net.cpp:96] Setting up silence
I0626 03:07:48.612267 21710 net.cpp:172] silence does not need backward computation.
I0626 03:07:48.612275 21710 net.cpp:170] loss needs backward computation.
I0626 03:07:48.612283 21710 net.cpp:170] output needs backward computation.
I0626 03:07:48.612291 21710 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:48.612299 21710 net.cpp:170] inner3 needs backward computation.
I0626 03:07:48.612306 21710 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:48.612314 21710 net.cpp:170] inner2 needs backward computation.
I0626 03:07:48.612321 21710 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:48.612329 21710 net.cpp:170] inner1 needs backward computation.
I0626 03:07:48.612336 21710 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:48.612344 21710 net.cpp:172] label does not need backward computation.
I0626 03:07:48.612352 21710 net.cpp:172] data does not need backward computation.
I0626 03:07:48.612360 21710 net.cpp:208] This network produces output loss
I0626 03:07:48.612367 21710 net.cpp:208] This network produces output std
I0626 03:07:48.612380 21710 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:48.612390 21710 net.cpp:219] Network initialization done.
I0626 03:07:48.612402 21710 net.cpp:220] Memory required for data: 9903112
I0626 03:07:48.612442 21710 solver.cpp:41] Solver scaffolding done.
I0626 03:07:48.612452 21710 caffe.cpp:115] Finetuning from modules/image30x30_dim50/init.caffemodel
I0626 03:07:48.623172 21710 solver.cpp:160] Solving net
I0626 03:07:48.645025 21710 solver.cpp:192] Iteration 0, loss = 0.117716
I0626 03:07:48.645077 21710 solver.cpp:207]     Train net output #0: loss = 0.117716 (* 1 = 0.117716 loss)
I0626 03:07:48.645089 21710 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:48.645107 21710 solver.cpp:408] Iteration 0, lr = 0.01, mom = 0.9
I0626 03:07:48.741942 21710 solver.cpp:192] Iteration 10, loss = 0.0917137
I0626 03:07:48.742017 21710 solver.cpp:207]     Train net output #0: loss = 0.0917137 (* 1 = 0.0917137 loss)
I0626 03:07:48.742049 21710 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:48.742066 21710 solver.cpp:408] Iteration 10, lr = 0.01, mom = 0.9
I0626 03:07:48.851271 21710 solver.cpp:192] Iteration 20, loss = 0.100861
I0626 03:07:48.851331 21710 solver.cpp:207]     Train net output #0: loss = 0.100861 (* 1 = 0.100861 loss)
I0626 03:07:48.851343 21710 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:48.851354 21710 solver.cpp:408] Iteration 20, lr = 0.01, mom = 0.9
I0626 03:07:48.954252 21710 solver.cpp:192] Iteration 30, loss = 0.0937909
I0626 03:07:48.954326 21710 solver.cpp:207]     Train net output #0: loss = 0.0937909 (* 1 = 0.0937909 loss)
I0626 03:07:48.954344 21710 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:48.954362 21710 solver.cpp:408] Iteration 30, lr = 0.01, mom = 0.9
I0626 03:07:49.058908 21710 solver.cpp:192] Iteration 40, loss = 0.0981475
I0626 03:07:49.058985 21710 solver.cpp:207]     Train net output #0: loss = 0.0981475 (* 1 = 0.0981475 loss)
I0626 03:07:49.059003 21710 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:49.059020 21710 solver.cpp:408] Iteration 40, lr = 0.01, mom = 0.9
I0626 03:07:49.164980 21710 solver.cpp:192] Iteration 50, loss = 0.0983058
I0626 03:07:49.165058 21710 solver.cpp:207]     Train net output #0: loss = 0.0983058 (* 1 = 0.0983058 loss)
I0626 03:07:49.165077 21710 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:49.165096 21710 solver.cpp:408] Iteration 50, lr = 0.01, mom = 0.9
I0626 03:07:49.271875 21710 solver.cpp:192] Iteration 60, loss = 0.0945559
I0626 03:07:49.271948 21710 solver.cpp:207]     Train net output #0: loss = 0.0945559 (* 1 = 0.0945559 loss)
I0626 03:07:49.271966 21710 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:49.271982 21710 solver.cpp:408] Iteration 60, lr = 0.01, mom = 0.9
I0626 03:07:49.380532 21710 solver.cpp:192] Iteration 70, loss = 0.0874461
I0626 03:07:49.380604 21710 solver.cpp:207]     Train net output #0: loss = 0.0874461 (* 1 = 0.0874461 loss)
I0626 03:07:49.380642 21710 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:49.380659 21710 solver.cpp:408] Iteration 70, lr = 0.01, mom = 0.9
I0626 03:07:49.476575 21710 solver.cpp:192] Iteration 80, loss = 0.0869916
I0626 03:07:49.476645 21710 solver.cpp:207]     Train net output #0: loss = 0.0869916 (* 1 = 0.0869916 loss)
I0626 03:07:49.476662 21710 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:49.476677 21710 solver.cpp:408] Iteration 80, lr = 0.01, mom = 0.9
I0626 03:07:49.560365 21710 solver.cpp:192] Iteration 90, loss = 0.0951667
I0626 03:07:49.560437 21710 solver.cpp:207]     Train net output #0: loss = 0.0951667 (* 1 = 0.0951667 loss)
I0626 03:07:49.560454 21710 solver.cpp:207]     Train net output #1: std = 0
I0626 03:07:49.560470 21710 solver.cpp:408] Iteration 90, lr = 0.01, mom = 0.9
I0626 03:07:49.659714 21710 solver.cpp:318] Snapshotting to modules/image30x30_dim50/exp/test_iter_100.caffemodel
I0626 03:07:49.700752 21710 solver.cpp:325] Snapshotting solver state to modules/image30x30_dim50/exp/test_iter_100.solverstate
I0626 03:07:49.728498 21710 solver.cpp:229] Iteration 100, loss = 0.0850437
I0626 03:07:49.728544 21710 solver.cpp:234] Optimization Done.
I0626 03:07:49.728554 21710 caffe.cpp:121] Optimization Done.
I0626 03:07:49.802469  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0626 03:07:49.802500  8898 net.cpp:277] The NetState phase (0) differed from the phase (1) specified by a rule in layer label
I0626 03:07:49.802614  8898 net.cpp:39] Initializing net from parameters: 
name: "net"
layers {
  top: "data"
  name: "data"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/database/total"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  top: "label"
  name: "label"
  type: DATA
  data_param {
    source: "modules/image30x30_dim50/train_weight"
    batch_size: 256
    backend: LEVELDB
    seek: "00000000"
  }
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 1
  }
}
layers {
  bottom: "data"
  top: "inner1"
  name: "inner1"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner1"
  top: "inner1"
  name: "inner1relu"
  type: RELU
}
layers {
  bottom: "inner1"
  top: "inner2"
  name: "inner2"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner2"
  top: "inner2"
  name: "inner2relu"
  type: RELU
}
layers {
  bottom: "inner2"
  top: "inner3"
  name: "inner3"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 2000
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "inner3"
  top: "inner3"
  name: "inner3relu"
  type: RELU
}
layers {
  bottom: "inner3"
  top: "output"
  name: "output"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "gaussian"
      std: 0.05
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "output"
  bottom: "label"
  top: "loss"
  top: "std"
  top: "ind"
  top: "proba"
  name: "loss"
  type: MULTI_T_LOSS
  blobs_lr: 1
  blobs_lr: 0
  blobs_lr: 0
  multi_t_loss_param {
    num_center: 5
    lambda: 2
    beta: 1
    weight_filler {
      type: "gaussian"
      std: 0.5
    }
    bandwidth: 0.1
    alpha: 1
  }
}
layers {
  bottom: "label"
  bottom: "ind"
  bottom: "proba"
  name: "silence"
  type: SILENCE
}
I0626 03:07:49.802705  8898 net.cpp:67] Creating Layer data
I0626 03:07:49.802718  8898 net.cpp:358] data -> data
I0626 03:07:49.802733  8898 net.cpp:96] Setting up data
I0626 03:07:49.802743  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/database/total
I0626 03:07:50.008991  8898 data_layer.cpp:145] output data size: 256,3600,1,1
I0626 03:07:50.010754  8898 net.cpp:103] Top shape: 256 3600 1 1 (921600)
I0626 03:07:50.010810  8898 net.cpp:67] Creating Layer label
I0626 03:07:50.010834  8898 net.cpp:358] label -> label
I0626 03:07:50.010864  8898 net.cpp:96] Setting up label
I0626 03:07:50.010882  8898 data_layer.cpp:45] Opening leveldb modules/image30x30_dim50/train_weight
I0626 03:07:50.092664  8898 data_layer.cpp:145] output data size: 256,5,1,1
I0626 03:07:50.092774  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:50.092809  8898 net.cpp:67] Creating Layer label_label_0_split
I0626 03:07:50.092828  8898 net.cpp:396] label_label_0_split <- label
I0626 03:07:50.092851  8898 net.cpp:358] label_label_0_split -> label_label_0_split_0
I0626 03:07:50.092878  8898 net.cpp:358] label_label_0_split -> label_label_0_split_1
I0626 03:07:50.092900  8898 net.cpp:96] Setting up label_label_0_split
I0626 03:07:50.092918  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:50.092934  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:50.092957  8898 net.cpp:67] Creating Layer inner1
I0626 03:07:50.092973  8898 net.cpp:396] inner1 <- data
I0626 03:07:50.092993  8898 net.cpp:358] inner1 -> inner1
I0626 03:07:50.093017  8898 net.cpp:96] Setting up inner1
I0626 03:07:50.119406  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:50.119474  8898 net.cpp:67] Creating Layer inner1relu
I0626 03:07:50.119485  8898 net.cpp:396] inner1relu <- inner1
I0626 03:07:50.119498  8898 net.cpp:347] inner1relu -> inner1 (in-place)
I0626 03:07:50.119516  8898 net.cpp:96] Setting up inner1relu
I0626 03:07:50.119524  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:50.119547  8898 net.cpp:67] Creating Layer inner2
I0626 03:07:50.119555  8898 net.cpp:396] inner2 <- inner1
I0626 03:07:50.119567  8898 net.cpp:358] inner2 -> inner2
I0626 03:07:50.119578  8898 net.cpp:96] Setting up inner2
I0626 03:07:50.122489  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:50.122505  8898 net.cpp:67] Creating Layer inner2relu
I0626 03:07:50.122514  8898 net.cpp:396] inner2relu <- inner2
I0626 03:07:50.122522  8898 net.cpp:347] inner2relu -> inner2 (in-place)
I0626 03:07:50.122531  8898 net.cpp:96] Setting up inner2relu
I0626 03:07:50.122539  8898 net.cpp:103] Top shape: 256 500 1 1 (128000)
I0626 03:07:50.122548  8898 net.cpp:67] Creating Layer inner3
I0626 03:07:50.122556  8898 net.cpp:396] inner3 <- inner2
I0626 03:07:50.122565  8898 net.cpp:358] inner3 -> inner3
I0626 03:07:50.122576  8898 net.cpp:96] Setting up inner3
I0626 03:07:50.134186  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:50.134225  8898 net.cpp:67] Creating Layer inner3relu
I0626 03:07:50.134235  8898 net.cpp:396] inner3relu <- inner3
I0626 03:07:50.134246  8898 net.cpp:347] inner3relu -> inner3 (in-place)
I0626 03:07:50.134256  8898 net.cpp:96] Setting up inner3relu
I0626 03:07:50.134264  8898 net.cpp:103] Top shape: 256 2000 1 1 (512000)
I0626 03:07:50.134275  8898 net.cpp:67] Creating Layer output
I0626 03:07:50.134282  8898 net.cpp:396] output <- inner3
I0626 03:07:50.134292  8898 net.cpp:358] output -> output
I0626 03:07:50.134304  8898 net.cpp:96] Setting up output
I0626 03:07:50.135510  8898 net.cpp:103] Top shape: 256 50 1 1 (12800)
I0626 03:07:50.135529  8898 net.cpp:67] Creating Layer loss
I0626 03:07:50.135538  8898 net.cpp:396] loss <- output
I0626 03:07:50.135547  8898 net.cpp:396] loss <- label_label_0_split_0
I0626 03:07:50.135557  8898 net.cpp:358] loss -> loss
I0626 03:07:50.135571  8898 net.cpp:358] loss -> std
I0626 03:07:50.135581  8898 net.cpp:358] loss -> ind
I0626 03:07:50.135592  8898 net.cpp:358] loss -> proba
I0626 03:07:50.135602  8898 net.cpp:96] Setting up loss
I0626 03:07:50.135610  8898 multi_t_loss_layer.cpp:17] 5 50 12800
I0626 03:07:50.135624  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:50.135632  8898 net.cpp:109]     with loss weight 1
I0626 03:07:50.135648  8898 net.cpp:103] Top shape: 1 1 1 1 (1)
I0626 03:07:50.135656  8898 net.cpp:103] Top shape: 256 1 1 1 (256)
I0626 03:07:50.135664  8898 net.cpp:103] Top shape: 256 5 1 1 (1280)
I0626 03:07:50.135677  8898 net.cpp:67] Creating Layer silence
I0626 03:07:50.135685  8898 net.cpp:396] silence <- label_label_0_split_1
I0626 03:07:50.135694  8898 net.cpp:396] silence <- ind
I0626 03:07:50.135702  8898 net.cpp:396] silence <- proba
I0626 03:07:50.135710  8898 net.cpp:96] Setting up silence
I0626 03:07:50.135718  8898 net.cpp:172] silence does not need backward computation.
I0626 03:07:50.135725  8898 net.cpp:170] loss needs backward computation.
I0626 03:07:50.135733  8898 net.cpp:170] output needs backward computation.
I0626 03:07:50.135741  8898 net.cpp:170] inner3relu needs backward computation.
I0626 03:07:50.135748  8898 net.cpp:170] inner3 needs backward computation.
I0626 03:07:50.135756  8898 net.cpp:170] inner2relu needs backward computation.
I0626 03:07:50.135763  8898 net.cpp:170] inner2 needs backward computation.
I0626 03:07:50.135771  8898 net.cpp:170] inner1relu needs backward computation.
I0626 03:07:50.135778  8898 net.cpp:170] inner1 needs backward computation.
I0626 03:07:50.135787  8898 net.cpp:172] label_label_0_split does not need backward computation.
I0626 03:07:50.135794  8898 net.cpp:172] label does not need backward computation.
I0626 03:07:50.135802  8898 net.cpp:172] data does not need backward computation.
I0626 03:07:50.135808  8898 net.cpp:208] This network produces output loss
I0626 03:07:50.135816  8898 net.cpp:208] This network produces output std
I0626 03:07:50.135830  8898 net.cpp:469] Collecting Learning Rate and Weight Decay.
I0626 03:07:50.135840  8898 net.cpp:219] Network initialization done.
I0626 03:07:50.135859  8898 net.cpp:220] Memory required for data: 9903112
41e-03   7.70846543e-04   9.96262224e-01   6.63572569e-04
    1.00999791e-03]
 [  1.83873539e-03   6.50715840e-04   1.07061416e-03   7.29327556e-04
    9.95710607e-01]
 [  1.15683202e-03   6.38867978e-04   9.96647797e-01   6.53291645e-04
    9.03210895e-04]
 [  6.22266707e-03   1.68210845e-03   3.10485565e-03   2.62113702e-03
    9.86369232e-01]
 [  1.20514588e-03   6.67382868e-04   9.96480574e-01   7.00121398e-04
    9.46776207e-04]
 [  1.08778635e-03   6.58126084e-04   9.96721637e-01   6.56508911e-04
    8.75942034e-04]
 [  5.20071022e-03   1.47531472e-03   2.59501722e-03   2.04840293e-03
    9.88680555e-01]]
[2 4 4 ..., 4 3 4]
0.000433463372345
frame_00000001_obj_001.jpg -> 2		frame_00000001_obj_003.jpg -> 4		frame_00000002_obj_001.jpg -> 4		frame_00000002_obj_002.jpg -> 2
frame_00000003_obj_001.jpg -> 4		frame_00000003_obj_002.jpg -> 2		frame_00000004_obj_001.jpg -> 4		frame_00000004_obj_002.jpg -> 2
frame_00000005_obj_002.jpg -> 2		frame_00000006_obj_001.jpg -> 4		frame_00000006_obj_002.jpg -> 2		frame_00000007_obj_002.jpg -> 2
frame_00000008_obj_002.jpg -> 2		frame_00000009_obj_002.jpg -> 2		frame_00000010_obj_002.jpg -> 2		frame_00000011_obj_001.jpg -> 4
frame_00000011_obj_002.jpg -> 2		frame_00000012_obj_002.jpg -> 2		frame_00000013_obj_002.jpg -> 2		frame_00000014_obj_002.jpg -> 2
frame_00000015_obj_002.jpg -> 2		frame_00000016_obj_002.jpg -> 2		frame_00000017_obj_001.jpg -> 1		frame_00000017_obj_002.jpg -> 2
frame_00000018_obj_002.jpg -> 2		frame_00000019_obj_001.jpg -> 1		frame_00000019_obj_002.jpg -> 2		frame_00000020_obj_001.jpg -> 1
frame_00000020_obj_002.jpg -> 2		frame_00000021_obj_001.jpg -> 1		frame_00000021_obj_002.jpg -> 2		frame_00000022_obj_001.jpg -> 1
frame_00000022_obj_002.jpg -> 2		frame_00000023_obj_001.jpg -> 1		frame_00000023_obj_002.jpg -> 2		frame_00000024_obj_001.jpg -> 1
frame_00000024_obj_002.jpg -> 2		frame_00000025_obj_001.jpg -> 1		frame_00000025_obj_002.jpg -> 2		frame_00000026_obj_001.jpg -> 1
frame_00000026_obj_002.jpg -> 2		frame_00000027_obj_001.jpg -> 1		frame_00000027_obj_003.jpg -> 2		frame_00000028_obj_001.jpg -> 1
frame_00000028_obj_003.jpg -> 2		frame_00000029_obj_001.jpg -> 1		frame_00000029_obj_003.jpg -> 2		frame_00000030_obj_001.jpg -> 1
frame_00000030_obj_002.jpg -> 2		frame_00000030_obj_003.jpg -> 2		frame_00000031_obj_001.jpg -> 1		frame_00000031_obj_002.jpg -> 1
frame_00000031_obj_003.jpg -> 2		frame_00000032_obj_001.jpg -> 1		frame_00000032_obj_002.jpg -> 1		frame_00000032_obj_003.jpg -> 2
frame_00000033_obj_001.jpg -> 1		frame_00000033_obj_002.jpg -> 1		frame_00000033_obj_003.jpg -> 2		frame_00000034_obj_001.jpg -> 1
frame_00000034_obj_002.jpg -> 1		frame_00000034_obj_003.jpg -> 2		frame_00000035_obj_001.jpg -> 1		frame_00000035_obj_003.jpg -> 1
frame_00000035_obj_004.jpg -> 2		frame_00000035_obj_005.jpg -> 2		frame_00000036_obj_001.jpg -> 1		frame_00000036_obj_003.jpg -> 1
frame_00000036_obj_004.jpg -> 2		frame_00000036_obj_005.jpg -> 2		frame_00000037_obj_002.jpg -> 1		frame_00000037_obj_003.jpg -> 1
frame_00000038_obj_002.jpg -> 1		frame_00000038_obj_003.jpg -> 2		frame_00000038_obj_004.jpg -> 1		frame_00000039_obj_003.jpg -> 1
frame_00000039_obj_004.jpg -> 1		frame_00000040_obj_002.jpg -> 1		frame_00000040_obj_003.jpg -> 1		frame_00000041_obj_003.jpg -> 1
frame_00000041_obj_004.jpg -> 1		frame_00000042_obj_003.jpg -> 1		frame_00000042_obj_004.jpg -> 1		frame_00000043_obj_002.jpg -> 1
frame_00000043_obj_003.jpg -> 1		frame_00000044_obj_003.jpg -> 1		frame_00000044_obj_004.jpg -> 1		frame_00000045_obj_002.jpg -> 1
frame_00000045_obj_003.jpg -> 1		frame_00000046_obj_002.jpg -> 1		frame_00000046_obj_003.jpg -> 1		frame_00000047_obj_001.jpg -> 1
frame_00000047_obj_004.jpg -> 1		frame_00000048_obj_003.jpg -> 1		frame_00000048_obj_004.jpg -> 1		frame_00000049_obj_001.jpg -> 1
frame_00000049_obj_004.jpg -> 1		frame_00000050_obj_001.jpg -> 1		frame_00000050_obj_004.jpg -> 1		frame_00000051_obj_001.jpg -> 1
frame_00000051_obj_004.jpg -> 1		frame_00000052_obj_001.jpg -> 1		frame_00000052_obj_003.jpg -> 1		frame_00000052_obj_004.jpg -> 3
frame_00000053_obj_001.jpg -> 1		frame_00000053_obj_004.jpg -> 1		frame_00000053_obj_005.jpg -> 3		frame_00000054_obj_001.jpg -> 1
frame_00000054_obj_007.jpg -> 1		frame_00000054_obj_008.jpg -> 3		frame_00000055_obj_001.jpg -> 1		frame_00000055_obj_006.jpg -> 1
frame_00000055_obj_007.jpg -> 3		frame_00000056_obj_001.jpg -> 1		frame_00000056_obj_005.jpg -> 1		frame_00000056_obj_006.jpg -> 3
frame_00000057_obj_001.jpg -> 1		frame_00000057_obj_005.jpg -> 1		frame_00000057_obj_006.jpg -> 3		frame_00000058_obj_001.jpg -> 1
frame_00000058_obj_007.jpg -> 1		frame_00000058_obj_008.jpg -> 3		frame_00000059_obj_001.jpg -> 1		frame_00000059_obj_005.jpg -> 1
frame_00000059_obj_006.jpg -> 3		frame_00000060_obj_001.jpg -> 1		frame_00000060_obj_005.jpg -> 1		frame_00000060_obj_006.jpg -> 3
frame_00000061_obj_001.jpg -> 1		frame_00000061_obj_005.jpg -> 1		frame_00000061_obj_006.jpg -> 3		frame_00000062_obj_001.jpg -> 1
frame_00000062_obj_005.jpg -> 1		frame_00000062_obj_006.jpg -> 3		frame_00000063_obj_004.jpg -> 1		frame_00000063_obj_005.jpg -> 3
frame_00000064_obj_004.jpg -> 1		frame_00000064_obj_005.jpg -> 3		frame_00000065_obj_004.jpg -> 1		frame_00000065_obj_005.jpg -> 3
frame_00000066_obj_004.jpg -> 1		frame_00000066_obj_005.jpg -> 3		frame_00000067_obj_004.jpg -> 1		frame_00000067_obj_005.jpg -> 3
frame_00000068_obj_004.jpg -> 1		frame_00000068_obj_005.jpg -> 3		frame_00000069_obj_005.jpg -> 1		frame_00000069_obj_006.jpg -> 3
frame_00000070_obj_004.jpg -> 1		frame_00000070_obj_005.jpg -> 3		frame_00000071_obj_004.jpg -> 1		frame_00000071_obj_005.jpg -> 3
frame_00000072_obj_005.jpg -> 1		frame_00000072_obj_006.jpg -> 3		frame_00000073_obj_001.jpg -> 1		frame_00000073_obj_006.jpg -> 1
frame_00000073_obj_007.jpg -> 3		frame_00000074_obj_006.jpg -> 1		frame_00000074_obj_007.jpg -> 3		frame_00000075_obj_007.jpg -> 1
frame_00000075_obj_008.jpg -> 3		frame_00000076_obj_006.jpg -> 1		frame_00000076_obj_007.jpg -> 3		frame_00000077_obj_007.jpg -> 1
frame_00000077_obj_008.jpg -> 3		frame_00000078_obj_006.jpg -> 1		frame_00000078_obj_007.jpg -> 3		frame_00000079_obj_006.jpg -> 1
frame_00000079_obj_007.jpg -> 3		frame_00000080_obj_007.jpg -> 1		frame_00000080_obj_008.jpg -> 3		frame_00000081_obj_006.jpg -> 1
frame_00000081_obj_007.jpg -> 3		frame_00000082_obj_006.jpg -> 2		frame_00000082_obj_007.jpg -> 3		frame_00000083_obj_005.jpg -> 2
frame_00000083_obj_006.jpg -> 3		frame_00000084_obj_005.jpg -> 2		frame_00000084_obj_006.jpg -> 3		frame_00000085_obj_003.jpg -> 2
frame_00000085_obj_006.jpg -> 3		frame_00000086_obj_007.jpg -> 2		frame_00000086_obj_008.jpg -> 3		frame_00000087_obj_003.jpg -> 2
frame_00000087_obj_006.jpg -> 3		frame_00000088_obj_004.jpg -> 2		frame_00000088_obj_007.jpg -> 3		frame_00000089_obj_003.jpg -> 2
frame_00000089_obj_006.jpg -> 3		frame_00000090_obj_005.jpg -> 2		frame_00000090_obj_006.jpg -> 4		frame_00000091_obj_003.jpg -> 2
frame_00000091_obj_007.jpg -> 4		frame_00000092_obj_002.jpg -> 2		frame_00000092_obj_005.jpg -> 4		frame_00000093_obj_002.jpg -> 1
frame_00000093_obj_005.jpg -> 4		frame_00000094_obj_002.jpg -> 2		frame_00000094_obj_005.jpg -> 3		frame_00000094_obj_006.jpg -> 4
frame_00000095_obj_004.jpg -> 2		frame_00000095_obj_005.jpg -> 3		frame_00000095_obj_006.jpg -> 4		frame_00000096_obj_001.jpg -> 1
frame_00000096_obj_005.jpg -> 3		frame_00000096_obj_006.jpg -> 4		frame_00000096_obj_007.jpg -> 3		frame_00000097_obj_001.jpg -> 1
frame_00000097_obj_005.jpg -> 4		frame_00000097_obj_006.jpg -> 3		frame_00000098_obj_001.jpg -> 1		frame_00000098_obj_005.jpg -> 4
frame_00000098_obj_006.jpg -> 3		frame_00000099_obj_001.jpg -> 1		frame_00000099_obj_005.jpg -> 4		frame_00000099_obj_006.jpg -> 3
frame_00000100_obj_004.jpg -> 1		frame_00000100_obj_007.jpg -> 4		frame_00000100_obj_008.jpg -> 3		frame_00000101_obj_004.jpg -> 1
frame_00000101_obj_006.jpg -> 4		frame_00000101_obj_007.jpg -> 3		frame_00000102_obj_003.jpg -> 1		frame_00000102_obj_006.jpg -> 4
frame_00000102_obj_007.jpg -> 3		frame_00000103_obj_004.jpg -> 1		frame_00000103_obj_006.jpg -> 4		frame_00000103_obj_007.jpg -> 3
frame_00000104_obj_001.jpg -> 1		frame_00000104_obj_007.jpg -> 4		frame_00000104_obj_008.jpg -> 3		frame_00000105_obj_001.jpg -> 1
frame_00000105_obj_007.jpg -> 4		frame_00000105_obj_008.jpg -> 3		frame_00000106_obj_001.jpg -> 1		frame_00000106_obj_006.jpg -> 4
frame_00000106_obj_008.jpg -> 3		frame_00000107_obj_001.jpg -> 1		frame_00000107_obj_007.jpg -> 4		frame_00000107_obj_008.jpg -> 3
frame_00000108_obj_001.jpg -> 1		frame_00000108_obj_006.jpg -> 4		frame_00000108_obj_007.jpg -> 3		frame_00000109_obj_001.jpg -> 1
frame_00000109_obj_008.jpg -> 4		frame_00000109_obj_009.jpg -> 3		frame_00000110_obj_001.jpg -> 1		frame_00000110_obj_008.jpg -> 4
frame_00000110_obj_009.jpg -> 3		frame_00000111_obj_001.jpg -> 1		frame_00000111_obj_007.jpg -> 4		frame_00000111_obj_008.jpg -> 3
frame_00000112_obj_001.jpg -> 1		frame_00000112_obj_007.jpg -> 4		frame_00000112_obj_008.jpg -> 3		frame_00000113_obj_001.jpg -> 1
frame_00000113_obj_002.jpg -> 3		frame_00000113_obj_007.jpg -> 4		frame_00000113_obj_008.jpg -> 3		frame_00000114_obj_001.jpg -> 1
frame_00000114_obj_002.jpg -> 3		frame_00000114_obj_007.jpg -> 4		frame_00000114_obj_008.jpg -> 3		frame_00000115_obj_002.jpg -> 1
frame_00000115_obj_008.jpg -> 4		frame_00000115_obj_009.jpg -> 3		frame_00000116_obj_002.jpg -> 1		frame_00000116_obj_006.jpg -> 3
frame_00000116_obj_008.jpg -> 3		frame_00000117_obj_002.jpg -> 1		frame_00000117_obj_003.jpg -> 3		frame_00000117_obj_006.jpg -> 3
frame_00000117_obj_008.jpg -> 3		frame_00000118_obj_002.jpg -> 1		frame_00000118_obj_006.jpg -> 3		frame_00000119_obj_002.jpg -> 1
frame_00000119_obj_005.jpg -> 3		frame_00000119_obj_006.jpg -> 3		frame_00000120_obj_002.jpg -> 3		frame_00000120_obj_003.jpg -> 1
frame_00000120_obj_007.jpg -> 3		frame_00000120_obj_008.jpg -> 3		frame_00000121_obj_002.jpg -> 3		frame_00000121_obj_003.jpg -> 1
frame_00000121_obj_007.jpg -> 3		frame_00000122_obj_002.jpg -> 3		frame_00000122_obj_003.jpg -> 1		frame_00000122_obj_008.jpg -> 3
frame_00000123_obj_003.jpg -> 3		frame_00000123_obj_004.jpg -> 1		frame_00000123_obj_009.jpg -> 3		frame_00000124_obj_002.jpg -> 1
frame_00000124_obj_007.jpg -> 3		frame_00000124_obj_008.jpg -> 3		frame_00000125_obj_002.jpg -> 4		frame_00000125_obj_003.jpg -> 1
frame_00000125_obj_007.jpg -> 3		frame_00000125_obj_008.jpg -> 3		frame_00000126_obj_002.jpg -> 4		frame_00000126_obj_003.jpg -> 1
frame_00000126_obj_005.jpg -> 2		frame_00000126_obj_007.jpg -> 3		frame_00000126_obj_008.jpg -> 4		frame_00000127_obj_003.jpg -> 1
frame_00000127_obj_007.jpg -> 4		frame_00000128_obj_002.jpg -> 1		frame_00000128_obj_006.jpg -> 4		frame_00000129_obj_001.jpg -> 1
frame_00000129_obj_005.jpg -> 4		frame_00000130_obj_004.jpg -> 4		frame_00000131_obj_004.jpg -> 4		frame_00000132_obj_004.jpg -> 4
frame_00000133_obj_006.jpg -> 4		frame_00000134_obj_004.jpg -> 4		frame_00000135_obj_005.jpg -> 4		frame_00000136_obj_005.jpg -> 4
frame_00000137_obj_002.jpg -> 2		frame_00000137_obj_007.jpg -> 4		frame_00000138_obj_005.jpg -> 4		frame_00000139_obj_004.jpg -> 4
frame_00000140_obj_004.jpg -> 4		frame_00000141_obj_004.jpg -> 4		frame_00000142_obj_005.jpg -> 4		frame_00000143_obj_007.jpg -> 4
frame_00000144_obj_006.jpg -> 4		frame_00000145_obj_006.jpg -> 4		frame_00000146_obj_005.jpg -> 4		frame_00000147_obj_006.jpg -> 4
frame_00000148_obj_006.jpg -> 4		frame_00000149_obj_005.jpg -> 4		frame_00000150_obj_005.jpg -> 4		frame_00000151_obj_005.jpg -> 4
frame_00000152_obj_005.jpg -> 4		frame_00000153_obj_007.jpg -> 4		frame_00000154_obj_008.jpg -> 4		frame_00000155_obj_008.jpg -> 4
frame_00000156_obj_009.jpg -> 4		frame_00000157_obj_007.jpg -> 4		frame_00000158_obj_007.jpg -> 4		frame_00000159_obj_007.jpg -> 4
frame_00000160_obj_007.jpg -> 4		frame_00000161_obj_007.jpg -> 4		frame_00000162_obj_006.jpg -> 4		frame_00000163_obj_006.jpg -> 1
frame_00000164_obj_008.jpg -> 1		frame_00000165_obj_007.jpg -> 1		frame_00000166_obj_008.jpg -> 1		frame_00000167_obj_008.jpg -> 1
frame_00000168_obj_009.jpg -> 1		frame_00000169_obj_008.jpg -> 1		frame_00000170_obj_008.jpg -> 1		frame_00000171_obj_004.jpg -> 1
frame_00000172_obj_004.jpg -> 1		frame_00000173_obj_002.jpg -> 1		frame_00000173_obj_004.jpg -> 1		frame_00000174_obj_003.jpg -> 4
frame_00000174_obj_005.jpg -> 1		frame_00000175_obj_004.jpg -> 1		frame_00000176_obj_006.jpg -> 1		frame_00000177_obj_004.jpg -> 1
frame_00000178_obj_005.jpg -> 1		frame_00000179_obj_005.jpg -> 1		frame_00000180_obj_006.jpg -> 1		frame_00000181_obj_006.jpg -> 1
frame_00000182_obj_004.jpg -> 1		frame_00000183_obj_001.jpg -> 1		frame_00000183_obj_005.jpg -> 1		frame_00000184_obj_003.jpg -> 1
frame_00000185_obj_003.jpg -> 1		frame_00000186_obj_003.jpg -> 1		frame_00000187_obj_004.jpg -> 1		frame_00000188_obj_006.jpg -> 3
frame_00000189_obj_007.jpg -> 3		frame_00000189_obj_008.jpg -> 3		frame_00000190_obj_008.jpg -> 3		frame_00000191_obj_008.jpg -> 3
frame_00000192_obj_006.jpg -> 3		frame_00000193_obj_006.jpg -> 3		frame_00000194_obj_007.jpg -> 3		frame_00000195_obj_008.jpg -> 3
frame_00000196_obj_007.jpg -> 3		frame_00000197_obj_007.jpg -> 3		frame_00000198_obj_007.jpg -> 3		frame_00000199_obj_007.jpg -> 3
frame_00000200_obj_007.jpg -> 3		frame_00000201_obj_005.jpg -> 3		frame_00000202_obj_005.jpg -> 3		frame_00000203_obj_006.jpg -> 3
frame_00000204_obj_005.jpg -> 3		frame_00000205_obj_005.jpg -> 3		frame_00000206_obj_006.jpg -> 3		frame_00000207_obj_006.jpg -> 3
frame_00000208_obj_006.jpg -> 3		frame_00000209_obj_006.jpg -> 3		frame_00000210_obj_005.jpg -> 3		frame_00000211_obj_005.jpg -> 3
frame_00000211_obj_006.jpg -> 3		frame_00000212_obj_006.jpg -> 3		frame_00000212_obj_007.jpg -> 3		frame_00000213_obj_007.jpg -> 3
frame_00000214_obj_006.jpg -> 0		frame_00000214_obj_007.jpg -> 3		frame_00000215_obj_005.jpg -> 3		frame_00000215_obj_006.jpg -> 3
frame_00000216_obj_005.jpg -> 0		frame_00000216_obj_006.jpg -> 3		frame_00000217_obj_005.jpg -> 3		frame_00000218_obj_004.jpg -> 3
frame_00000219_obj_004.jpg -> 3		frame_00000220_obj_004.jpg -> 3		frame_00000221_obj_004.jpg -> 3		frame_00000222_obj_004.jpg -> 3
frame_00000223_obj_001.jpg -> 1		frame_00000223_obj_004.jpg -> 3		frame_00000224_obj_004.jpg -> 3		frame_00000225_obj_004.jpg -> 3
frame_00000226_obj_004.jpg -> 3		frame_00000227_obj_003.jpg -> 3		frame_00000228_obj_003.jpg -> 3		frame_00000229_obj_003.jpg -> 3
frame_00000230_obj_003.jpg -> 0		frame_00000231_obj_004.jpg -> 0		frame_00000232_obj_004.jpg -> 0		frame_00000233_obj_003.jpg -> 0
frame_00000234_obj_003.jpg -> 0		frame_00000235_obj_003.jpg -> 0		frame_00000236_obj_003.jpg -> 0		frame_00000237_obj_003.jpg -> 0
frame_00000238_obj_003.jpg -> 0		frame_00000239_obj_003.jpg -> 0		frame_00000240_obj_002.jpg -> 2		frame_00000240_obj_003.jpg -> 0
frame_00000241_obj_001.jpg -> 2		frame_00000241_obj_002.jpg -> 2		frame_00000241_obj_003.jpg -> 3		frame_00000242_obj_002.jpg -> 2
frame_00000242_obj_003.jpg -> 2		frame_00000242_obj_004.jpg -> 0		frame_00000243_obj_002.jpg -> 2		frame_00000243_obj_003.jpg -> 2
frame_00000243_obj_004.jpg -> 0		frame_00000244_obj_001.jpg -> 2		frame_00000244_obj_002.jpg -> 0		frame_00000244_obj_003.jpg -> 3
frame_00000244_obj_004.jpg -> 3		frame_00000245_obj_001.jpg -> 2		frame_00000245_obj_002.jpg -> 0		frame_00000245_obj_004.jpg -> 3
frame_00000246_obj_001.jpg -> 2		frame_00000246_obj_002.jpg -> 2		frame_00000246_obj_003.jpg -> 3		frame_00000246_obj_004.jpg -> 3
frame_00000247_obj_002.jpg -> 2		frame_00000247_obj_003.jpg -> 0		frame_00000247_obj_004.jpg -> 2		frame_00000247_obj_005.jpg -> 3
frame_00000248_obj_001.jpg -> 2		frame_00000248_obj_002.jpg -> 0		frame_00000248_obj_003.jpg -> 2		frame_00000248_obj_004.jpg -> 3
frame_00000248_obj_005.jpg -> 0		frame_00000249_obj_002.jpg -> 0		frame_00000249_obj_003.jpg -> 2		frame_00000249_obj_004.jpg -> 3
frame_00000249_obj_005.jpg -> 0		frame_00000250_obj_001.jpg -> 2		frame_00000250_obj_002.jpg -> 0		frame_00000250_obj_003.jpg -> 2
frame_00000250_obj_004.jpg -> 3		frame_00000250_obj_005.jpg -> 0		frame_00000251_obj_002.jpg -> 3		frame_00000251_obj_003.jpg -> 2
frame_00000251_obj_004.jpg -> 0		frame_00000251_obj_005.jpg -> 0		frame_00000251_obj_006.jpg -> 2		frame_00000252_obj_001.jpg -> 2
frame_00000252_obj_002.jpg -> 0		frame_00000252_obj_003.jpg -> 2		frame_00000253_obj_001.jpg -> 2		frame_00000253_obj_002.jpg -> 0
frame_00000253_obj_003.jpg -> 2		frame_00000254_obj_001.jpg -> 2		frame_00000254_obj_002.jpg -> 3		frame_00000254_obj_003.jpg -> 0
frame_00000254_obj_004.jpg -> 2		frame_00000255_obj_001.jpg -> 2		frame_00000255_obj_002.jpg -> 3		frame_00000255_obj_003.jpg -> 0
frame_00000255_obj_004.jpg -> 2		frame_00000256_obj_001.jpg -> 2		frame_00000256_obj_002.jpg -> 0		frame_00000256_obj_003.jpg -> 0
frame_00000256_obj_004.jpg -> 2		frame_00000257_obj_002.jpg -> 0		frame_00000257_obj_003.jpg -> 0		frame_00000257_obj_004.jpg -> 2
frame_00000258_obj_001.jpg -> 2		frame_00000258_obj_002.jpg -> 1		frame_00000258_obj_003.jpg -> 0		frame_00000258_obj_004.jpg -> 3
frame_00000259_obj_001.jpg -> 0		frame_00000259_obj_002.jpg -> 2		frame_00000259_obj_003.jpg -> 0		frame_00000259_obj_004.jpg -> 3
frame_00000260_obj_001.jpg -> 0		frame_00000260_obj_002.jpg -> 1		frame_00000260_obj_003.jpg -> 0		frame_00000260_obj_004.jpg -> 3
frame_00000261_obj_001.jpg -> 0		frame_00000261_obj_002.jpg -> 0		frame_00000261_obj_003.jpg -> 3		frame_00000262_obj_001.jpg -> 0
frame_00000262_obj_002.jpg -> 3		frame_00000263_obj_001.jpg -> 2		frame_00000263_obj_003.jpg -> 0		frame_00000263_obj_004.jpg -> 0
frame_00000263_obj_005.jpg -> 3		frame_00000264_obj_002.jpg -> 4		frame_00000264_obj_003.jpg -> 0		frame_00000264_obj_004.jpg -> 3
frame_00000265_obj_001.jpg -> 2		frame_00000265_obj_002.jpg -> 0		frame_00000265_obj_003.jpg -> 3		frame_00000266_obj_001.jpg -> 0
frame_00000266_obj_002.jpg -> 3		frame_00000267_obj_001.jpg -> 0		frame_00000267_obj_002.jpg -> 3		frame_00000268_obj_001.jpg -> 0
frame_00000268_obj_002.jpg -> 3		frame_00000269_obj_001.jpg -> 0		frame_00000269_obj_002.jpg -> 3		frame_00000270_obj_001.jpg -> 0
frame_00000270_obj_002.jpg -> 3		frame_00000271_obj_001.jpg -> 0		frame_00000271_obj_002.jpg -> 0		frame_00000271_obj_003.jpg -> 3
frame_00000272_obj_001.jpg -> 0		frame_00000272_obj_002.jpg -> 0		frame_00000272_obj_003.jpg -> 3		frame_00000273_obj_001.jpg -> 0
frame_00000273_obj_002.jpg -> 0		frame_00000273_obj_003.jpg -> 3		frame_00000274_obj_001.jpg -> 0		frame_00000274_obj_003.jpg -> 3
frame_00000275_obj_002.jpg -> 0		frame_00000275_obj_003.jpg -> 3		frame_00000275_obj_004.jpg -> 3		frame_00000276_obj_002.jpg -> 0
frame_00000276_obj_003.jpg -> 3		frame_00000277_obj_001.jpg -> 0		frame_00000277_obj_002.jpg -> 3		frame_00000278_obj_001.jpg -> 0
frame_00000278_obj_002.jpg -> 3		frame_00000279_obj_001.jpg -> 0		frame_00000279_obj_002.jpg -> 3		frame_00000280_obj_001.jpg -> 0
frame_00000280_obj_002.jpg -> 3		frame_00000281_obj_001.jpg -> 0		frame_00000281_obj_002.jpg -> 3		frame_00000282_obj_001.jpg -> 0
frame_00000282_obj_002.jpg -> 3		frame_00000283_obj_001.jpg -> 0		frame_00000283_obj_002.jpg -> 3		frame_00000284_obj_001.jpg -> 0
frame_00000284_obj_002.jpg -> 3		frame_00000285_obj_001.jpg -> 3		frame_00000285_obj_002.jpg -> 0		frame_00000287_obj_002.jpg -> 3
frame_00000288_obj_002.jpg -> 3		frame_00000289_obj_002.jpg -> 3		frame_00000290_obj_002.jpg -> 3		frame_00000291_obj_002.jpg -> 3
frame_00000292_obj_002.jpg -> 3		frame_00000293_obj_002.jpg -> 3		frame_00000294_obj_002.jpg -> 3		frame_00000295_obj_002.jpg -> 3
frame_00000296_obj_002.jpg -> 4		frame_00000297_obj_002.jpg -> 4		frame_00000298_obj_002.jpg -> 4		frame_00000299_obj_002.jpg -> 4
frame_00000300_obj_002.jpg -> 4		frame_00000301_obj_002.jpg -> 4		frame_00000302_obj_002.jpg -> 4		frame_00000303_obj_002.jpg -> 4
frame_00000304_obj_002.jpg -> 4		frame_00000305_obj_001.jpg -> 0		frame_00000305_obj_002.jpg -> 4		frame_00000306_obj_002.jpg -> 4
frame_00000307_obj_002.jpg -> 0		frame_00000307_obj_003.jpg -> 4		frame_00000308_obj_002.jpg -> 0		frame_00000308_obj_003.jpg -> 4
frame_00000309_obj_002.jpg -> 0		frame_00000309_obj_003.jpg -> 4		frame_00000310_obj_002.jpg -> 0		frame_00000310_obj_003.jpg -> 4
frame_00000311_obj_002.jpg -> 0		frame_00000311_obj_003.jpg -> 4		frame_00000312_obj_001.jpg -> 0		frame_00000312_obj_002.jpg -> 4
frame_00000313_obj_001.jpg -> 0		frame_00000313_obj_002.jpg -> 4		frame_00000314_obj_001.jpg -> 0		frame_00000314_obj_003.jpg -> 4
frame_00000315_obj_001.jpg -> 0		frame_00000315_obj_003.jpg -> 4		frame_00000316_obj_002.jpg -> 0		frame_00000316_obj_003.jpg -> 4
frame_00000317_obj_003.jpg -> 4		frame_00000318_obj_001.jpg -> 2		frame_00000318_obj_002.jpg -> 3		frame_00000318_obj_003.jpg -> 4
frame_00000319_obj_002.jpg -> 3		frame_00000319_obj_003.jpg -> 4		frame_00000320_obj_001.jpg -> 4		frame_00000320_obj_003.jpg -> 3
frame_00000321_obj_002.jpg -> 3		frame_00000321_obj_003.jpg -> 1		frame_00000322_obj_002.jpg -> 1		frame_00000322_obj_004.jpg -> 3
frame_00000323_obj_002.jpg -> 1		frame_00000323_obj_004.jpg -> 3		frame_00000324_obj_001.jpg -> 1		frame_00000324_obj_003.jpg -> 3
frame_00000325_obj_001.jpg -> 1		frame_00000325_obj_002.jpg -> 1		frame_00000325_obj_003.jpg -> 3		frame_00000326_obj_001.jpg -> 1
frame_00000326_obj_002.jpg -> 0		frame_00000326_obj_003.jpg -> 3		frame_00000327_obj_001.jpg -> 1		frame_00000327_obj_002.jpg -> 1
frame_00000327_obj_003.jpg -> 3		frame_00000328_obj_001.jpg -> 1		frame_00000328_obj_002.jpg -> 1		frame_00000328_obj_003.jpg -> 3
frame_00000329_obj_001.jpg -> 1		frame_00000329_obj_002.jpg -> 1		frame_00000329_obj_003.jpg -> 3		frame_00000330_obj_001.jpg -> 1
frame_00000330_obj_002.jpg -> 1		frame_00000330_obj_003.jpg -> 3		frame_00000331_obj_001.jpg -> 1		frame_00000331_obj_002.jpg -> 1
frame_00000331_obj_003.jpg -> 3		frame_00000332_obj_001.jpg -> 1		frame_00000332_obj_002.jpg -> 1		frame_00000332_obj_003.jpg -> 3
frame_00000333_obj_001.jpg -> 1		frame_00000333_obj_002.jpg -> 1		frame_00000333_obj_003.jpg -> 3		frame_00000334_obj_001.jpg -> 1
frame_00000334_obj_002.jpg -> 1		frame_00000334_obj_003.jpg -> 0		frame_00000335_obj_001.jpg -> 1		frame_00000335_obj_002.jpg -> 1
frame_00000335_obj_003.jpg -> 0		frame_00000336_obj_001.jpg -> 1		frame_00000336_obj_002.jpg -> 1		frame_00000336_obj_003.jpg -> 0
frame_00000337_obj_001.jpg -> 1		frame_00000337_obj_002.jpg -> 0		frame_00000338_obj_001.jpg -> 1		frame_00000338_obj_002.jpg -> 1
frame_00000338_obj_003.jpg -> 0		frame_00000339_obj_002.jpg -> 0		frame_00000340_obj_001.jpg -> 1		frame_00000340_obj_002.jpg -> 3
frame_00000340_obj_003.jpg -> 0		frame_00000341_obj_001.jpg -> 1		frame_00000341_obj_002.jpg -> 0		frame_00000342_obj_001.jpg -> 1
frame_00000342_obj_002.jpg -> 0		frame_00000343_obj_001.jpg -> 1		frame_00000343_obj_002.jpg -> 0		frame_00000344_obj_001.jpg -> 1
frame_00000344_obj_002.jpg -> 1		frame_00000344_obj_003.jpg -> 0		frame_00000345_obj_001.jpg -> 1		frame_00000345_obj_002.jpg -> 1
frame_00000345_obj_003.jpg -> 0		frame_00000346_obj_002.jpg -> 1		frame_00000346_obj_003.jpg -> 0		frame_00000347_obj_003.jpg -> 0
frame_00000348_obj_003.jpg -> 0		frame_00000349_obj_003.jpg -> 0		frame_00000350_obj_003.jpg -> 0		frame_00000351_obj_002.jpg -> 0
frame_00000352_obj_002.jpg -> 0		frame_00000353_obj_002.jpg -> 0		frame_00000354_obj_002.jpg -> 0		frame_00000355_obj_002.jpg -> 0
frame_00000356_obj_002.jpg -> 0		frame_00000357_obj_002.jpg -> 0		frame_00000358_obj_002.jpg -> 0		frame_00000359_obj_002.jpg -> 0
frame_00000360_obj_002.jpg -> 0		frame_00000361_obj_002.jpg -> 0		frame_00000362_obj_002.jpg -> 0		frame_00000363_obj_002.jpg -> 0
frame_00000364_obj_002.jpg -> 0		frame_00000365_obj_002.jpg -> 0		frame_00000366_obj_002.jpg -> 0		frame_00000367_obj_002.jpg -> 0
frame_00000368_obj_002.jpg -> 0		frame_00000369_obj_002.jpg -> 0		frame_00000370_obj_002.jpg -> 0		frame_00000371_obj_002.jpg -> 0
frame_00000372_obj_002.jpg -> 0		frame_00000373_obj_002.jpg -> 0		frame_00000374_obj_002.jpg -> 0		frame_00000375_obj_002.jpg -> 0
frame_00000376_obj_002.jpg -> 0		frame_00000377_obj_002.jpg -> 0		frame_00000378_obj_002.jpg -> 0		frame_00000379_obj_002.jpg -> 0
frame_00000380_obj_002.jpg -> 0		frame_00000381_obj_002.jpg -> 0		frame_00000382_obj_002.jpg -> 0		frame_00000383_obj_002.jpg -> 0
frame_00000384_obj_003.jpg -> 0		frame_00000384_obj_004.jpg -> 3		frame_00000385_obj_003.jpg -> 0		frame_00000385_obj_004.jpg -> 3
frame_00000386_obj_003.jpg -> 0		frame_00000386_obj_004.jpg -> 3		frame_00000387_obj_002.jpg -> 0		frame_00000387_obj_004.jpg -> 3
frame_00000388_obj_003.jpg -> 0		frame_00000388_obj_004.jpg -> 3		frame_00000389_obj_005.jpg -> 0		frame_00000389_obj_006.jpg -> 3
frame_00000390_obj_002.jpg -> 0		frame_00000390_obj_004.jpg -> 3		frame_00000391_obj_003.jpg -> 0		frame_00000391_obj_005.jpg -> 3
frame_00000392_obj_001.jpg -> 0		frame_00000392_obj_003.jpg -> 3		frame_00000393_obj_002.jpg -> 0		frame_00000393_obj_004.jpg -> 3
frame_00000394_obj_001.jpg -> 0		frame_00000394_obj_003.jpg -> 3		frame_00000395_obj_002.jpg -> 0		frame_00000395_obj_004.jpg -> 3
frame_00000396_obj_002.jpg -> 0		frame_00000396_obj_004.jpg -> 3		frame_00000397_obj_002.jpg -> 0		frame_00000397_obj_004.jpg -> 3
frame_00000398_obj_001.jpg -> 0		frame_00000398_obj_003.jpg -> 3		frame_00000399_obj_002.jpg -> 0		frame_00000399_obj_004.jpg -> 3
frame_00000400_obj_002.jpg -> 0		frame_00000400_obj_004.jpg -> 3		frame_00000401_obj_002.jpg -> 0		frame_00000401_obj_004.jpg -> 3
frame_00000402_obj_003.jpg -> 0		frame_00000402_obj_004.jpg -> 3		frame_00000403_obj_003.jpg -> 0		frame_00000403_obj_004.jpg -> 3
frame_00000404_obj_002.jpg -> 0		frame_00000404_obj_004.jpg -> 3		frame_00000405_obj_002.jpg -> 0		frame_00000405_obj_003.jpg -> 3
frame_00000406_obj_001.jpg -> 0		frame_00000406_obj_003.jpg -> 3		frame_00000407_obj_001.jpg -> 0		frame_00000407_obj_003.jpg -> 3
frame_00000408_obj_001.jpg -> 0		frame_00000408_obj_003.jpg -> 3		frame_00000409_obj_001.jpg -> 0		frame_00000409_obj_003.jpg -> 3
frame_00000410_obj_002.jpg -> 0		frame_00000410_obj_003.jpg -> 3		frame_00000411_obj_004.jpg -> 3		frame_00000412_obj_005.jpg -> 3
frame_00000413_obj_002.jpg -> 3		frame_00000414_obj_002.jpg -> 3		frame_00000415_obj_003.jpg -> 4		frame_00000416_obj_004.jpg -> 4
frame_00000417_obj_005.jpg -> 2		frame_00000417_obj_006.jpg -> 4		frame_00000418_obj_004.jpg -> 4		frame_00000419_obj_004.jpg -> 4
frame_00000420_obj_004.jpg -> 2		frame_00000420_obj_005.jpg -> 4		frame_00000421_obj_004.jpg -> 4		frame_00000422_obj_001.jpg -> 2
frame_00000422_obj_003.jpg -> 3		frame_00000422_obj_004.jpg -> 4		frame_00000423_obj_003.jpg -> 3		frame_00000423_obj_004.jpg -> 4
frame_00000424_obj_003.jpg -> 3		frame_00000424_obj_004.jpg -> 4		frame_00000425_obj_004.jpg -> 3		frame_00000425_obj_005.jpg -> 4
frame_00000426_obj_004.jpg -> 3		frame_00000426_obj_005.jpg -> 4		frame_00000427_obj_004.jpg -> 3		frame_00000427_obj_005.jpg -> 4
frame_00000428_obj_003.jpg -> 3		frame_00000428_obj_004.jpg -> 4		frame_00000429_obj_001.jpg -> 2		frame_00000429_obj_003.jpg -> 3
frame_00000429_obj_004.jpg -> 4		frame_00000430_obj_003.jpg -> 3		frame_00000430_obj_004.jpg -> 4		frame_00000431_obj_001.jpg -> 2
frame_00000431_obj_003.jpg -> 3		frame_00000431_obj_004.jpg -> 4		frame_00000432_obj_002.jpg -> 2		frame_00000432_obj_005.jpg -> 3
frame_00000432_obj_006.jpg -> 4		frame_00000433_obj_001.jpg -> 2		frame_00000433_obj_003.jpg -> 3		frame_00000433_obj_004.jpg -> 4
frame_00000434_obj_001.jpg -> 2		frame_00000434_obj_003.jpg -> 3		frame_00000434_obj_004.jpg -> 4		frame_00000435_obj_001.jpg -> 2
frame_00000435_obj_003.jpg -> 3		frame_00000435_obj_004.jpg -> 4		frame_00000436_obj_001.jpg -> 2		frame_00000436_obj_003.jpg -> 3
frame_00000436_obj_004.jpg -> 3		frame_00000437_obj_001.jpg -> 2		frame_00000437_obj_004.jpg -> 3		frame_00000437_obj_005.jpg -> 3
frame_00000438_obj_003.jpg -> 3		frame_00000438_obj_004.jpg -> 4		frame_00000439_obj_001.jpg -> 2		frame_00000439_obj_003.jpg -> 3
frame_00000439_obj_004.jpg -> 4		frame_00000440_obj_001.jpg -> 2		frame_00000440_obj_003.jpg -> 3		frame_00000440_obj_004.jpg -> 4
frame_00000441_obj_001.jpg -> 2		frame_00000441_obj_003.jpg -> 3		frame_00000441_obj_004.jpg -> 4		frame_00000442_obj_001.jpg -> 2
frame_00000442_obj_003.jpg -> 3		frame_00000442_obj_004.jpg -> 4		frame_00000443_obj_001.jpg -> 2		frame_00000443_obj_003.jpg -> 3
frame_00000443_obj_004.jpg -> 4		frame_00000444_obj_001.jpg -> 2		frame_00000444_obj_003.jpg -> 3		frame_00000444_obj_004.jpg -> 4
frame_00000445_obj_001.jpg -> 2		frame_00000445_obj_003.jpg -> 3		frame_00000445_obj_004.jpg -> 4		frame_00000446_obj_001.jpg -> 2
frame_00000446_obj_003.jpg -> 3		frame_00000446_obj_004.jpg -> 4		frame_00000447_obj_001.jpg -> 2		frame_00000447_obj_003.jpg -> 3
frame_00000447_obj_004.jpg -> 4		frame_00000448_obj_001.jpg -> 2		frame_00000448_obj_003.jpg -> 3		frame_00000448_obj_004.jpg -> 4
frame_00000449_obj_001.jpg -> 2		frame_00000449_obj_003.jpg -> 4		frame_00000450_obj_001.jpg -> 2		frame_00000450_obj_003.jpg -> 4
frame_00000451_obj_001.jpg -> 2		frame_00000451_obj_003.jpg -> 4		frame_00000452_obj_001.jpg -> 2		frame_00000452_obj_003.jpg -> 4
frame_00000453_obj_001.jpg -> 2		frame_00000453_obj_003.jpg -> 3		frame_00000453_obj_004.jpg -> 4		frame_00000454_obj_001.jpg -> 2
frame_00000454_obj_003.jpg -> 3		frame_00000454_obj_004.jpg -> 4		frame_00000455_obj_001.jpg -> 2		frame_00000455_obj_003.jpg -> 3
frame_00000455_obj_004.jpg -> 4		frame_00000456_obj_001.jpg -> 2		frame_00000456_obj_003.jpg -> 3		frame_00000456_obj_004.jpg -> 4
frame_00000457_obj_001.jpg -> 2		frame_00000457_obj_003.jpg -> 3		frame_00000457_obj_004.jpg -> 4		frame_00000458_obj_001.jpg -> 2
frame_00000458_obj_003.jpg -> 3		frame_00000458_obj_004.jpg -> 4		frame_00000459_obj_001.jpg -> 2		frame_00000459_obj_003.jpg -> 3
frame_00000459_obj_004.jpg -> 4		frame_00000460_obj_001.jpg -> 2		frame_00000460_obj_002.jpg -> 3		frame_00000460_obj_003.jpg -> 2
frame_00000460_obj_005.jpg -> 1		frame_00000461_obj_001.jpg -> 3		frame_00000461_obj_003.jpg -> 3		frame_00000461_obj_004.jpg -> 1
frame_00000462_obj_001.jpg -> 3		frame_00000462_obj_003.jpg -> 3		frame_00000462_obj_004.jpg -> 1		frame_00000463_obj_001.jpg -> 3
frame_00000463_obj_003.jpg -> 1		frame_00000464_obj_002.jpg -> 3		frame_00000464_obj_003.jpg -> 3		frame_00000464_obj_005.jpg -> 1
frame_00000465_obj_001.jpg -> 3		frame_00000465_obj_002.jpg -> 2		frame_00000465_obj_004.jpg -> 1		frame_00000466_obj_001.jpg -> 2
frame_00000466_obj_003.jpg -> 1		frame_00000467_obj_001.jpg -> 3		frame_00000467_obj_002.jpg -> 0		frame_00000467_obj_004.jpg -> 1
frame_00000468_obj_001.jpg -> 3		frame_00000468_obj_002.jpg -> 0		frame_00000468_obj_004.jpg -> 1		frame_00000469_obj_002.jpg -> 3
frame_00000469_obj_005.jpg -> 1		frame_00000470_obj_001.jpg -> 3		frame_00000470_obj_004.jpg -> 1		frame_00000471_obj_001.jpg -> 3
frame_00000471_obj_002.jpg -> 1		frame_00000472_obj_001.jpg -> 3		frame_00000472_obj_003.jpg -> 1		frame_00000473_obj_002.jpg -> 1
frame_00000474_obj_003.jpg -> 1		frame_00000475_obj_001.jpg -> 2		frame_00000475_obj_003.jpg -> 1		frame_00000476_obj_003.jpg -> 1
frame_00000477_obj_004.jpg -> 1		frame_00000478_obj_002.jpg -> 4		frame_00000478_obj_005.jpg -> 1		frame_00000479_obj_002.jpg -> 4
frame_00000479_obj_005.jpg -> 1		frame_00000480_obj_002.jpg -> 4		frame_00000480_obj_004.jpg -> 1		frame_00000481_obj_002.jpg -> 4
frame_00000481_obj_004.jpg -> 1		frame_00000482_obj_002.jpg -> 4		frame_00000482_obj_004.jpg -> 1		frame_00000483_obj_002.jpg -> 4
frame_00000483_obj_004.jpg -> 1		frame_00000484_obj_002.jpg -> 4		frame_00000484_obj_004.jpg -> 1		frame_00000485_obj_001.jpg -> 2
frame_00000485_obj_002.jpg -> 4		frame_00000485_obj_005.jpg -> 1		frame_00000486_obj_002.jpg -> 4		frame_00000486_obj_005.jpg -> 1
frame_00000487_obj_002.jpg -> 4		frame_00000487_obj_004.jpg -> 1		frame_00000488_obj_002.jpg -> 1		frame_00000488_obj_004.jpg -> 1
frame_00000489_obj_002.jpg -> 1		frame_00000489_obj_004.jpg -> 1		frame_00000490_obj_002.jpg -> 1		frame_00000490_obj_004.jpg -> 1
frame_00000491_obj_001.jpg -> 1		frame_00000491_obj_003.jpg -> 1		frame_00000492_obj_001.jpg -> 1		frame_00000492_obj_003.jpg -> 1
frame_00000493_obj_001.jpg -> 1		frame_00000493_obj_003.jpg -> 1		frame_00000494_obj_001.jpg -> 2		frame_00000494_obj_002.jpg -> 1
frame_00000494_obj_004.jpg -> 1		frame_00000495_obj_001.jpg -> 2		frame_00000495_obj_002.jpg -> 1		frame_00000495_obj_004.jpg -> 1
frame_00000496_obj_001.jpg -> 2		frame_00000496_obj_002.jpg -> 1		frame_00000496_obj_004.jpg -> 1		frame_00000497_obj_001.jpg -> 2
frame_00000497_obj_002.jpg -> 1		frame_00000497_obj_004.jpg -> 1		frame_00000498_obj_001.jpg -> 1		frame_00000498_obj_003.jpg -> 1
frame_00000499_obj_001.jpg -> 1		frame_00000499_obj_003.jpg -> 1		frame_00000500_obj_001.jpg -> 1		frame_00000500_obj_003.jpg -> 1
frame_00000501_obj_001.jpg -> 2		frame_00000501_obj_002.jpg -> 1		frame_00000501_obj_004.jpg -> 1		frame_00000502_obj_002.jpg -> 1
frame_00000502_obj_004.jpg -> 1		frame_00000503_obj_001.jpg -> 1		frame_00000503_obj_003.jpg -> 1		frame_00000504_obj_001.jpg -> 1
frame_00000504_obj_003.jpg -> 1		frame_00000505_obj_001.jpg -> 1		frame_00000505_obj_003.jpg -> 1		frame_00000506_obj_001.jpg -> 1
frame_00000506_obj_003.jpg -> 1		frame_00000507_obj_001.jpg -> 1		frame_00000507_obj_003.jpg -> 1		frame_00000508_obj_001.jpg -> 1
frame_00000508_obj_003.jpg -> 1		frame_00000509_obj_001.jpg -> 1		frame_00000509_obj_003.jpg -> 1		frame_00000510_obj_001.jpg -> 1
frame_00000510_obj_003.jpg -> 1		frame_00000511_obj_002.jpg -> 1		frame_00000512_obj_002.jpg -> 1		frame_00000513_obj_002.jpg -> 1
frame_00000514_obj_003.jpg -> 1		frame_00000515_obj_002.jpg -> 1		frame_00000516_obj_001.jpg -> 1		frame_00000517_obj_002.jpg -> 1
frame_00000518_obj_001.jpg -> 1		frame_00000539_obj_001.jpg -> 3		frame_00000540_obj_001.jpg -> 3		frame_00000547_obj_001.jpg -> 3
frame_00000550_obj_002.jpg -> 3		frame_00000551_obj_001.jpg -> 3		frame_00000552_obj_001.jpg -> 3		frame_00000553_obj_001.jpg -> 3
frame_00000554_obj_001.jpg -> 3		frame_00000555_obj_001.jpg -> 3		frame_00000556_obj_001.jpg -> 3		frame_00000557_obj_002.jpg -> 3
frame_00000563_obj_001.jpg -> 3		frame_00000564_obj_001.jpg -> 3		frame_00000565_obj_001.jpg -> 3		frame_00000566_obj_001.jpg -> 3
frame_00000567_obj_001.jpg -> 3		frame_00000568_obj_001.jpg -> 3		frame_00000569_obj_001.jpg -> 3		frame_00000570_obj_001.jpg -> 3
frame_00000571_obj_001.jpg -> 3		frame_00000572_obj_001.jpg -> 4		frame_00000573_obj_001.jpg -> 4		frame_00000574_obj_001.jpg -> 4
frame_00000575_obj_001.jpg -> 4		frame_00000576_obj_001.jpg -> 4		frame_00000577_obj_001.jpg -> 4		frame_00000578_obj_001.jpg -> 4
frame_00000579_obj_001.jpg -> 4		frame_00000580_obj_001.jpg -> 4		frame_00000581_obj_001.jpg -> 4		frame_00000582_obj_001.jpg -> 4
frame_00000583_obj_001.jpg -> 4		frame_00000584_obj_001.jpg -> 4		frame_00000585_obj_001.jpg -> 4		frame_00000586_obj_001.jpg -> 4
frame_00000587_obj_001.jpg -> 4		frame_00000588_obj_001.jpg -> 4		frame_00000589_obj_001.jpg -> 4		frame_00000590_obj_001.jpg -> 4
frame_00000591_obj_002.jpg -> 4		frame_00000592_obj_001.jpg -> 4		frame_00000593_obj_001.jpg -> 4		frame_00000594_obj_001.jpg -> 4
frame_00000595_obj_003.jpg -> 4		frame_00000596_obj_003.jpg -> 4		frame_00000597_obj_003.jpg -> 4		frame_00000598_obj_003.jpg -> 4
frame_00000599_obj_002.jpg -> 4		frame_00000600_obj_002.jpg -> 4		frame_00000601_obj_002.jpg -> 4		frame_00000602_obj_003.jpg -> 4
frame_00000603_obj_003.jpg -> 4		frame_00000604_obj_003.jpg -> 4		frame_00000605_obj_002.jpg -> 4		frame_00000606_obj_001.jpg -> 4
frame_00000607_obj_003.jpg -> 4		frame_00000608_obj_002.jpg -> 4		frame_00000609_obj_002.jpg -> 4		frame_00000610_obj_002.jpg -> 4
frame_00000611_obj_004.jpg -> 4		frame_00000612_obj_002.jpg -> 2		frame_00000612_obj_004.jpg -> 4		frame_00000613_obj_004.jpg -> 4
frame_00000614_obj_002.jpg -> 4		frame_00000615_obj_002.jpg -> 4		frame_00000616_obj_002.jpg -> 4		frame_00000617_obj_002.jpg -> 4
frame_00000618_obj_002.jpg -> 4		frame_00000619_obj_003.jpg -> 4		frame_00000620_obj_003.jpg -> 4		frame_00000621_obj_002.jpg -> 1
frame_00000623_obj_001.jpg -> 4		frame_00000624_obj_001.jpg -> 4		frame_00000625_obj_001.jpg -> 4		frame_00000626_obj_001.jpg -> 4
frame_00000626_obj_005.jpg -> 3		frame_00000627_obj_001.jpg -> 4		frame_00000628_obj_006.jpg -> 3		frame_00000629_obj_004.jpg -> 2
frame_00000630_obj_005.jpg -> 2		frame_00000630_obj_007.jpg -> 3		frame_00000631_obj_005.jpg -> 3		frame_00000632_obj_004.jpg -> 3
frame_00000633_obj_005.jpg -> 3		frame_00000640_obj_005.jpg -> 3		frame_00000641_obj_006.jpg -> 3		frame_00000642_obj_006.jpg -> 3
frame_00000643_obj_002.jpg -> 2		frame_00000643_obj_007.jpg -> 3		frame_00000644_obj_008.jpg -> 3		frame_00000645_obj_006.jpg -> 3
frame_00000646_obj_009.jpg -> 3		frame_00000647_obj_002.jpg -> 1		frame_00000647_obj_008.jpg -> 3		frame_00000648_obj_008.jpg -> 3
frame_00000649_obj_007.jpg -> 3		frame_00000650_obj_001.jpg -> 2		frame_00000650_obj_007.jpg -> 3		frame_00000651_obj_006.jpg -> 3
frame_00000652_obj_007.jpg -> 3		frame_00000653_obj_007.jpg -> 3		frame_00000654_obj_007.jpg -> 3		frame_00000655_obj_006.jpg -> 3
frame_00000656_obj_006.jpg -> 3		frame_00000657_obj_007.jpg -> 3		frame_00000658_obj_006.jpg -> 4		frame_00000659_obj_006.jpg -> 4
frame_00000660_obj_006.jpg -> 4		frame_00000661_obj_006.jpg -> 4		frame_00000662_obj_003.jpg -> 2		frame_00000662_obj_006.jpg -> 4
frame_00000663_obj_006.jpg -> 4		frame_00000664_obj_006.jpg -> 4		frame_00000665_obj_001.jpg -> 2		frame_00000665_obj_006.jpg -> 4
frame_00000666_obj_006.jpg -> 4		frame_00000667_obj_005.jpg -> 4		frame_00000668_obj_005.jpg -> 4		frame_00000669_obj_007.jpg -> 4
frame_00000670_obj_006.jpg -> 4		frame_00000671_obj_006.jpg -> 4		frame_00000672_obj_007.jpg -> 4		frame_00000673_obj_006.jpg -> 4
frame_00000674_obj_005.jpg -> 4		frame_00000675_obj_005.jpg -> 4		frame_00000676_obj_005.jpg -> 4		frame_00000677_obj_005.jpg -> 4
frame_00000678_obj_005.jpg -> 4		frame_00000679_obj_006.jpg -> 4		frame_00000680_obj_004.jpg -> 4		frame_00000681_obj_004.jpg -> 4
frame_00000682_obj_005.jpg -> 4		frame_00000683_obj_005.jpg -> 4		frame_00000684_obj_004.jpg -> 4		frame_00000685_obj_004.jpg -> 4
frame_00000686_obj_005.jpg -> 4		frame_00000687_obj_004.jpg -> 4		frame_00000688_obj_004.jpg -> 4		frame_00000689_obj_004.jpg -> 4
frame_00000690_obj_004.jpg -> 4		frame_00000691_obj_004.jpg -> 4		frame_00000692_obj_004.jpg -> 4		frame_00000693_obj_004.jpg -> 4
frame_00000694_obj_004.jpg -> 4		frame_00000695_obj_004.jpg -> 4		frame_00000696_obj_004.jpg -> 4		frame_00000697_obj_004.jpg -> 4
frame_00000698_obj_006.jpg -> 4		frame_00000699_obj_006.jpg -> 4		frame_00000700_obj_005.jpg -> 4		frame_00000701_obj_004.jpg -> 4
frame_00000702_obj_002.jpg -> 2		frame_00000702_obj_004.jpg -> 4		frame_00000703_obj_005.jpg -> 4		frame_00000704_obj_004.jpg -> 4
frame_00000705_obj_004.jpg -> 1		frame_00000706_obj_007.jpg -> 1		frame_00000707_obj_006.jpg -> 1		frame_00000708_obj_005.jpg -> 1
frame_00000709_obj_004.jpg -> 1		frame_00000709_obj_006.jpg -> 1		frame_00000710_obj_005.jpg -> 1		frame_00000711_obj_003.jpg -> 2
frame_00000711_obj_006.jpg -> 1		frame_00000712_obj_005.jpg -> 1		frame_00000713_obj_005.jpg -> 1		frame_00000736_obj_005.jpg -> 3
frame_00000737_obj_007.jpg -> 3		frame_00000738_obj_006.jpg -> 3		frame_00000739_obj_005.jpg -> 3		frame_00000740_obj_005.jpg -> 3
frame_00000741_obj_005.jpg -> 3		frame_00000742_obj_005.jpg -> 3		frame_00000743_obj_006.jpg -> 3		frame_00000744_obj_005.jpg -> 3
frame_00000745_obj_006.jpg -> 3		frame_00000746_obj_005.jpg -> 3		frame_00000747_obj_005.jpg -> 3		frame_00000748_obj_005.jpg -> 3
frame_00000749_obj_006.jpg -> 3		frame_00000750_obj_005.jpg -> 3		frame_00000751_obj_005.jpg -> 3		frame_00000752_obj_005.jpg -> 3
frame_00000753_obj_005.jpg -> 3		frame_00000754_obj_005.jpg -> 3		frame_00000755_obj_005.jpg -> 3		frame_00000756_obj_004.jpg -> 3
frame_00000757_obj_004.jpg -> 3		frame_00000758_obj_004.jpg -> 3		frame_00000759_obj_003.jpg -> 3		frame_00000760_obj_003.jpg -> 3
frame_00000761_obj_004.jpg -> 3		frame_00000762_obj_003.jpg -> 3		frame_00000763_obj_003.jpg -> 4		frame_00000764_obj_004.jpg -> 4
frame_00000765_obj_003.jpg -> 4		frame_00000766_obj_002.jpg -> 4		frame_00000767_obj_001.jpg -> 4		frame_00000768_obj_001.jpg -> 4
frame_00000769_obj_001.jpg -> 4		frame_00000770_obj_001.jpg -> 4		frame_00000771_obj_001.jpg -> 4		frame_00000772_obj_001.jpg -> 4
frame_00000773_obj_001.jpg -> 4		frame_00000774_obj_001.jpg -> 4		frame_00000775_obj_001.jpg -> 4		frame_00000776_obj_001.jpg -> 4
frame_00000777_obj_001.jpg -> 2		frame_00000777_obj_002.jpg -> 4		frame_00000778_obj_001.jpg -> 2		frame_00000778_obj_002.jpg -> 4
frame_00000779_obj_001.jpg -> 2		frame_00000779_obj_002.jpg -> 4		frame_00000780_obj_001.jpg -> 4		frame_00000781_obj_001.jpg -> 4
frame_00000782_obj_001.jpg -> 2		frame_00000782_obj_002.jpg -> 4		frame_00000783_obj_002.jpg -> 4		frame_00000784_obj_001.jpg -> 2
frame_00000784_obj_002.jpg -> 4		frame_00000785_obj_001.jpg -> 2		frame_00000785_obj_002.jpg -> 4		frame_00000786_obj_001.jpg -> 2
frame_00000786_obj_002.jpg -> 4		frame_00000787_obj_001.jpg -> 2		frame_00000787_obj_002.jpg -> 4		frame_00000788_obj_001.jpg -> 2
frame_00000788_obj_002.jpg -> 4		frame_00000789_obj_002.jpg -> 1		frame_00000790_obj_001.jpg -> 2		frame_00000790_obj_002.jpg -> 1
frame_00000791_obj_001.jpg -> 1		frame_00000792_obj_001.jpg -> 1		frame_00000793_obj_001.jpg -> 4		frame_00000794_obj_001.jpg -> 1
frame_00000795_obj_001.jpg -> 1		frame_00000796_obj_001.jpg -> 1		frame_00000796_obj_002.jpg -> 3		frame_00000797_obj_001.jpg -> 1
frame_00000797_obj_002.jpg -> 3		frame_00000798_obj_001.jpg -> 1		frame_00000798_obj_002.jpg -> 3		frame_00000799_obj_001.jpg -> 1
frame_00000799_obj_002.jpg -> 3		frame_00000800_obj_001.jpg -> 1		frame_00000800_obj_002.jpg -> 3		frame_00000801_obj_001.jpg -> 1
frame_00000801_obj_002.jpg -> 3		frame_00000802_obj_001.jpg -> 1		frame_00000803_obj_001.jpg -> 1		frame_00000804_obj_001.jpg -> 1
frame_00000804_obj_002.jpg -> 3		frame_00000805_obj_001.jpg -> 1		frame_00000805_obj_002.jpg -> 3		frame_00000806_obj_001.jpg -> 1
frame_00000806_obj_002.jpg -> 3		frame_00000807_obj_001.jpg -> 1		frame_00000807_obj_002.jpg -> 3		frame_00000808_obj_001.jpg -> 1
frame_00000808_obj_002.jpg -> 3		frame_00000809_obj_001.jpg -> 1		frame_00000809_obj_002.jpg -> 4		frame_00000810_obj_001.jpg -> 1
frame_00000810_obj_002.jpg -> 4		frame_00000811_obj_001.jpg -> 1		frame_00000811_obj_002.jpg -> 4		frame_00000812_obj_001.jpg -> 1
frame_00000812_obj_002.jpg -> 4		frame_00000813_obj_001.jpg -> 1		frame_00000813_obj_002.jpg -> 4		frame_00000814_obj_001.jpg -> 1
frame_00000814_obj_002.jpg -> 4		frame_00000815_obj_001.jpg -> 1		frame_00000815_obj_002.jpg -> 4		frame_00000816_obj_001.jpg -> 1
frame_00000816_obj_002.jpg -> 4		frame_00000817_obj_001.jpg -> 1		frame_00000817_obj_002.jpg -> 4		frame_00000818_obj_001.jpg -> 1
frame_00000818_obj_002.jpg -> 4		frame_00000819_obj_001.jpg -> 4		frame_00000820_obj_002.jpg -> 4		frame_00000821_obj_002.jpg -> 4
frame_00000822_obj_001.jpg -> 4		frame_00000823_obj_001.jpg -> 4		frame_00000824_obj_001.jpg -> 4		frame_00000825_obj_001.jpg -> 1
frame_00000826_obj_001.jpg -> 1		frame_00000827_obj_001.jpg -> 1		frame_00000828_obj_001.jpg -> 1		frame_00000829_obj_001.jpg -> 1
frame_00000830_obj_001.jpg -> 1		frame_00000831_obj_001.jpg -> 1		frame_00000832_obj_001.jpg -> 1		frame_00000833_obj_001.jpg -> 1
frame_00000834_obj_001.jpg -> 1		frame_00000835_obj_001.jpg -> 1		frame_00000836_obj_003.jpg -> 1		frame_00000837_obj_002.jpg -> 1
frame_00000838_obj_002.jpg -> 1		frame_00000839_obj_001.jpg -> 2		frame_00000839_obj_002.jpg -> 1		frame_00000840_obj_001.jpg -> 2
frame_00000840_obj_002.jpg -> 1		frame_00000841_obj_001.jpg -> 2		frame_00000841_obj_002.jpg -> 1		frame_00000842_obj_002.jpg -> 1
frame_00000843_obj_003.jpg -> 1		frame_00000844_obj_004.jpg -> 1		frame_00000845_obj_002.jpg -> 1		frame_00000846_obj_002.jpg -> 1
frame_00000847_obj_002.jpg -> 1		frame_00000848_obj_002.jpg -> 1		frame_00000848_obj_003.jpg -> 2		frame_00000849_obj_002.jpg -> 1
frame_00000849_obj_003.jpg -> 2		frame_00000850_obj_002.jpg -> 1		frame_00000850_obj_003.jpg -> 2		frame_00000851_obj_001.jpg -> 1
frame_00000851_obj_003.jpg -> 2		frame_00000852_obj_001.jpg -> 1		frame_00000852_obj_003.jpg -> 2		frame_00000853_obj_001.jpg -> 1
frame_00000853_obj_003.jpg -> 2		frame_00000854_obj_002.jpg -> 1		frame_00000854_obj_003.jpg -> 2		frame_00000855_obj_002.jpg -> 1
frame_00000855_obj_003.jpg -> 2		frame_00000856_obj_002.jpg -> 1		frame_00000856_obj_003.jpg -> 2		frame_00000857_obj_002.jpg -> 1
frame_00000857_obj_003.jpg -> 2		frame_00000858_obj_002.jpg -> 1		frame_00000858_obj_003.jpg -> 2		frame_00000859_obj_002.jpg -> 1
frame_00000859_obj_004.jpg -> 2		frame_00000860_obj_002.jpg -> 1		frame_00000860_obj_003.jpg -> 2		frame_00000861_obj_001.jpg -> 1
frame_00000861_obj_002.jpg -> 2		frame_00000862_obj_001.jpg -> 1		frame_00000862_obj_002.jpg -> 2		frame_00000863_obj_001.jpg -> 1
frame_00000863_obj_002.jpg -> 2		frame_00000864_obj_001.jpg -> 2		frame_00000865_obj_001.jpg -> 2		frame_00000866_obj_001.jpg -> 2
frame_00000867_obj_001.jpg -> 2		frame_00000868_obj_003.jpg -> 1		frame_00000869_obj_001.jpg -> 1		frame_00000870_obj_001.jpg -> 1
frame_00000871_obj_001.jpg -> 1		frame_00000872_obj_001.jpg -> 1		frame_00000873_obj_001.jpg -> 1		frame_00000874_obj_001.jpg -> 1
frame_00000875_obj_001.jpg -> 1		frame_00000876_obj_001.jpg -> 1		frame_00000877_obj_001.jpg -> 1		frame_00000878_obj_001.jpg -> 1
frame_00000879_obj_001.jpg -> 1		frame_00000880_obj_001.jpg -> 1		frame_00000881_obj_001.jpg -> 1		frame_00000882_obj_001.jpg -> 1
frame_00000883_obj_001.jpg -> 1		frame_00000884_obj_001.jpg -> 1		frame_00000885_obj_001.jpg -> 1		frame_00000886_obj_001.jpg -> 1
frame_00000887_obj_001.jpg -> 1		frame_00000888_obj_001.jpg -> 1		frame_00000889_obj_001.jpg -> 1		frame_00000890_obj_001.jpg -> 1
frame_00000891_obj_001.jpg -> 1		frame_00000892_obj_001.jpg -> 1		frame_00000893_obj_001.jpg -> 1		frame_00000894_obj_001.jpg -> 1
frame_00000895_obj_001.jpg -> 1		frame_00000896_obj_001.jpg -> 1		frame_00000897_obj_001.jpg -> 1		frame_00000898_obj_001.jpg -> 1
frame_00000899_obj_001.jpg -> 1		frame_00000900_obj_001.jpg -> 1		frame_00000901_obj_001.jpg -> 1		frame_00000902_obj_001.jpg -> 1
frame_00000903_obj_001.jpg -> 1		frame_00000904_obj_001.jpg -> 1		frame_00000905_obj_001.jpg -> 1		frame_00000906_obj_001.jpg -> 1
frame_00000907_obj_001.jpg -> 2		frame_00000907_obj_002.jpg -> 1		frame_00000908_obj_001.jpg -> 2		frame_00000908_obj_002.jpg -> 1
frame_00000909_obj_002.jpg -> 1		frame_00000910_obj_001.jpg -> 2		frame_00000910_obj_002.jpg -> 1		frame_00000911_obj_001.jpg -> 1
frame_00000912_obj_001.jpg -> 1		frame_00000913_obj_001.jpg -> 1		frame_00000914_obj_001.jpg -> 1		frame_00000915_obj_001.jpg -> 1
frame_00000916_obj_001.jpg -> 1		frame_00000917_obj_001.jpg -> 1		frame_00000918_obj_001.jpg -> 1		frame_00000919_obj_001.jpg -> 1
frame_00000920_obj_001.jpg -> 1		frame_00000953_obj_002.jpg -> 3		frame_00000954_obj_002.jpg -> 3		frame_00000955_obj_001.jpg -> 2
frame_00000955_obj_002.jpg -> 3		frame_00000956_obj_002.jpg -> 3		frame_00000957_obj_002.jpg -> 3		frame_00000958_obj_003.jpg -> 3
frame_00000959_obj_003.jpg -> 3		frame_00000960_obj_003.jpg -> 3		frame_00000961_obj_004.jpg -> 3		frame_00000962_obj_003.jpg -> 3
frame_00000963_obj_003.jpg -> 3		frame_00000964_obj_002.jpg -> 3		frame_00000965_obj_003.jpg -> 2		frame_00000965_obj_004.jpg -> 3
frame_00000966_obj_003.jpg -> 2		frame_00000966_obj_004.jpg -> 3		frame_00000967_obj_003.jpg -> 2		frame_00000967_obj_004.jpg -> 3
frame_00000968_obj_003.jpg -> 2		frame_00000968_obj_004.jpg -> 3		frame_00000969_obj_003.jpg -> 2		frame_00000969_obj_004.jpg -> 3
frame_00000970_obj_003.jpg -> 2		frame_00000970_obj_004.jpg -> 3		frame_00000971_obj_002.jpg -> 2		frame_00000971_obj_003.jpg -> 3
frame_00000972_obj_003.jpg -> 2		frame_00000972_obj_004.jpg -> 3		frame_00000973_obj_003.jpg -> 2		frame_00000973_obj_004.jpg -> 3
frame_00000974_obj_003.jpg -> 2		frame_00000974_obj_004.jpg -> 3		frame_00000975_obj_003.jpg -> 2		frame_00000975_obj_004.jpg -> 3
frame_00000976_obj_003.jpg -> 2		frame_00000976_obj_004.jpg -> 3		frame_00000977_obj_002.jpg -> 2		frame_00000977_obj_003.jpg -> 3
frame_00000978_obj_003.jpg -> 2		frame_00000978_obj_004.jpg -> 3		frame_00000979_obj_003.jpg -> 2		frame_00000979_obj_004.jpg -> 3
frame_00000980_obj_002.jpg -> 2		frame_00000980_obj_003.jpg -> 4		frame_00000981_obj_003.jpg -> 2		frame_00000981_obj_004.jpg -> 4
frame_00000982_obj_004.jpg -> 2		frame_00000982_obj_005.jpg -> 4		frame_00000983_obj_003.jpg -> 2		frame_00000983_obj_004.jpg -> 4
frame_00000984_obj_002.jpg -> 4		frame_00000984_obj_003.jpg -> 2		frame_00000985_obj_002.jpg -> 4		frame_00000985_obj_003.jpg -> 2
frame_00000986_obj_003.jpg -> 4		frame_00000986_obj_004.jpg -> 2		frame_00000987_obj_002.jpg -> 4		frame_00000987_obj_003.jpg -> 2
frame_00000988_obj_001.jpg -> 4		frame_00000988_obj_002.jpg -> 2		frame_00000989_obj_001.jpg -> 4		frame_00000989_obj_002.jpg -> 2
frame_00000990_obj_001.jpg -> 4		frame_00000990_obj_002.jpg -> 2		frame_00000991_obj_003.jpg -> 4		frame_00000991_obj_004.jpg -> 2
frame_00000992_obj_003.jpg -> 4		frame_00000992_obj_004.jpg -> 2		frame_00000993_obj_002.jpg -> 4		frame_00000993_obj_003.jpg -> 2
frame_00000994_obj_003.jpg -> 4		frame_00000994_obj_004.jpg -> 2		frame_00000995_obj_001.jpg -> 2		frame_00000995_obj_004.jpg -> 4
frame_00000995_obj_005.jpg -> 2		frame_00000996_obj_002.jpg -> 4		frame_00000996_obj_003.jpg -> 2		frame_00000997_obj_002.jpg -> 4
frame_00000997_obj_003.jpg -> 2		frame_00000998_obj_002.jpg -> 4		frame_00000998_obj_003.jpg -> 2		frame_00000998_obj_004.jpg -> 3
frame_00000999_obj_003.jpg -> 4		frame_00000999_obj_004.jpg -> 2		frame_00000999_obj_005.jpg -> 3		frame_00001000_obj_001.jpg -> 4
frame_00001000_obj_003.jpg -> 4		frame_00001000_obj_004.jpg -> 3		frame_00001001_obj_002.jpg -> 4		frame_00001001_obj_004.jpg -> 2
frame_00001001_obj_005.jpg -> 3		frame_00001002_obj_002.jpg -> 4		frame_00001002_obj_004.jpg -> 2		frame_00001002_obj_005.jpg -> 3
frame_00001003_obj_001.jpg -> 2		frame_00001003_obj_002.jpg -> 4		frame_00001003_obj_004.jpg -> 2		frame_00001003_obj_005.jpg -> 3
frame_00001004_obj_002.jpg -> 4		frame_00001004_obj_004.jpg -> 2		frame_00001004_obj_005.jpg -> 3		frame_00001005_obj_002.jpg -> 4
frame_00001005_obj_004.jpg -> 2		frame_00001005_obj_005.jpg -> 3		frame_00001006_obj_001.jpg -> 4		frame_00001006_obj_003.jpg -> 4
frame_00001006_obj_004.jpg -> 3		frame_00001007_obj_001.jpg -> 1		frame_00001007_obj_004.jpg -> 4		frame_00001007_obj_005.jpg -> 3
frame_00001008_obj_001.jpg -> 4		frame_00001008_obj_004.jpg -> 4		frame_00001008_obj_005.jpg -> 3		frame_00001009_obj_001.jpg -> 4
frame_00001009_obj_004.jpg -> 4		frame_00001009_obj_005.jpg -> 3		frame_00001010_obj_001.jpg -> 4		frame_00001010_obj_004.jpg -> 2
frame_00001010_obj_005.jpg -> 3		frame_00001011_obj_001.jpg -> 4		frame_00001011_obj_004.jpg -> 4		frame_00001011_obj_005.jpg -> 3
frame_00001012_obj_001.jpg -> 1		frame_00001012_obj_004.jpg -> 4		frame_00001012_obj_005.jpg -> 3		frame_00001013_obj_001.jpg -> 1
frame_00001013_obj_004.jpg -> 4		frame_00001013_obj_005.jpg -> 3		frame_00001014_obj_001.jpg -> 1		frame_00001014_obj_004.jpg -> 4
frame_00001014_obj_005.jpg -> 3		frame_00001015_obj_001.jpg -> 1		frame_00001015_obj_004.jpg -> 4		frame_00001015_obj_005.jpg -> 4
frame_00001016_obj_001.jpg -> 1		frame_00001016_obj_004.jpg -> 4		frame_00001016_obj_005.jpg -> 4		frame_00001017_obj_001.jpg -> 1
frame_00001017_obj_004.jpg -> 4		frame_00001017_obj_005.jpg -> 4		frame_00001018_obj_001.jpg -> 1		frame_00001018_obj_004.jpg -> 4
frame_00001018_obj_005.jpg -> 4		frame_00001019_obj_001.jpg -> 1		frame_00001019_obj_004.jpg -> 2		frame_00001019_obj_005.jpg -> 0
frame_00001020_obj_002.jpg -> 3		frame_00001020_obj_003.jpg -> 2		frame_00001020_obj_004.jpg -> 0		frame_00001021_obj_002.jpg -> 2
frame_00001021_obj_003.jpg -> 0		frame_00001022_obj_003.jpg -> 2		frame_00001022_obj_004.jpg -> 0		frame_00001023_obj_003.jpg -> 4
frame_00001023_obj_004.jpg -> 2		frame_00001023_obj_005.jpg -> 0		frame_00001024_obj_003.jpg -> 4		frame_00001024_obj_004.jpg -> 2
frame_00001024_obj_005.jpg -> 0		frame_00001025_obj_002.jpg -> 4		frame_00001025_obj_003.jpg -> 0		frame_00001026_obj_002.jpg -> 4
frame_00001026_obj_003.jpg -> 0		frame_00001027_obj_003.jpg -> 2		frame_00001027_obj_004.jpg -> 0		frame_00001028_obj_002.jpg -> 4
frame_00001028_obj_003.jpg -> 2		frame_00001028_obj_004.jpg -> 0		frame_00001029_obj_002.jpg -> 4		frame_00001029_obj_003.jpg -> 0
frame_00001030_obj_003.jpg -> 2		frame_00001030_obj_004.jpg -> 0		frame_00001031_obj_002.jpg -> 4		frame_00001031_obj_003.jpg -> 4
frame_00001031_obj_004.jpg -> 0		frame_00001032_obj_003.jpg -> 0		frame_00001032_obj_004.jpg -> 2		frame_00001033_obj_002.jpg -> 0
frame_00001033_obj_003.jpg -> 2		frame_00001034_obj_002.jpg -> 0		frame_00001034_obj_003.jpg -> 1		frame_00001035_obj_003.jpg -> 0
frame_00001035_obj_004.jpg -> 1		frame_00001036_obj_002.jpg -> 1		frame_00001036_obj_004.jpg -> 0		frame_00001037_obj_004.jpg -> 1
frame_00001037_obj_005.jpg -> 0		frame_00001038_obj_004.jpg -> 0		frame_00001038_obj_006.jpg -> 2		frame_00001039_obj_002.jpg -> 1
frame_00001039_obj_004.jpg -> 0		frame_00001040_obj_003.jpg -> 1		frame_00001040_obj_004.jpg -> 0		frame_00001041_obj_003.jpg -> 1
frame_00001041_obj_004.jpg -> 0		frame_00001042_obj_005.jpg -> 1		frame_00001042_obj_006.jpg -> 0		frame_00001043_obj_005.jpg -> 1
frame_00001043_obj_006.jpg -> 0		frame_00001044_obj_002.jpg -> 0		frame_00001044_obj_004.jpg -> 1		frame_00001045_obj_002.jpg -> 0
frame_00001045_obj_004.jpg -> 1		frame_00001046_obj_003.jpg -> 1		frame_00001046_obj_005.jpg -> 1		frame_00001047_obj_002.jpg -> 1
frame_00001047_obj_004.jpg -> 1		frame_00001048_obj_001.jpg -> 1		frame_00001048_obj_004.jpg -> 1		frame_00001049_obj_001.jpg -> 1
frame_00001049_obj_005.jpg -> 1		frame_00001049_obj_007.jpg -> 3		frame_00001050_obj_001.jpg -> 1		frame_00001050_obj_004.jpg -> 1
frame_00001050_obj_006.jpg -> 3		frame_00001051_obj_001.jpg -> 1		frame_00001051_obj_006.jpg -> 1		frame_00001051_obj_008.jpg -> 3
frame_00001052_obj_001.jpg -> 1		frame_00001052_obj_004.jpg -> 1		frame_00001052_obj_007.jpg -> 3		frame_00001053_obj_002.jpg -> 1
frame_00001053_obj_005.jpg -> 1		frame_00001053_obj_008.jpg -> 3		frame_00001054_obj_001.jpg -> 1		frame_00001054_obj_004.jpg -> 1
frame_00001054_obj_007.jpg -> 3		frame_00001055_obj_003.jpg -> 1		frame_00001055_obj_006.jpg -> 3		frame_00001056_obj_002.jpg -> 1
frame_00001056_obj_006.jpg -> 3		frame_00001057_obj_002.jpg -> 1		frame_00001057_obj_006.jpg -> 3		frame_00001058_obj_002.jpg -> 1
frame_00001058_obj_006.jpg -> 3		frame_00001059_obj_002.jpg -> 1		frame_00001059_obj_006.jpg -> 3		frame_00001060_obj_002.jpg -> 1
frame_00001060_obj_007.jpg -> 3		frame_00001061_obj_002.jpg -> 1		frame_00001061_obj_007.jpg -> 3		frame_00001062_obj_002.jpg -> 1
frame_00001062_obj_006.jpg -> 3		frame_00001063_obj_002.jpg -> 1		frame_00001063_obj_006.jpg -> 3		frame_00001064_obj_002.jpg -> 1
frame_00001064_obj_006.jpg -> 3		frame_00001065_obj_003.jpg -> 1		frame_00001065_obj_007.jpg -> 3		frame_00001066_obj_004.jpg -> 1
frame_00001066_obj_008.jpg -> 3		frame_00001067_obj_006.jpg -> 3		frame_00001068_obj_006.jpg -> 3		frame_00001069_obj_006.jpg -> 3
frame_00001070_obj_006.jpg -> 3		frame_00001071_obj_006.jpg -> 3		frame_00001072_obj_006.jpg -> 3		frame_00001073_obj_005.jpg -> 3
frame_00001074_obj_005.jpg -> 3		frame_00001075_obj_005.jpg -> 3		frame_00001076_obj_005.jpg -> 3		frame_00001077_obj_005.jpg -> 3
frame_00001078_obj_005.jpg -> 3		frame_00001079_obj_005.jpg -> 3		frame_00001080_obj_005.jpg -> 3		frame_00001081_obj_004.jpg -> 3
frame_00001082_obj_004.jpg -> 4		frame_00001083_obj_005.jpg -> 4		frame_00001084_obj_004.jpg -> 4		frame_00001085_obj_005.jpg -> 4
frame_00001086_obj_004.jpg -> 4		frame_00001087_obj_004.jpg -> 4		frame_00001088_obj_005.jpg -> 4		frame_00001089_obj_005.jpg -> 4
frame_00001090_obj_004.jpg -> 4		frame_00001091_obj_004.jpg -> 4		frame_00001092_obj_005.jpg -> 4		frame_00001093_obj_004.jpg -> 4
frame_00001093_obj_005.jpg -> 4		frame_00001094_obj_004.jpg -> 4		frame_00001094_obj_005.jpg -> 4		frame_00001095_obj_003.jpg -> 4
frame_00001095_obj_004.jpg -> 4		frame_00001096_obj_003.jpg -> 4		frame_00001096_obj_004.jpg -> 4		frame_00001097_obj_003.jpg -> 4
frame_00001097_obj_004.jpg -> 4		frame_00001098_obj_003.jpg -> 4		frame_00001098_obj_004.jpg -> 4		frame_00001099_obj_003.jpg -> 4
frame_00001100_obj_003.jpg -> 3		frame_00001101_obj_003.jpg -> 3		frame_00001102_obj_003.jpg -> 3		frame_00001103_obj_003.jpg -> 3
frame_00001104_obj_003.jpg -> 3		frame_00001105_obj_001.jpg -> 2		frame_00001106_obj_001.jpg -> 2		frame_00001146_obj_002.jpg -> 0
frame_00001185_obj_006.jpg -> 2		frame_00001187_obj_005.jpg -> 2		frame_00001188_obj_005.jpg -> 3		frame_00001194_obj_004.jpg -> 3
frame_00001195_obj_006.jpg -> 3		frame_00001196_obj_006.jpg -> 3		frame_00001197_obj_005.jpg -> 3		frame_00001198_obj_005.jpg -> 4
frame_00001199_obj_005.jpg -> 4		frame_00001200_obj_006.jpg -> 4		frame_00001201_obj_006.jpg -> 4		frame_00001202_obj_006.jpg -> 4
frame_00001203_obj_004.jpg -> 4		frame_00001204_obj_004.jpg -> 4		frame_00001205_obj_005.jpg -> 4		frame_00001206_obj_004.jpg -> 4
frame_00001207_obj_004.jpg -> 4		frame_00001208_obj_002.jpg -> 4		frame_00001209_obj_002.jpg -> 4		frame_00001210_obj_002.jpg -> 1
frame_00001211_obj_002.jpg -> 1		frame_00001212_obj_002.jpg -> 1		frame_00001213_obj_002.jpg -> 1		frame_00001214_obj_003.jpg -> 1
frame_00001215_obj_003.jpg -> 1		frame_00001216_obj_002.jpg -> 3		frame_00001216_obj_004.jpg -> 1		frame_00001217_obj_002.jpg -> 3
frame_00001217_obj_004.jpg -> 1		frame_00001218_obj_002.jpg -> 1		frame_00001219_obj_001.jpg -> 2		frame_00001219_obj_003.jpg -> 1
frame_00001220_obj_002.jpg -> 1		frame_00001221_obj_002.jpg -> 1		frame_00001222_obj_001.jpg -> 2		frame_00001222_obj_002.jpg -> 1
frame_00001223_obj_002.jpg -> 1		frame_00001224_obj_001.jpg -> 3		frame_00001224_obj_002.jpg -> 1		frame_00001225_obj_001.jpg -> 3
frame_00001225_obj_003.jpg -> 1		frame_00001226_obj_002.jpg -> 1		frame_00001227_obj_002.jpg -> 1		frame_00001228_obj_001.jpg -> 2
frame_00001228_obj_002.jpg -> 1		frame_00001229_obj_001.jpg -> 2		frame_00001229_obj_002.jpg -> 1		frame_00001229_obj_003.jpg -> 2
frame_00001230_obj_001.jpg -> 1		frame_00001230_obj_002.jpg -> 2		frame_00001231_obj_001.jpg -> 2		frame_00001231_obj_002.jpg -> 1
frame_00001231_obj_003.jpg -> 2		frame_00001232_obj_001.jpg -> 2		frame_00001232_obj_002.jpg -> 1		frame_00001233_obj_001.jpg -> 2
frame_00001233_obj_002.jpg -> 1		frame_00001234_obj_001.jpg -> 2		frame_00001234_obj_002.jpg -> 1		frame_00001235_obj_001.jpg -> 1
frame_00001236_obj_001.jpg -> 1		frame_00001237_obj_001.jpg -> 1		frame_00001238_obj_001.jpg -> 1		frame_00001239_obj_001.jpg -> 1
frame_00001240_obj_001.jpg -> 1		frame_00001241_obj_001.jpg -> 2		frame_00001241_obj_002.jpg -> 1		frame_00001242_obj_001.jpg -> 1
frame_00001243_obj_001.jpg -> 2		frame_00001243_obj_002.jpg -> 1		frame_00001244_obj_001.jpg -> 1		frame_00001245_obj_001.jpg -> 1
frame_00001246_obj_001.jpg -> 1		frame_00001247_obj_002.jpg -> 1		frame_00001248_obj_001.jpg -> 1		frame_00001249_obj_002.jpg -> 1
frame_00001250_obj_001.jpg -> 1		frame_00001251_obj_001.jpg -> 1		frame_00001252_obj_001.jpg -> 4		frame_00001253_obj_001.jpg -> 0
frame_00001253_obj_002.jpg -> 1		frame_00001254_obj_001.jpg -> 1		frame_00001254_obj_002.jpg -> 1		frame_00001255_obj_001.jpg -> 1
frame_00001256_obj_001.jpg -> 1		frame_00001257_obj_001.jpg -> 1		frame_00001258_obj_001.jpg -> 1		frame_00001260_obj_001.jpg -> 1
frame_00001261_obj_001.jpg -> 1		frame_00001262_obj_001.jpg -> 1		frame_00001263_obj_001.jpg -> 1		frame_00001266_obj_001.jpg -> 2
frame_00001267_obj_001.jpg -> 2		frame_00001268_obj_001.jpg -> 2		frame_00001269_obj_001.jpg -> 2		frame_00001270_obj_001.jpg -> 2
frame_00001271_obj_001.jpg -> 2		frame_00001272_obj_001.jpg -> 2		frame_00001273_obj_001.jpg -> 2		frame_00001276_obj_001.jpg -> 2
frame_00001283_obj_003.jpg -> 3		frame_00001284_obj_003.jpg -> 3		frame_00001285_obj_003.jpg -> 3		frame_00001286_obj_003.jpg -> 3
frame_00001287_obj_003.jpg -> 3		frame_00001288_obj_004.jpg -> 3		frame_00001289_obj_004.jpg -> 3		frame_00001290_obj_004.jpg -> 3
frame_00001291_obj_004.jpg -> 3		frame_00001292_obj_004.jpg -> 3		frame_00001293_obj_004.jpg -> 3		frame_00001294_obj_004.jpg -> 3
frame_00001295_obj_004.jpg -> 3		frame_00001296_obj_004.jpg -> 3		frame_00001297_obj_004.jpg -> 3		frame_00001298_obj_004.jpg -> 3
frame_00001299_obj_004.jpg -> 3		frame_00001300_obj_001.jpg -> 1		frame_00001300_obj_005.jpg -> 3		frame_00001301_obj_001.jpg -> 1
frame_00001301_obj_005.jpg -> 3		frame_00001302_obj_001.jpg -> 1		frame_00001302_obj_005.jpg -> 3		frame_00001303_obj_004.jpg -> 3
frame_00001304_obj_004.jpg -> 3		frame_00001305_obj_004.jpg -> 4		frame_00001306_obj_004.jpg -> 4		frame_00001307_obj_004.jpg -> 4
frame_00001308_obj_004.jpg -> 4		frame_00001309_obj_004.jpg -> 4		frame_00001310_obj_003.jpg -> 4		frame_00001311_obj_003.jpg -> 4
frame_00001312_obj_003.jpg -> 4		frame_00001313_obj_004.jpg -> 4		frame_00001314_obj_004.jpg -> 4		frame_00001315_obj_004.jpg -> 4
frame_00001316_obj_003.jpg -> 1		frame_00001316_obj_004.jpg -> 4		frame_00001317_obj_004.jpg -> 4		frame_00001318_obj_004.jpg -> 1
frame_00001318_obj_005.jpg -> 4		frame_00001319_obj_004.jpg -> 1		frame_00001319_obj_005.jpg -> 4		frame_00001320_obj_003.jpg -> 1
frame_00001320_obj_004.jpg -> 4		frame_00001321_obj_004.jpg -> 4		frame_00001322_obj_003.jpg -> 1		frame_00001322_obj_004.jpg -> 4
frame_00001323_obj_004.jpg -> 1		frame_00001323_obj_005.jpg -> 4		frame_00001324_obj_005.jpg -> 4		frame_00001325_obj_005.jpg -> 1
frame_00001325_obj_006.jpg -> 4		frame_00001326_obj_005.jpg -> 4		frame_00001327_obj_004.jpg -> 1		frame_00001327_obj_005.jpg -> 4
frame_00001328_obj_004.jpg -> 1		frame_00001328_obj_005.jpg -> 4		frame_00001329_obj_001.jpg -> 1		frame_00001329_obj_004.jpg -> 1
frame_00001329_obj_005.jpg -> 4		frame_00001330_obj_001.jpg -> 1		frame_00001330_obj_004.jpg -> 1		frame_00001330_obj_005.jpg -> 4
frame_00001331_obj_002.jpg -> 2		frame_00001331_obj_005.jpg -> 1		frame_00001331_obj_006.jpg -> 4		frame_00001332_obj_003.jpg -> 1
frame_00001332_obj_004.jpg -> 4		frame_00001333_obj_003.jpg -> 1		frame_00001333_obj_004.jpg -> 4		frame_00001334_obj_001.jpg -> 1
frame_00001334_obj_004.jpg -> 1		frame_00001334_obj_005.jpg -> 4		frame_00001335_obj_001.jpg -> 1		frame_00001335_obj_004.jpg -> 1
frame_00001335_obj_005.jpg -> 1		frame_00001336_obj_001.jpg -> 1		frame_00001336_obj_004.jpg -> 4		frame_00001336_obj_005.jpg -> 1
frame_00001337_obj_001.jpg -> 1		frame_00001337_obj_004.jpg -> 4		frame_00001337_obj_005.jpg -> 1		frame_00001338_obj_001.jpg -> 1
frame_00001338_obj_004.jpg -> 4		frame_00001338_obj_005.jpg -> 1		frame_00001339_obj_001.jpg -> 1		frame_00001339_obj_004.jpg -> 4
frame_00001339_obj_005.jpg -> 1		frame_00001340_obj_002.jpg -> 1		frame_00001340_obj_004.jpg -> 4		frame_00001340_obj_005.jpg -> 1
frame_00001341_obj_001.jpg -> 1		frame_00001341_obj_003.jpg -> 1		frame_00001341_obj_005.jpg -> 4		frame_00001342_obj_001.jpg -> 1
frame_00001342_obj_003.jpg -> 1		frame_00001342_obj_005.jpg -> 1		frame_00001343_obj_003.jpg -> 1		frame_00001343_obj_005.jpg -> 1
frame_00001344_obj_001.jpg -> 1		frame_00001344_obj_002.jpg -> 1		frame_00001344_obj_005.jpg -> 1		frame_00001345_obj_001.jpg -> 1
frame_00001345_obj_002.jpg -> 1		frame_00001345_obj_005.jpg -> 1		frame_00001346_obj_001.jpg -> 1		frame_00001346_obj_004.jpg -> 1
frame_00001347_obj_001.jpg -> 1		frame_00001347_obj_002.jpg -> 1		frame_00001347_obj_005.jpg -> 1		frame_00001348_obj_003.jpg -> 1
frame_00001348_obj_004.jpg -> 1		frame_00001349_obj_003.jpg -> 1		frame_00001350_obj_001.jpg -> 1		frame_00001350_obj_004.jpg -> 1
frame_00001351_obj_004.jpg -> 1		frame_00001352_obj_003.jpg -> 1		frame_00001353_obj_004.jpg -> 1		frame_00001354_obj_003.jpg -> 1
frame_00001355_obj_003.jpg -> 1		frame_00001356_obj_003.jpg -> 1		frame_00001357_obj_003.jpg -> 1		frame_00001358_obj_003.jpg -> 1
frame_00001359_obj_003.jpg -> 1		frame_00001360_obj_003.jpg -> 1		frame_00001361_obj_003.jpg -> 1		frame_00001362_obj_003.jpg -> 1
frame_00001363_obj_003.jpg -> 1		frame_00001364_obj_004.jpg -> 1		frame_00001365_obj_004.jpg -> 1		frame_00001366_obj_005.jpg -> 1
frame_00001367_obj_005.jpg -> 1		frame_00001368_obj_005.jpg -> 1		frame_00001369_obj_005.jpg -> 1		frame_00001370_obj_004.jpg -> 1
frame_00001371_obj_003.jpg -> 1		frame_00001372_obj_002.jpg -> 1		frame_00001373_obj_002.jpg -> 1		frame_00001374_obj_002.jpg -> 1
frame_00001375_obj_002.jpg -> 1		frame_00001376_obj_002.jpg -> 1		frame_00001377_obj_002.jpg -> 1		frame_00001378_obj_002.jpg -> 1
frame_00001379_obj_002.jpg -> 1		frame_00001380_obj_002.jpg -> 1		frame_00001381_obj_002.jpg -> 1		frame_00001382_obj_002.jpg -> 1
frame_00001383_obj_002.jpg -> 1		frame_00001384_obj_002.jpg -> 1		frame_00001385_obj_002.jpg -> 1		frame_00001386_obj_002.jpg -> 1
frame_00001387_obj_002.jpg -> 1		frame_00001388_obj_002.jpg -> 1		frame_00001389_obj_001.jpg -> 2		frame_00001390_obj_001.jpg -> 2
frame_00001391_obj_001.jpg -> 2		frame_00001392_obj_001.jpg -> 2		frame_00001393_obj_001.jpg -> 2		frame_00001394_obj_001.jpg -> 1
frame_00001395_obj_001.jpg -> 2		frame_00001396_obj_001.jpg -> 2		frame_00001397_obj_001.jpg -> 2		frame_00001398_obj_001.jpg -> 2
frame_00001399_obj_001.jpg -> 2		frame_00001400_obj_001.jpg -> 2		frame_00001401_obj_001.jpg -> 2		frame_00001508_obj_001.jpg -> 2
frame_00001509_obj_002.jpg -> 2		frame_00001510_obj_002.jpg -> 2		frame_00001518_obj_003.jpg -> 1		frame_00001520_obj_003.jpg -> 1
frame_00001521_obj_003.jpg -> 2		frame_00001521_obj_004.jpg -> 1		frame_00001522_obj_003.jpg -> 2		frame_00001522_obj_004.jpg -> 1
frame_00001523_obj_003.jpg -> 2		frame_00001523_obj_004.jpg -> 1		frame_00001524_obj_003.jpg -> 2		frame_00001524_obj_004.jpg -> 1
frame_00001525_obj_003.jpg -> 2		frame_00001525_obj_004.jpg -> 2		frame_00001526_obj_003.jpg -> 2		frame_00001526_obj_004.jpg -> 1
frame_00001527_obj_003.jpg -> 2		frame_00001527_obj_004.jpg -> 1		frame_00001528_obj_003.jpg -> 2		frame_00001529_obj_003.jpg -> 2
frame_00001530_obj_003.jpg -> 2		frame_00001530_obj_004.jpg -> 1		frame_00001531_obj_003.jpg -> 2		frame_00001531_obj_004.jpg -> 1
frame_00001532_obj_003.jpg -> 2		frame_00001532_obj_004.jpg -> 1		frame_00001533_obj_003.jpg -> 2		frame_00001533_obj_004.jpg -> 1
frame_00001534_obj_004.jpg -> 2		frame_00001534_obj_005.jpg -> 1		frame_00001535_obj_004.jpg -> 2		frame_00001535_obj_005.jpg -> 1
frame_00001536_obj_003.jpg -> 2		frame_00001536_obj_004.jpg -> 1		frame_00001537_obj_003.jpg -> 2		frame_00001537_obj_004.jpg -> 1
frame_00001537_obj_005.jpg -> 2		frame_00001538_obj_003.jpg -> 2		frame_00001538_obj_004.jpg -> 1		frame_00001538_obj_005.jpg -> 3
frame_00001538_obj_006.jpg -> 2		frame_00001539_obj_003.jpg -> 2		frame_00001539_obj_004.jpg -> 1		frame_00001539_obj_005.jpg -> 3
frame_00001539_obj_006.jpg -> 2		frame_00001540_obj_004.jpg -> 2		frame_00001540_obj_005.jpg -> 1		frame_00001540_obj_006.jpg -> 3
frame_00001540_obj_007.jpg -> 2		frame_00001541_obj_003.jpg -> 2		frame_00001541_obj_004.jpg -> 3		frame_00001541_obj_005.jpg -> 3
frame_00001542_obj_004.jpg -> 2		frame_00001542_obj_005.jpg -> 3		frame_00001542_obj_006.jpg -> 3		frame_00001543_obj_003.jpg -> 2
frame_00001543_obj_004.jpg -> 3		frame_00001543_obj_005.jpg -> 3		frame_00001544_obj_003.jpg -> 2		frame_00001544_obj_004.jpg -> 3
frame_00001544_obj_005.jpg -> 3		frame_00001545_obj_003.jpg -> 2		frame_00001545_obj_004.jpg -> 3		frame_00001545_obj_005.jpg -> 3
frame_00001546_obj_003.jpg -> 2		frame_00001546_obj_004.jpg -> 3		frame_00001546_obj_005.jpg -> 3		frame_00001547_obj_003.jpg -> 2
frame_00001547_obj_004.jpg -> 3		frame_00001547_obj_005.jpg -> 3		frame_00001548_obj_003.jpg -> 2		frame_00001548_obj_004.jpg -> 3
frame_00001549_obj_003.jpg -> 2		frame_00001549_obj_004.jpg -> 3		frame_00001550_obj_003.jpg -> 2		frame_00001550_obj_004.jpg -> 3
frame_00001550_obj_005.jpg -> 3		frame_00001551_obj_002.jpg -> 2		frame_00001551_obj_003.jpg -> 3		frame_00001551_obj_004.jpg -> 3
frame_00001552_obj_002.jpg -> 3		frame_00001552_obj_003.jpg -> 3		frame_00001552_obj_004.jpg -> 2		frame_00001553_obj_004.jpg -> 3
frame_00001553_obj_005.jpg -> 2		frame_00001554_obj_003.jpg -> 3		frame_00001554_obj_004.jpg -> 2		frame_00001555_obj_003.jpg -> 3
frame_00001555_obj_004.jpg -> 2		frame_00001556_obj_002.jpg -> 3		frame_00001556_obj_003.jpg -> 2		frame_00001557_obj_002.jpg -> 3
frame_00001557_obj_003.jpg -> 2		frame_00001558_obj_002.jpg -> 4		frame_00001558_obj_003.jpg -> 2		frame_00001559_obj_002.jpg -> 4
frame_00001559_obj_003.jpg -> 2		frame_00001560_obj_002.jpg -> 4		frame_00001560_obj_003.jpg -> 2		frame_00001561_obj_002.jpg -> 4
frame_00001561_obj_003.jpg -> 2		frame_00001562_obj_002.jpg -> 4		frame_00001562_obj_003.jpg -> 2		frame_00001563_obj_002.jpg -> 4
frame_00001563_obj_003.jpg -> 2		frame_00001564_obj_002.jpg -> 4		frame_00001564_obj_003.jpg -> 2		frame_00001565_obj_002.jpg -> 4
frame_00001565_obj_003.jpg -> 2		frame_00001566_obj_002.jpg -> 4		frame_00001566_obj_003.jpg -> 2		frame_00001567_obj_002.jpg -> 2
frame_00001567_obj_003.jpg -> 4		frame_00001568_obj_002.jpg -> 0		frame_00001568_obj_003.jpg -> 2		frame_00001569_obj_002.jpg -> 0
frame_00001569_obj_003.jpg -> 2		frame_00001570_obj_002.jpg -> 0		frame_00001570_obj_003.jpg -> 2		frame_00001571_obj_001.jpg -> 1
frame_00001571_obj_002.jpg -> 0		frame_00001571_obj_003.jpg -> 2		frame_00001572_obj_001.jpg -> 1		frame_00001572_obj_002.jpg -> 0
frame_00001572_obj_003.jpg -> 2		frame_00001573_obj_001.jpg -> 1		frame_00001573_obj_002.jpg -> 0		frame_00001573_obj_003.jpg -> 2
frame_00001574_obj_002.jpg -> 0		frame_00001574_obj_003.jpg -> 2		frame_00001575_obj_002.jpg -> 0		frame_00001575_obj_003.jpg -> 2
frame_00001576_obj_002.jpg -> 0		frame_00001576_obj_003.jpg -> 2		frame_00001577_obj_001.jpg -> 1		frame_00001577_obj_002.jpg -> 0
frame_00001577_obj_003.jpg -> 2		frame_00001578_obj_001.jpg -> 1		frame_00001578_obj_002.jpg -> 2		frame_00001578_obj_003.jpg -> 0
frame_00001579_obj_001.jpg -> 2		frame_00001579_obj_002.jpg -> 0		frame_00001580_obj_001.jpg -> 0		frame_00001580_obj_002.jpg -> 2
frame_00001581_obj_001.jpg -> 0		frame_00001581_obj_002.jpg -> 2		frame_00001582_obj_001.jpg -> 2		frame_00001582_obj_002.jpg -> 0
frame_00001583_obj_001.jpg -> 3		frame_00001583_obj_002.jpg -> 2		frame_00001583_obj_003.jpg -> 0		frame_00001584_obj_001.jpg -> 4
frame_00001584_obj_002.jpg -> 3		frame_00001584_obj_003.jpg -> 0		frame_00001585_obj_001.jpg -> 4		frame_00001585_obj_002.jpg -> 3
frame_00001585_obj_003.jpg -> 0		frame_00001586_obj_001.jpg -> 4		frame_00001586_obj_002.jpg -> 3		frame_00001586_obj_003.jpg -> 0
frame_00001587_obj_002.jpg -> 4		frame_00001587_obj_003.jpg -> 3		frame_00001587_obj_004.jpg -> 0		frame_00001588_obj_002.jpg -> 3
frame_00001588_obj_003.jpg -> 3		frame_00001588_obj_004.jpg -> 0		frame_00001589_obj_002.jpg -> 3		frame_00001589_obj_003.jpg -> 0
frame_00001589_obj_004.jpg -> 3		frame_00001590_obj_001.jpg -> 3		frame_00001590_obj_002.jpg -> 0		frame_00001590_obj_003.jpg -> 3
frame_00001591_obj_001.jpg -> 4		frame_00001591_obj_002.jpg -> 0		frame_00001591_obj_003.jpg -> 3		frame_00001592_obj_001.jpg -> 2
frame_00001592_obj_002.jpg -> 3		frame_00001592_obj_003.jpg -> 0		frame_00001593_obj_001.jpg -> 2		frame_00001593_obj_002.jpg -> 3
frame_00001593_obj_003.jpg -> 0		frame_00001594_obj_001.jpg -> 3		frame_00001594_obj_002.jpg -> 3		frame_00001594_obj_003.jpg -> 0
frame_00001595_obj_001.jpg -> 3		frame_00001595_obj_002.jpg -> 3		frame_00001595_obj_003.jpg -> 0		frame_00001596_obj_002.jpg -> 3
frame_00001596_obj_003.jpg -> 3		frame_00001596_obj_004.jpg -> 0		frame_00001597_obj_001.jpg -> 3		frame_00001597_obj_002.jpg -> 3
frame_00001597_obj_003.jpg -> 0		frame_00001598_obj_001.jpg -> 0		frame_00001598_obj_002.jpg -> 3		frame_00001598_obj_003.jpg -> 3
frame_00001599_obj_001.jpg -> 0		frame_00001599_obj_002.jpg -> 3		frame_00001599_obj_003.jpg -> 3		frame_00001600_obj_002.jpg -> 0
frame_00001600_obj_003.jpg -> 3		frame_00001600_obj_004.jpg -> 3		frame_00001601_obj_001.jpg -> 0		frame_00001601_obj_002.jpg -> 3
frame_00001601_obj_003.jpg -> 3		frame_00001602_obj_001.jpg -> 0		frame_00001602_obj_002.jpg -> 3		frame_00001602_obj_003.jpg -> 3
frame_00001603_obj_001.jpg -> 0		frame_00001603_obj_002.jpg -> 3		frame_00001603_obj_003.jpg -> 3		frame_00001604_obj_001.jpg -> 0
frame_00001604_obj_002.jpg -> 3		frame_00001604_obj_003.jpg -> 3		frame_00001605_obj_001.jpg -> 0		frame_00001605_obj_002.jpg -> 3
frame_00001605_obj_003.jpg -> 3		frame_00001606_obj_001.jpg -> 2		frame_00001606_obj_003.jpg -> 0		frame_00001606_obj_004.jpg -> 3
frame_00001606_obj_005.jpg -> 3		frame_00001607_obj_002.jpg -> 2		frame_00001607_obj_003.jpg -> 0		frame_00001607_obj_004.jpg -> 3
frame_00001607_obj_005.jpg -> 3		frame_00001608_obj_001.jpg -> 2		frame_00001608_obj_002.jpg -> 0		frame_00001608_obj_003.jpg -> 3
frame_00001608_obj_004.jpg -> 3		frame_00001609_obj_002.jpg -> 0		frame_00001609_obj_003.jpg -> 3		frame_00001609_obj_004.jpg -> 3
frame_00001610_obj_002.jpg -> 0		frame_00001610_obj_003.jpg -> 3		frame_00001610_obj_004.jpg -> 3		frame_00001611_obj_003.jpg -> 0
frame_00001611_obj_004.jpg -> 3		frame_00001611_obj_005.jpg -> 3		frame_00001612_obj_003.jpg -> 0		frame_00001612_obj_004.jpg -> 3
frame_00001612_obj_005.jpg -> 3		frame_00001613_obj_003.jpg -> 0		frame_00001613_obj_004.jpg -> 3		frame_00001613_obj_005.jpg -> 3
frame_00001614_obj_001.jpg -> 0		frame_00001614_obj_003.jpg -> 3		frame_00001614_obj_004.jpg -> 3		frame_00001615_obj_001.jpg -> 2
frame_00001615_obj_004.jpg -> 3		frame_00001615_obj_005.jpg -> 3		frame_00001615_obj_006.jpg -> 3		frame_00001616_obj_003.jpg -> 3
frame_00001616_obj_004.jpg -> 3		frame_00001616_obj_005.jpg -> 3		frame_00001617_obj_002.jpg -> 0		frame_00001617_obj_004.jpg -> 3
frame_00001617_obj_005.jpg -> 3		frame_00001618_obj_003.jpg -> 0		frame_00001618_obj_004.jpg -> 3		frame_00001618_obj_005.jpg -> 3
frame_00001619_obj_001.jpg -> 2		frame_00001619_obj_004.jpg -> 0		frame_00001619_obj_005.jpg -> 3		frame_00001619_obj_006.jpg -> 3
frame_00001620_obj_003.jpg -> 0		frame_00001620_obj_004.jpg -> 3		frame_00001620_obj_005.jpg -> 3		frame_00001621_obj_005.jpg -> 0
frame_00001621_obj_006.jpg -> 3		frame_00001621_obj_007.jpg -> 3		frame_00001622_obj_006.jpg -> 0		frame_00001622_obj_007.jpg -> 3
frame_00001622_obj_008.jpg -> 3		frame_00001623_obj_007.jpg -> 3		frame_00001623_obj_008.jpg -> 3		frame_00001624_obj_006.jpg -> 3
frame_00001624_obj_007.jpg -> 3		frame_00001625_obj_005.jpg -> 3		frame_00001625_obj_006.jpg -> 3		frame_00001626_obj_006.jpg -> 3
frame_00001626_obj_007.jpg -> 3		frame_00001627_obj_006.jpg -> 3		frame_00001627_obj_007.jpg -> 3		frame_00001628_obj_005.jpg -> 3
frame_00001628_obj_006.jpg -> 3		frame_00001629_obj_006.jpg -> 3		frame_00001629_obj_007.jpg -> 3		frame_00001630_obj_007.jpg -> 3
frame_00001630_obj_008.jpg -> 3		frame_00001631_obj_008.jpg -> 3		frame_00001631_obj_009.jpg -> 3		frame_00001632_obj_006.jpg -> 3
frame_00001632_obj_007.jpg -> 3		frame_00001633_obj_006.jpg -> 3		frame_00001633_obj_007.jpg -> 4		frame_00001634_obj_005.jpg -> 3
frame_00001634_obj_006.jpg -> 4		frame_00001635_obj_004.jpg -> 3		frame_00001635_obj_005.jpg -> 4		frame_00001636_obj_004.jpg -> 3
frame_00001636_obj_005.jpg -> 4		frame_00001637_obj_005.jpg -> 3		frame_00001637_obj_006.jpg -> 4		frame_00001638_obj_006.jpg -> 3
frame_00001638_obj_007.jpg -> 4		frame_00001639_obj_005.jpg -> 2		frame_00001639_obj_006.jpg -> 3		frame_00001639_obj_007.jpg -> 4
frame_00001640_obj_004.jpg -> 3		frame_00001640_obj_005.jpg -> 4		frame_00001641_obj_003.jpg -> 1		frame_00001641_obj_005.jpg -> 3
frame_00001641_obj_006.jpg -> 4		frame_00001642_obj_005.jpg -> 3		frame_00001642_obj_006.jpg -> 4		frame_00001643_obj_005.jpg -> 3
frame_00001643_obj_006.jpg -> 4		frame_00001644_obj_004.jpg -> 3		frame_00001644_obj_005.jpg -> 4		frame_00001645_obj_005.jpg -> 4
frame_00001646_obj_005.jpg -> 4		frame_00001647_obj_005.jpg -> 4		frame_00001648_obj_005.jpg -> 4		frame_00001649_obj_001.jpg -> 3
frame_00001649_obj_006.jpg -> 4		frame_00001650_obj_001.jpg -> 3		frame_00001650_obj_005.jpg -> 4		
